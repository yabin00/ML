{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练样本预处理 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练样本准备完毕！\n",
      "共有数据 299470 条\n",
      "平均长度： 13.605252612949544\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "file = open(\"traindata.txt\", encoding='utf-8')\n",
    "test_str = \"中国首次火星探测任务天问一号探测器实施近火捕获制动\"\n",
    "\n",
    "new_sents = []\n",
    "sents_labels = []\n",
    "for line in file.readlines():\n",
    "    line = line.split()\n",
    "    new_sent = ''\n",
    "    sent_labels = ''\n",
    "    for word in line:\n",
    "        if len(word) == 1:\n",
    "            new_sent += word\n",
    "            sent_labels += 'S'\n",
    "        elif len(word) >= 2:\n",
    "            new_sent += word\n",
    "            sent_labels += 'B' + 'M'*(len(word)-2) + 'E'\n",
    "    if new_sent != '':\n",
    "        new_sents.append([new_sent])\n",
    "        sents_labels.append([sent_labels])\n",
    "print(\"训练样本准备完毕！\")\n",
    "print('共有数据 %d 条' % len(new_sents))\n",
    "print('平均长度：', np.mean([len(d[0]) for d in new_sents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qingshanleiluoxianfengxing']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sents[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SS']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 隐马模型实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计初始概率矩阵pi\n",
    "state = ['S', 'B', 'M', 'E']\n",
    "pi = np.zeros(4)\n",
    "for i in range(len(sents_labels)):\n",
    "    if sents_labels[i][0][0] == 'S':\n",
    "        pi[0] += 1\n",
    "    if sents_labels[i][0][0] == 'B':\n",
    "        pi[1] += 1\n",
    "pi /= np.sum(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计转移概率矩阵A和观测概率矩阵B\n",
    "A = np.zeros((4, 4))\n",
    "B = np.zeros((4, 65536)) # GB2312编码\n",
    "for i in range(len(sents_labels)):\n",
    "    for j in range(len(sents_labels[i][0])):\n",
    "        B[state.index(sents_labels[i][0][j]), ord(new_sents[i][0][j])] += 1 # 观测频率加1\n",
    "    for j in range(len(sents_labels[i][0]) - 1):\n",
    "        A[state.index(sents_labels[i][0][j]), state.index(sents_labels[i][0][j+1])] += 1 # 转移频率加1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99494482 0.00505518 0.         0.        ]\n",
      " [0.         0.         0.66209247 0.33790753]\n",
      " [0.         0.         0.34466914 0.65533086]\n",
      " [0.00342628 0.99657372 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    if np.sum(A[i]) != 0:\n",
    "        A[i] = A[i] / np.sum(A[i])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    B[i] /= np.sum(B[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
      "https://github.com/hmmlearn/hmmlearn/issues/335\n",
      "https://github.com/hmmlearn/hmmlearn/issues/340\n"
     ]
    }
   ],
   "source": [
    "from hmmlearn import hmm\n",
    "model = hmm.MultinomialHMM(n_components=4)\n",
    "model.startprob_ = pi\n",
    "model.emissionprob_ = B\n",
    "model.transmat_ = A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_trials must be set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     test_data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mord\u001b[39m(test_str[i]))\n\u001b[0;32m      4\u001b[0m test_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(test_data)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(states)\n",
      "File \u001b[1;32mF:\\ml\\Lib\\site-packages\\hmmlearn\\base.py:375\u001b[0m, in \u001b[0;36m_AbstractHMM.predict\u001b[1;34m(self, X, lengths)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, lengths\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    Find most likely state sequence corresponding to ``X``.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m        Labels for each sample from ``X``.\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 375\u001b[0m     _, state_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state_sequence\n",
      "File \u001b[1;32mF:\\ml\\Lib\\site-packages\\hmmlearn\\base.py:336\u001b[0m, in \u001b[0;36m_AbstractHMM.decode\u001b[1;34m(self, X, lengths, algorithm)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03mFind most likely state sequence corresponding to ``X``.\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03mscore : Compute the log probability under the model.\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    335\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartprob_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 336\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m algorithm \u001b[38;5;241m=\u001b[39m algorithm \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m algorithm \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m DECODER_ALGORITHMS:\n",
      "File \u001b[1;32mF:\\ml\\Lib\\site-packages\\hmmlearn\\hmm.py:936\u001b[0m, in \u001b[0;36mMultinomialHMM._check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features \u001b[38;5;241m=\u001b[39m n_features\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 936\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_trials must be set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: n_trials must be set"
     ]
    }
   ],
   "source": [
    "test_data = []\n",
    "for i in range(len(test_str)): # 得到编码\n",
    "    test_data.append(ord(test_str[i]))\n",
    "test_data = np.array(test_data).reshape(-1, 1)\n",
    "states = model.predict(test_data)\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'states' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m test_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mstates\u001b[49m)):\n\u001b[0;32m      3\u001b[0m     test_out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m test_str[i]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m states[i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m states[i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'states' is not defined"
     ]
    }
   ],
   "source": [
    "test_out = \"\"\n",
    "for i in range(len(states)):\n",
    "    test_out += test_str[i]\n",
    "    if states[i] == 0 or states[i] == 3:\n",
    "        test_out += ' '\n",
    "test_out = test_out.strip()\n",
    "print(test_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 条件随机场实现 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将训练语料改成crf++的格式，并写入文件crf_train_file\n",
    "crf_train_file = \"crf_train_file\"\n",
    "output_file = open(crf_train_file, 'w', encoding='utf-8')\n",
    "for i in range(len(new_sents)):\n",
    "    for j in range(len(new_sents[i][0])):\n",
    "        output_file.write(new_sents[i][0][j] + ' ' + sents_labels[i][0][j] + '\\n')\n",
    "    output_file.write('\\n')\n",
    "output_file.close()\n",
    "\n",
    "# 将测试文本改成crf++的格式，并写入文件crf_test_file\n",
    "crf_test_file = \"crf_test_file\"\n",
    "output_file = open(crf_test_file, 'w', encoding='utf-8')\n",
    "for i in range(len(test_str)):\n",
    "    output_file.write(test_str[i] + '\\n')\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将crf_learn.exe、crf_test.exe、libcrfpp.dll文件拷贝到目录下，定义一个模板文件：template。在控制台环境下，执行“crf_learn template crf_train_file crf_model”命令进行训练，得到模型文件：crf_model。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  在控制台环境下，执行“crf_test -m crf_model crf_test_file > crf_test_output”命令得到测试语句的输出文件：crf_test_output。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'crf_test_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 将测试语句的分词输出改写方便观看的格式。\u001b[39;00m\n\u001b[0;32m      2\u001b[0m crf_test_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrf_test_output\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m input_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcrf_test_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m input_file\u001b[38;5;241m.\u001b[39mreadlines():\n",
      "File \u001b[1;32mF:\\ml\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'crf_test_output'"
     ]
    }
   ],
   "source": [
    "# 将测试语句的分词输出改写方便观看的格式。\n",
    "crf_test_output = \"crf_test_output\"\n",
    "input_file = open(crf_test_output, encoding='utf-8')\n",
    "str = \"\"\n",
    "for line in input_file.readlines():\n",
    "    line = line.split()\n",
    "    if len(line) == 2:\n",
    "        if line[1] == 'E' or line[1] == 'S':\n",
    "            str += line[0] + ' '\n",
    "        else:\n",
    "            str += line[0]\n",
    "input_file.close()\n",
    "print(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow2框架下循环神经网络实现 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# 重要参数\n",
    "tags = {'S': 0, 'B': 1, 'M': 2, 'E': 3, 'X': 4} # 标签\n",
    "embedding_size = 32 # 词向量大小\n",
    "maxlen = 32 # 序列长度，长于则截断，短于则填充0\n",
    "hidden_size = 32\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "checkpointfilepath = 'weights.best.hdf5' # 中间结果保存文件\n",
    "modepath = 'dz.h5' # 模型保存文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不同字的个数：4222\n",
      "字典创建完毕！\n"
     ]
    }
   ],
   "source": [
    "# 1.提取出所有用到的字，形成字典\n",
    "stat = {}\n",
    "for i in range(len(new_sents)):\n",
    "    for v in new_sents[i][0]:\n",
    "        stat[v] = stat.get(v, 0) + 1\n",
    "stat = sorted(stat.items(), key=lambda x:x[1], reverse=True)\n",
    "vocab = [s[0] for s in stat]\n",
    "print(\"不同字的个数：\" + str(len(vocab)))\n",
    "char2id = {c : i + 1 for i, c in enumerate(vocab)} # 编号0为填充值，因此从1开始编号\n",
    "id2char = {i + 1 : c for i, c in enumerate(vocab)}\n",
    "print(\"字典创建完毕！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练样本准备完毕，训练样本共299470句。\n"
     ]
    }
   ],
   "source": [
    "# 2.将训练语句转化为训练样本\n",
    "trainX = []\n",
    "trainY = []\n",
    "for i in range(len(new_sents)):\n",
    "    x = [0] * maxlen # 默认填充值\n",
    "    y = [4] * maxlen # 默认标签X\n",
    "    sent = new_sents[i][0]\n",
    "    labe = sents_labels[i][0]\n",
    "    replace_len = len(sent)\n",
    "    if len(sent) > maxlen:\n",
    "        replace_len = maxlen\n",
    "    for j in range(replace_len):\n",
    "        x[j] = char2id[sent[j]]\n",
    "        y[j] = tags[labe[j]]\n",
    "    trainX.append(x)\n",
    "    trainY.append(y)\n",
    "trainX = np.array(trainX)\n",
    "trainY = tf.keras.utils.to_categorical(trainY, 5)\n",
    "print(\"训练样本准备完毕，训练样本共\" + str(len(trainX)) + \"句。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized keyword arguments passed to Embedding: {'input_length': 32}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCheckpoint\n\u001b[0;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(maxlen,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxlen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_zero\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m(X)\n\u001b[0;32m      7\u001b[0m blstm \u001b[38;5;241m=\u001b[39m Bidirectional(LSTM(hidden_size, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), merge_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconcat\u001b[39m\u001b[38;5;124m'\u001b[39m)(embedding)\n\u001b[0;32m      8\u001b[0m blstm \u001b[38;5;241m=\u001b[39m Dropout(\u001b[38;5;241m0.4\u001b[39m)(blstm)\n",
      "File \u001b[1;32mF:\\ml\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:81\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, embeddings_constraint, mask_zero, lora_rank, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     72\u001b[0m     input_dim,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     80\u001b[0m ):\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m input_dim\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim \u001b[38;5;241m=\u001b[39m output_dim\n",
      "File \u001b[1;32mF:\\ml\\Lib\\site-packages\\keras\\src\\layers\\layer.py:264\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[1;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_shape_arg \u001b[38;5;241m=\u001b[39m input_shape_arg\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized keyword arguments \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m     )\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast \u001b[38;5;241m=\u001b[39m autocast\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized keyword arguments passed to Embedding: {'input_length': 32}"
     ]
    }
   ],
   "source": [
    "# 3.搭建模型，并训练\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Dropout, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "X = Input(shape=(maxlen,), dtype='int32')\n",
    "embedding = Embedding(input_dim=len(vocab)+1, output_dim=embedding_size, input_length=maxlen, mask_zero=True)(X)\n",
    "blstm = Bidirectional(LSTM(hidden_size, return_sequences=True), merge_mode='concat')(embedding)\n",
    "blstm = Dropout(0.4)(blstm)\n",
    "blstm = Bidirectional(LSTM(hidden_size, return_sequences=True), merge_mode='concat')(blstm)\n",
    "blstm = Dropout(0.4)(blstm)\n",
    "output = TimeDistributed(Dense(5, activation='softmax'))(blstm)\n",
    "model = Model(X, output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "984/984 [==============================] - 211s 181ms/step - loss: 0.1928 - accuracy: 0.7169\n",
      "WARNING:tensorflow:Can save best model only with acc available, skipping.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.exists(checkpointfilepath): # 与下面的checkpoint起到及时保存训练结果的作用\n",
    "    print(\"加载前次训练模型参数。。。\")\n",
    "    model.load_weights(checkpointfilepath)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "checkpoint = ModelCheckpoint(checkpointfilepath, monitor='acc', verbose=1, save_best_only=True,\n",
    "                            mode='max')\n",
    "model.fit(trainX, trainY, batch_size=batch_size, epochs=epochs, callbacks=[checkpoint])\n",
    "model.save(modepath)\n",
    "#print(model.evaluate(trainX, trainY, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.利用训练好的模型进行分词\n",
    "def predict(testsent):\n",
    "    # 将汉字句子转换成模型需要的输入形式\n",
    "    x = [0] * maxlen\n",
    "    replace_len = len(testsent)\n",
    "    if len(testsent) > maxlen:\n",
    "        replace_len = maxlen\n",
    "    for j in range(replace_len):\n",
    "        x[j] = char2id[testsent[j]]\n",
    "    # 调用模型进行预测\n",
    "    label = model.predict([x]) \n",
    "    # 根据模型预测结果对输入句子进行切分\n",
    "    label = np.array(label)[0]\n",
    "    s = ''\n",
    "    for i in range(len(testsent)):\n",
    "        tag = np.argmax(label[i])\n",
    "        if tag == 0 or tag == 3: # 单字和词结尾加空格切分\n",
    "            s += testsent[i] + ' '\n",
    "        elif tag ==1 or tag == 2:\n",
    "            s += testsent[i]\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国 首次 火星 探测 任务 天问 一 号 探测器 实施 近火 捕获 制动 \n"
     ]
    }
   ],
   "source": [
    "predict(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
