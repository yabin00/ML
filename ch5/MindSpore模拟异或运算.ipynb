{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "import numpy as np\n",
    "from mindspore import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 样本实例\n",
    "XX = np.array([[0.0,0.0],\n",
    "              [0.0,1.0],\n",
    "              [1.0,0.0],\n",
    "              [1.0,1.0]])\n",
    "# 样本标签\n",
    "L = np.array([[0.0,1.0],\n",
    "              [1.0,0.0],\n",
    "              [1.0,0.0],\n",
    "              [0.0,1.0]])\n",
    "\n",
    "train_data = []\n",
    "eval_data = []\n",
    "for i in range(len(XX)):\n",
    "    train_data.append( (XX[i].astype(np.float32), L[i].astype(np.float32)) )\n",
    "    eval_data.append( list(XX[i]) )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import dataset as ds\n",
    "\n",
    "ds_train = ds.GeneratorDataset(train_data, column_names=['samples', 'label'])\n",
    "ds_train = ds_train.batch(4)\n",
    "ds_train = ds_train.repeat(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[4, 2], dtype=Float32, value=\n",
       "[[0.00000000e+000, 0.00000000e+000],\n",
       " [0.00000000e+000, 1.00000000e+000],\n",
       " [1.00000000e+000, 0.00000000e+000],\n",
       " [1.00000000e+000, 1.00000000e+000]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([0., 0.], dtype=float32), array([0., 1.], dtype=float32)),\n",
       " (array([0., 1.], dtype=float32), array([1., 0.], dtype=float32)),\n",
       " (array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)),\n",
       " (array([1., 1.], dtype=float32), array([0., 1.], dtype=float32))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.common.initializer import Normal\n",
    "from mindspore import nn, Parameter\n",
    "\n",
    "class xorNN(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(xorNN, self).__init__()\n",
    "        self.fc1 = nn.Dense(2, 4, Normal(0.02), Normal(0.02), True)\n",
    "        self.fc2 = nn.Dense(4, 2, Normal(0.02), Normal(0.02), True)\n",
    "        self.activate = nn.Sigmoid()\n",
    "        \n",
    "    def construct(self, x):\n",
    "        x = self.activate(self.fc1(x))\n",
    "        x = self.activate(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "net = xorNN() # 实例化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1, loss is 0.25\n",
      "epoch: 2 step: 1, loss is 0.25\n",
      "epoch: 3 step: 1, loss is 0.2500000298023224\n",
      "epoch: 4 step: 1, loss is 0.25000011920928955\n",
      "epoch: 5 step: 1, loss is 0.2499999850988388\n",
      "epoch: 6 step: 1, loss is 0.2500000298023224\n",
      "epoch: 7 step: 1, loss is 0.2500000596046448\n",
      "epoch: 8 step: 1, loss is 0.2499999701976776\n",
      "epoch: 9 step: 1, loss is 0.2500000298023224\n",
      "epoch: 10 step: 1, loss is 0.2500000298023224\n",
      "epoch: 11 step: 1, loss is 0.2500000298023224\n",
      "epoch: 12 step: 1, loss is 0.25\n",
      "epoch: 13 step: 1, loss is 0.2500000298023224\n",
      "epoch: 14 step: 1, loss is 0.2499999850988388\n",
      "epoch: 15 step: 1, loss is 0.25\n",
      "epoch: 16 step: 1, loss is 0.25\n",
      "epoch: 17 step: 1, loss is 0.25\n",
      "epoch: 18 step: 1, loss is 0.2500000298023224\n",
      "epoch: 19 step: 1, loss is 0.25\n",
      "epoch: 20 step: 1, loss is 0.25\n",
      "epoch: 21 step: 1, loss is 0.25\n",
      "epoch: 22 step: 1, loss is 0.25\n",
      "epoch: 23 step: 1, loss is 0.25\n",
      "epoch: 24 step: 1, loss is 0.25\n",
      "epoch: 25 step: 1, loss is 0.2500000298023224\n",
      "epoch: 26 step: 1, loss is 0.25\n",
      "epoch: 27 step: 1, loss is 0.25\n",
      "epoch: 28 step: 1, loss is 0.25\n",
      "epoch: 29 step: 1, loss is 0.25\n",
      "epoch: 30 step: 1, loss is 0.2499999850988388\n",
      "epoch: 31 step: 1, loss is 0.2499999850988388\n",
      "epoch: 32 step: 1, loss is 0.25\n",
      "epoch: 33 step: 1, loss is 0.25\n",
      "epoch: 34 step: 1, loss is 0.25\n",
      "epoch: 35 step: 1, loss is 0.2499999850988388\n",
      "epoch: 36 step: 1, loss is 0.25\n",
      "epoch: 37 step: 1, loss is 0.25\n",
      "epoch: 38 step: 1, loss is 0.25\n",
      "epoch: 39 step: 1, loss is 0.25\n",
      "epoch: 40 step: 1, loss is 0.25\n",
      "epoch: 41 step: 1, loss is 0.25\n",
      "epoch: 42 step: 1, loss is 0.25\n",
      "epoch: 43 step: 1, loss is 0.25\n",
      "epoch: 44 step: 1, loss is 0.25\n",
      "epoch: 45 step: 1, loss is 0.25\n",
      "epoch: 46 step: 1, loss is 0.25\n",
      "epoch: 47 step: 1, loss is 0.2499999850988388\n",
      "epoch: 48 step: 1, loss is 0.2499999850988388\n",
      "epoch: 49 step: 1, loss is 0.25\n",
      "epoch: 50 step: 1, loss is 0.2499999850988388\n",
      "epoch: 51 step: 1, loss is 0.25\n",
      "epoch: 52 step: 1, loss is 0.25\n",
      "epoch: 53 step: 1, loss is 0.25\n",
      "epoch: 54 step: 1, loss is 0.25\n",
      "epoch: 55 step: 1, loss is 0.2499999850988388\n",
      "epoch: 56 step: 1, loss is 0.2499999850988388\n",
      "epoch: 57 step: 1, loss is 0.2499999850988388\n",
      "epoch: 58 step: 1, loss is 0.25\n",
      "epoch: 59 step: 1, loss is 0.25\n",
      "epoch: 60 step: 1, loss is 0.25\n",
      "epoch: 61 step: 1, loss is 0.2499999850988388\n",
      "epoch: 62 step: 1, loss is 0.25\n",
      "epoch: 63 step: 1, loss is 0.25\n",
      "epoch: 64 step: 1, loss is 0.25\n",
      "epoch: 65 step: 1, loss is 0.25\n",
      "epoch: 66 step: 1, loss is 0.25\n",
      "epoch: 67 step: 1, loss is 0.2499999701976776\n",
      "epoch: 68 step: 1, loss is 0.25\n",
      "epoch: 69 step: 1, loss is 0.25\n",
      "epoch: 70 step: 1, loss is 0.25\n",
      "epoch: 71 step: 1, loss is 0.2499999850988388\n",
      "epoch: 72 step: 1, loss is 0.25\n",
      "epoch: 73 step: 1, loss is 0.25\n",
      "epoch: 74 step: 1, loss is 0.25\n",
      "epoch: 75 step: 1, loss is 0.25\n",
      "epoch: 76 step: 1, loss is 0.25\n",
      "epoch: 77 step: 1, loss is 0.2499999701976776\n",
      "epoch: 78 step: 1, loss is 0.25\n",
      "epoch: 79 step: 1, loss is 0.25\n",
      "epoch: 80 step: 1, loss is 0.2499999850988388\n",
      "epoch: 81 step: 1, loss is 0.25\n",
      "epoch: 82 step: 1, loss is 0.25\n",
      "epoch: 83 step: 1, loss is 0.2499999850988388\n",
      "epoch: 84 step: 1, loss is 0.2499999850988388\n",
      "epoch: 85 step: 1, loss is 0.25\n",
      "epoch: 86 step: 1, loss is 0.25\n",
      "epoch: 87 step: 1, loss is 0.25\n",
      "epoch: 88 step: 1, loss is 0.2499999850988388\n",
      "epoch: 89 step: 1, loss is 0.2499999850988388\n",
      "epoch: 90 step: 1, loss is 0.2499999701976776\n",
      "epoch: 91 step: 1, loss is 0.25\n",
      "epoch: 92 step: 1, loss is 0.2499999850988388\n",
      "epoch: 93 step: 1, loss is 0.2499999701976776\n",
      "epoch: 94 step: 1, loss is 0.25\n",
      "epoch: 95 step: 1, loss is 0.2499999850988388\n",
      "epoch: 96 step: 1, loss is 0.2499999701976776\n",
      "epoch: 97 step: 1, loss is 0.2499999701976776\n",
      "epoch: 98 step: 1, loss is 0.25\n",
      "epoch: 99 step: 1, loss is 0.25\n",
      "epoch: 100 step: 1, loss is 0.25\n",
      "epoch: 101 step: 1, loss is 0.2499999701976776\n",
      "epoch: 102 step: 1, loss is 0.2499999701976776\n",
      "epoch: 103 step: 1, loss is 0.2499999850988388\n",
      "epoch: 104 step: 1, loss is 0.2499999701976776\n",
      "epoch: 105 step: 1, loss is 0.2499999701976776\n",
      "epoch: 106 step: 1, loss is 0.2499999850988388\n",
      "epoch: 107 step: 1, loss is 0.24999995529651642\n",
      "epoch: 108 step: 1, loss is 0.2499999701976776\n",
      "epoch: 109 step: 1, loss is 0.24999995529651642\n",
      "epoch: 110 step: 1, loss is 0.2499999701976776\n",
      "epoch: 111 step: 1, loss is 0.24999995529651642\n",
      "epoch: 112 step: 1, loss is 0.2499999701976776\n",
      "epoch: 113 step: 1, loss is 0.24999994039535522\n",
      "epoch: 114 step: 1, loss is 0.24999995529651642\n",
      "epoch: 115 step: 1, loss is 0.24999995529651642\n",
      "epoch: 116 step: 1, loss is 0.24999994039535522\n",
      "epoch: 117 step: 1, loss is 0.24999995529651642\n",
      "epoch: 118 step: 1, loss is 0.24999992549419403\n",
      "epoch: 119 step: 1, loss is 0.24999991059303284\n",
      "epoch: 120 step: 1, loss is 0.24999989569187164\n",
      "epoch: 121 step: 1, loss is 0.24999991059303284\n",
      "epoch: 122 step: 1, loss is 0.24999989569187164\n",
      "epoch: 123 step: 1, loss is 0.24999989569187164\n",
      "epoch: 124 step: 1, loss is 0.24999989569187164\n",
      "epoch: 125 step: 1, loss is 0.24999986588954926\n",
      "epoch: 126 step: 1, loss is 0.24999986588954926\n",
      "epoch: 127 step: 1, loss is 0.24999985098838806\n",
      "epoch: 128 step: 1, loss is 0.24999986588954926\n",
      "epoch: 129 step: 1, loss is 0.24999982118606567\n",
      "epoch: 130 step: 1, loss is 0.24999982118606567\n",
      "epoch: 131 step: 1, loss is 0.2499997764825821\n",
      "epoch: 132 step: 1, loss is 0.2499997764825821\n",
      "epoch: 133 step: 1, loss is 0.2499997615814209\n",
      "epoch: 134 step: 1, loss is 0.24999970197677612\n",
      "epoch: 135 step: 1, loss is 0.24999968707561493\n",
      "epoch: 136 step: 1, loss is 0.24999968707561493\n",
      "epoch: 137 step: 1, loss is 0.24999964237213135\n",
      "epoch: 138 step: 1, loss is 0.24999961256980896\n",
      "epoch: 139 step: 1, loss is 0.24999959766864777\n",
      "epoch: 140 step: 1, loss is 0.24999956786632538\n",
      "epoch: 141 step: 1, loss is 0.2499995231628418\n",
      "epoch: 142 step: 1, loss is 0.24999947845935822\n",
      "epoch: 143 step: 1, loss is 0.24999943375587463\n",
      "epoch: 144 step: 1, loss is 0.24999937415122986\n",
      "epoch: 145 step: 1, loss is 0.24999932944774628\n",
      "epoch: 146 step: 1, loss is 0.2499992549419403\n",
      "epoch: 147 step: 1, loss is 0.24999919533729553\n",
      "epoch: 148 step: 1, loss is 0.24999913573265076\n",
      "epoch: 149 step: 1, loss is 0.2499990463256836\n",
      "epoch: 150 step: 1, loss is 0.24999897181987762\n",
      "epoch: 151 step: 1, loss is 0.24999888241291046\n",
      "epoch: 152 step: 1, loss is 0.2499987930059433\n",
      "epoch: 153 step: 1, loss is 0.24999868869781494\n",
      "epoch: 154 step: 1, loss is 0.2499985694885254\n",
      "epoch: 155 step: 1, loss is 0.24999843537807465\n",
      "epoch: 156 step: 1, loss is 0.2499983161687851\n",
      "epoch: 157 step: 1, loss is 0.24999818205833435\n",
      "epoch: 158 step: 1, loss is 0.2499980330467224\n",
      "epoch: 159 step: 1, loss is 0.24999785423278809\n",
      "epoch: 160 step: 1, loss is 0.24999767541885376\n",
      "epoch: 161 step: 1, loss is 0.24999746680259705\n",
      "epoch: 162 step: 1, loss is 0.24999725818634033\n",
      "epoch: 163 step: 1, loss is 0.24999704957008362\n",
      "epoch: 164 step: 1, loss is 0.24999678134918213\n",
      "epoch: 165 step: 1, loss is 0.24999655783176422\n",
      "epoch: 166 step: 1, loss is 0.24999630451202393\n",
      "epoch: 167 step: 1, loss is 0.24999597668647766\n",
      "epoch: 168 step: 1, loss is 0.24999567866325378\n",
      "epoch: 169 step: 1, loss is 0.24999535083770752\n",
      "epoch: 170 step: 1, loss is 0.24999499320983887\n",
      "epoch: 171 step: 1, loss is 0.24999462068080902\n",
      "epoch: 172 step: 1, loss is 0.2499941885471344\n",
      "epoch: 173 step: 1, loss is 0.24999378621578217\n",
      "epoch: 174 step: 1, loss is 0.24999335408210754\n",
      "epoch: 175 step: 1, loss is 0.24999284744262695\n",
      "epoch: 176 step: 1, loss is 0.24999234080314636\n",
      "epoch: 177 step: 1, loss is 0.24999183416366577\n",
      "epoch: 178 step: 1, loss is 0.2499912679195404\n",
      "epoch: 179 step: 1, loss is 0.24999064207077026\n",
      "epoch: 180 step: 1, loss is 0.24999003112316132\n",
      "epoch: 181 step: 1, loss is 0.2499893456697464\n",
      "epoch: 182 step: 1, loss is 0.2499886453151703\n",
      "epoch: 183 step: 1, loss is 0.2499879002571106\n",
      "epoch: 184 step: 1, loss is 0.24998711049556732\n",
      "epoch: 185 step: 1, loss is 0.24998629093170166\n",
      "epoch: 186 step: 1, loss is 0.24998542666435242\n",
      "epoch: 187 step: 1, loss is 0.2499845325946808\n",
      "epoch: 188 step: 1, loss is 0.24998356401920319\n",
      "epoch: 189 step: 1, loss is 0.2499825358390808\n",
      "epoch: 190 step: 1, loss is 0.24998149275779724\n",
      "epoch: 191 step: 1, loss is 0.2499804049730301\n",
      "epoch: 192 step: 1, loss is 0.24997924268245697\n",
      "epoch: 193 step: 1, loss is 0.24997800588607788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 194 step: 1, loss is 0.2499767541885376\n",
      "epoch: 195 step: 1, loss is 0.24997539818286896\n",
      "epoch: 196 step: 1, loss is 0.24997402727603912\n",
      "epoch: 197 step: 1, loss is 0.24997255206108093\n",
      "epoch: 198 step: 1, loss is 0.24997101724147797\n",
      "epoch: 199 step: 1, loss is 0.2499694526195526\n",
      "epoch: 200 step: 1, loss is 0.2499677538871765\n",
      "epoch: 201 step: 1, loss is 0.24996601045131683\n",
      "epoch: 202 step: 1, loss is 0.24996420741081238\n",
      "epoch: 203 step: 1, loss is 0.24996231496334076\n",
      "epoch: 204 step: 1, loss is 0.2499603033065796\n",
      "epoch: 205 step: 1, loss is 0.24995824694633484\n",
      "epoch: 206 step: 1, loss is 0.24995611608028412\n",
      "epoch: 207 step: 1, loss is 0.24995386600494385\n",
      "epoch: 208 step: 1, loss is 0.24995151162147522\n",
      "epoch: 209 step: 1, loss is 0.249949112534523\n",
      "epoch: 210 step: 1, loss is 0.24994657933712006\n",
      "epoch: 211 step: 1, loss is 0.24994395673274994\n",
      "epoch: 212 step: 1, loss is 0.24994122982025146\n",
      "epoch: 213 step: 1, loss is 0.24993839859962463\n",
      "epoch: 214 step: 1, loss is 0.24993544816970825\n",
      "epoch: 215 step: 1, loss is 0.2499324232339859\n",
      "epoch: 216 step: 1, loss is 0.2499292492866516\n",
      "epoch: 217 step: 1, loss is 0.24992600083351135\n",
      "epoch: 218 step: 1, loss is 0.24992257356643677\n",
      "epoch: 219 step: 1, loss is 0.2499190717935562\n",
      "epoch: 220 step: 1, loss is 0.24991540610790253\n",
      "epoch: 221 step: 1, loss is 0.24991163611412048\n",
      "epoch: 222 step: 1, loss is 0.2499077171087265\n",
      "epoch: 223 step: 1, loss is 0.24990369379520416\n",
      "epoch: 224 step: 1, loss is 0.24989953637123108\n",
      "epoch: 225 step: 1, loss is 0.24989521503448486\n",
      "epoch: 226 step: 1, loss is 0.2498907595872879\n",
      "epoch: 227 step: 1, loss is 0.2498861849308014\n",
      "epoch: 228 step: 1, loss is 0.24988140165805817\n",
      "epoch: 229 step: 1, loss is 0.24987651407718658\n",
      "epoch: 230 step: 1, loss is 0.24987147748470306\n",
      "epoch: 231 step: 1, loss is 0.2498662769794464\n",
      "epoch: 232 step: 1, loss is 0.24986092746257782\n",
      "epoch: 233 step: 1, loss is 0.2498553842306137\n",
      "epoch: 234 step: 1, loss is 0.24984970688819885\n",
      "epoch: 235 step: 1, loss is 0.24984388053417206\n",
      "epoch: 236 step: 1, loss is 0.24983784556388855\n",
      "epoch: 237 step: 1, loss is 0.2498316466808319\n",
      "epoch: 238 step: 1, loss is 0.24982532858848572\n",
      "epoch: 239 step: 1, loss is 0.24981877207756042\n",
      "epoch: 240 step: 1, loss is 0.249812051653862\n",
      "epoch: 241 step: 1, loss is 0.24980518221855164\n",
      "epoch: 242 step: 1, loss is 0.24979808926582336\n",
      "epoch: 243 step: 1, loss is 0.24979081749916077\n",
      "epoch: 244 step: 1, loss is 0.24978335201740265\n",
      "epoch: 245 step: 1, loss is 0.2497757077217102\n",
      "epoch: 246 step: 1, loss is 0.24976786971092224\n",
      "epoch: 247 step: 1, loss is 0.24975982308387756\n",
      "epoch: 248 step: 1, loss is 0.24975158274173737\n",
      "epoch: 249 step: 1, loss is 0.24974316358566284\n",
      "epoch: 250 step: 1, loss is 0.24973450601100922\n",
      "epoch: 251 step: 1, loss is 0.24972566962242126\n",
      "epoch: 252 step: 1, loss is 0.2497166097164154\n",
      "epoch: 253 step: 1, loss is 0.24970734119415283\n",
      "epoch: 254 step: 1, loss is 0.24969786405563354\n",
      "epoch: 255 step: 1, loss is 0.24968817830085754\n",
      "epoch: 256 step: 1, loss is 0.24967825412750244\n",
      "epoch: 257 step: 1, loss is 0.249668151140213\n",
      "epoch: 258 step: 1, loss is 0.2496577799320221\n",
      "epoch: 259 step: 1, loss is 0.24964721500873566\n",
      "epoch: 260 step: 1, loss is 0.24963639676570892\n",
      "epoch: 261 step: 1, loss is 0.24962535500526428\n",
      "epoch: 262 step: 1, loss is 0.24961405992507935\n",
      "epoch: 263 step: 1, loss is 0.2496025711297989\n",
      "epoch: 264 step: 1, loss is 0.24959084391593933\n",
      "epoch: 265 step: 1, loss is 0.24957886338233948\n",
      "epoch: 266 step: 1, loss is 0.24956664443016052\n",
      "epoch: 267 step: 1, loss is 0.24955418705940247\n",
      "epoch: 268 step: 1, loss is 0.24954146146774292\n",
      "epoch: 269 step: 1, loss is 0.24952851235866547\n",
      "epoch: 270 step: 1, loss is 0.2495153248310089\n",
      "epoch: 271 step: 1, loss is 0.24950186908245087\n",
      "epoch: 272 step: 1, loss is 0.24948817491531372\n",
      "epoch: 273 step: 1, loss is 0.2494741976261139\n",
      "epoch: 274 step: 1, loss is 0.24945996701717377\n",
      "epoch: 275 step: 1, loss is 0.24944552779197693\n",
      "epoch: 276 step: 1, loss is 0.2494307905435562\n",
      "epoch: 277 step: 1, loss is 0.24941575527191162\n",
      "epoch: 278 step: 1, loss is 0.24940051138401031\n",
      "epoch: 279 step: 1, loss is 0.24938496947288513\n",
      "epoch: 280 step: 1, loss is 0.24936920404434204\n",
      "epoch: 281 step: 1, loss is 0.2493530809879303\n",
      "epoch: 282 step: 1, loss is 0.24933677911758423\n",
      "epoch: 283 step: 1, loss is 0.2493201643228531\n",
      "epoch: 284 step: 1, loss is 0.24930326640605927\n",
      "epoch: 285 step: 1, loss is 0.24928608536720276\n",
      "epoch: 286 step: 1, loss is 0.24926865100860596\n",
      "epoch: 287 step: 1, loss is 0.24925093352794647\n",
      "epoch: 288 step: 1, loss is 0.2492329180240631\n",
      "epoch: 289 step: 1, loss is 0.24921463429927826\n",
      "epoch: 290 step: 1, loss is 0.24919603765010834\n",
      "epoch: 291 step: 1, loss is 0.24917720258235931\n",
      "epoch: 292 step: 1, loss is 0.24915802478790283\n",
      "epoch: 293 step: 1, loss is 0.24913857877254486\n",
      "epoch: 294 step: 1, loss is 0.2491188496351242\n",
      "epoch: 295 step: 1, loss is 0.24909883737564087\n",
      "epoch: 296 step: 1, loss is 0.24907849729061127\n",
      "epoch: 297 step: 1, loss is 0.24905790388584137\n",
      "epoch: 298 step: 1, loss is 0.2490369826555252\n",
      "epoch: 299 step: 1, loss is 0.24901577830314636\n",
      "epoch: 300 step: 1, loss is 0.24899426102638245\n",
      "epoch: 301 step: 1, loss is 0.24897243082523346\n",
      "epoch: 302 step: 1, loss is 0.24895034730434418\n",
      "epoch: 303 step: 1, loss is 0.24892790615558624\n",
      "epoch: 304 step: 1, loss is 0.24890518188476562\n",
      "epoch: 305 step: 1, loss is 0.24888217449188232\n",
      "epoch: 306 step: 1, loss is 0.24885880947113037\n",
      "epoch: 307 step: 1, loss is 0.24883516132831573\n",
      "epoch: 308 step: 1, loss is 0.24881121516227722\n",
      "epoch: 309 step: 1, loss is 0.24878692626953125\n",
      "epoch: 310 step: 1, loss is 0.2487623691558838\n",
      "epoch: 311 step: 1, loss is 0.24873749911785126\n",
      "epoch: 312 step: 1, loss is 0.24871228635311127\n",
      "epoch: 313 step: 1, loss is 0.24868673086166382\n",
      "epoch: 314 step: 1, loss is 0.24866092205047607\n",
      "epoch: 315 step: 1, loss is 0.24863475561141968\n",
      "epoch: 316 step: 1, loss is 0.2486083060503006\n",
      "epoch: 317 step: 1, loss is 0.24858148396015167\n",
      "epoch: 318 step: 1, loss is 0.24855440855026245\n",
      "epoch: 319 step: 1, loss is 0.24852696061134338\n",
      "epoch: 320 step: 1, loss is 0.24849919974803925\n",
      "epoch: 321 step: 1, loss is 0.24847112596035004\n",
      "epoch: 322 step: 1, loss is 0.24844273924827576\n",
      "epoch: 323 step: 1, loss is 0.24841400980949402\n",
      "epoch: 324 step: 1, loss is 0.2483849674463272\n",
      "epoch: 325 step: 1, loss is 0.24835559725761414\n",
      "epoch: 326 step: 1, loss is 0.2483258843421936\n",
      "epoch: 327 step: 1, loss is 0.248295858502388\n",
      "epoch: 328 step: 1, loss is 0.24826548993587494\n",
      "epoch: 329 step: 1, loss is 0.24823477864265442\n",
      "epoch: 330 step: 1, loss is 0.24820378422737122\n",
      "epoch: 331 step: 1, loss is 0.24817244708538055\n",
      "epoch: 332 step: 1, loss is 0.24814075231552124\n",
      "epoch: 333 step: 1, loss is 0.24810875952243805\n",
      "epoch: 334 step: 1, loss is 0.2480764240026474\n",
      "epoch: 335 step: 1, loss is 0.2480437457561493\n",
      "epoch: 336 step: 1, loss is 0.2480107545852661\n",
      "epoch: 337 step: 1, loss is 0.2479773759841919\n",
      "epoch: 338 step: 1, loss is 0.2479437291622162\n",
      "epoch: 339 step: 1, loss is 0.2479097545146942\n",
      "epoch: 340 step: 1, loss is 0.24787534773349762\n",
      "epoch: 341 step: 1, loss is 0.24784068763256073\n",
      "epoch: 342 step: 1, loss is 0.24780568480491638\n",
      "epoch: 343 step: 1, loss is 0.24777032434940338\n",
      "epoch: 344 step: 1, loss is 0.24773463606834412\n",
      "epoch: 345 step: 1, loss is 0.24769863486289978\n",
      "epoch: 346 step: 1, loss is 0.2476622611284256\n",
      "epoch: 347 step: 1, loss is 0.24762555956840515\n",
      "epoch: 348 step: 1, loss is 0.24758853018283844\n",
      "epoch: 349 step: 1, loss is 0.24755117297172546\n",
      "epoch: 350 step: 1, loss is 0.24751344323158264\n",
      "epoch: 351 step: 1, loss is 0.24747538566589355\n",
      "epoch: 352 step: 1, loss is 0.2474370151758194\n",
      "epoch: 353 step: 1, loss is 0.2473982572555542\n",
      "epoch: 354 step: 1, loss is 0.24735921621322632\n",
      "epoch: 355 step: 1, loss is 0.2473197877407074\n",
      "epoch: 356 step: 1, loss is 0.2472800612449646\n",
      "epoch: 357 step: 1, loss is 0.24723993241786957\n",
      "epoch: 358 step: 1, loss is 0.24719953536987305\n",
      "epoch: 359 step: 1, loss is 0.2471587359905243\n",
      "epoch: 360 step: 1, loss is 0.24711763858795166\n",
      "epoch: 361 step: 1, loss is 0.24707618355751038\n",
      "epoch: 362 step: 1, loss is 0.24703438580036163\n",
      "epoch: 363 step: 1, loss is 0.246992290019989\n",
      "epoch: 364 step: 1, loss is 0.24694979190826416\n",
      "epoch: 365 step: 1, loss is 0.24690698087215424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 366 step: 1, loss is 0.24686381220817566\n",
      "epoch: 367 step: 1, loss is 0.2468203604221344\n",
      "epoch: 368 step: 1, loss is 0.2467764914035797\n",
      "epoch: 369 step: 1, loss is 0.24673233926296234\n",
      "epoch: 370 step: 1, loss is 0.24668781459331512\n",
      "epoch: 371 step: 1, loss is 0.24664294719696045\n",
      "epoch: 372 step: 1, loss is 0.2465977519750595\n",
      "epoch: 373 step: 1, loss is 0.2465522289276123\n",
      "epoch: 374 step: 1, loss is 0.24650631844997406\n",
      "epoch: 375 step: 1, loss is 0.24646012485027313\n",
      "epoch: 376 step: 1, loss is 0.24641355872154236\n",
      "epoch: 377 step: 1, loss is 0.2463666796684265\n",
      "epoch: 378 step: 1, loss is 0.24631941318511963\n",
      "epoch: 379 step: 1, loss is 0.24627184867858887\n",
      "epoch: 380 step: 1, loss is 0.24622392654418945\n",
      "epoch: 381 step: 1, loss is 0.24617567658424377\n",
      "epoch: 382 step: 1, loss is 0.24612709879875183\n",
      "epoch: 383 step: 1, loss is 0.24607816338539124\n",
      "epoch: 384 step: 1, loss is 0.246028870344162\n",
      "epoch: 385 step: 1, loss is 0.24597924947738647\n",
      "epoch: 386 step: 1, loss is 0.2459293156862259\n",
      "epoch: 387 step: 1, loss is 0.24587900936603546\n",
      "epoch: 388 step: 1, loss is 0.24582840502262115\n",
      "epoch: 389 step: 1, loss is 0.245777428150177\n",
      "epoch: 390 step: 1, loss is 0.24572612345218658\n",
      "epoch: 391 step: 1, loss is 0.2456744760274887\n",
      "epoch: 392 step: 1, loss is 0.24562253057956696\n",
      "epoch: 393 step: 1, loss is 0.24557019770145416\n",
      "epoch: 394 step: 1, loss is 0.2455175668001175\n",
      "epoch: 395 step: 1, loss is 0.24546459317207336\n",
      "epoch: 396 step: 1, loss is 0.24541127681732178\n",
      "epoch: 397 step: 1, loss is 0.24535761773586273\n",
      "epoch: 398 step: 1, loss is 0.24530363082885742\n",
      "epoch: 399 step: 1, loss is 0.24524930119514465\n",
      "epoch: 400 step: 1, loss is 0.24519465863704681\n",
      "epoch: 401 step: 1, loss is 0.2451397031545639\n",
      "epoch: 402 step: 1, loss is 0.24508436024188995\n",
      "epoch: 403 step: 1, loss is 0.24502870440483093\n",
      "epoch: 404 step: 1, loss is 0.24497273564338684\n",
      "epoch: 405 step: 1, loss is 0.24491645395755768\n",
      "epoch: 406 step: 1, loss is 0.24485978484153748\n",
      "epoch: 407 step: 1, loss is 0.244802787899971\n",
      "epoch: 408 step: 1, loss is 0.24474547803401947\n",
      "epoch: 409 step: 1, loss is 0.24468784034252167\n",
      "epoch: 410 step: 1, loss is 0.2446298897266388\n",
      "epoch: 411 step: 1, loss is 0.24457159638404846\n",
      "epoch: 412 step: 1, loss is 0.24451297521591187\n",
      "epoch: 413 step: 1, loss is 0.24445399641990662\n",
      "epoch: 414 step: 1, loss is 0.2443947196006775\n",
      "epoch: 415 step: 1, loss is 0.2443351149559021\n",
      "epoch: 416 step: 1, loss is 0.24427518248558044\n",
      "epoch: 417 step: 1, loss is 0.24421490728855133\n",
      "epoch: 418 step: 1, loss is 0.24415431916713715\n",
      "epoch: 419 step: 1, loss is 0.2440934181213379\n",
      "epoch: 420 step: 1, loss is 0.24403218924999237\n",
      "epoch: 421 step: 1, loss is 0.24397063255310059\n",
      "epoch: 422 step: 1, loss is 0.24390873312950134\n",
      "epoch: 423 step: 1, loss is 0.24384653568267822\n",
      "epoch: 424 step: 1, loss is 0.24378401041030884\n",
      "epoch: 425 step: 1, loss is 0.2437211275100708\n",
      "epoch: 426 step: 1, loss is 0.2436579465866089\n",
      "epoch: 427 step: 1, loss is 0.24359449744224548\n",
      "epoch: 428 step: 1, loss is 0.24353063106536865\n",
      "epoch: 429 step: 1, loss is 0.24346652626991272\n",
      "epoch: 430 step: 1, loss is 0.24340207874774933\n",
      "epoch: 431 step: 1, loss is 0.24333730340003967\n",
      "epoch: 432 step: 1, loss is 0.24327218532562256\n",
      "epoch: 433 step: 1, loss is 0.24320678412914276\n",
      "epoch: 434 step: 1, loss is 0.2431410551071167\n",
      "epoch: 435 step: 1, loss is 0.24307501316070557\n",
      "epoch: 436 step: 1, loss is 0.24300861358642578\n",
      "epoch: 437 step: 1, loss is 0.2429419457912445\n",
      "epoch: 438 step: 1, loss is 0.24287497997283936\n",
      "epoch: 439 step: 1, loss is 0.24280767142772675\n",
      "epoch: 440 step: 1, loss is 0.24274004995822906\n",
      "epoch: 441 step: 1, loss is 0.24267208576202393\n",
      "epoch: 442 step: 1, loss is 0.2426038384437561\n",
      "epoch: 443 step: 1, loss is 0.24253524839878082\n",
      "epoch: 444 step: 1, loss is 0.24246640503406525\n",
      "epoch: 445 step: 1, loss is 0.24239720404148102\n",
      "epoch: 446 step: 1, loss is 0.2423277199268341\n",
      "epoch: 447 step: 1, loss is 0.24225789308547974\n",
      "epoch: 448 step: 1, loss is 0.2421877533197403\n",
      "epoch: 449 step: 1, loss is 0.24211736023426056\n",
      "epoch: 450 step: 1, loss is 0.24204657971858978\n",
      "epoch: 451 step: 1, loss is 0.2419755458831787\n",
      "epoch: 452 step: 1, loss is 0.24190419912338257\n",
      "epoch: 453 step: 1, loss is 0.24183249473571777\n",
      "epoch: 454 step: 1, loss is 0.2417605221271515\n",
      "epoch: 455 step: 1, loss is 0.24168823659420013\n",
      "epoch: 456 step: 1, loss is 0.2416156530380249\n",
      "epoch: 457 step: 1, loss is 0.2415427267551422\n",
      "epoch: 458 step: 1, loss is 0.24146954715251923\n",
      "epoch: 459 step: 1, loss is 0.24139602482318878\n",
      "epoch: 460 step: 1, loss is 0.24132223427295685\n",
      "epoch: 461 step: 1, loss is 0.24124810099601746\n",
      "epoch: 462 step: 1, loss is 0.241173654794693\n",
      "epoch: 463 step: 1, loss is 0.24109894037246704\n",
      "epoch: 464 step: 1, loss is 0.24102389812469482\n",
      "epoch: 465 step: 1, loss is 0.2409486025571823\n",
      "epoch: 466 step: 1, loss is 0.24087293446063995\n",
      "epoch: 467 step: 1, loss is 0.2407969832420349\n",
      "epoch: 468 step: 1, loss is 0.2407207041978836\n",
      "epoch: 469 step: 1, loss is 0.2406441867351532\n",
      "epoch: 470 step: 1, loss is 0.24056732654571533\n",
      "epoch: 471 step: 1, loss is 0.24049018323421478\n",
      "epoch: 472 step: 1, loss is 0.24041268229484558\n",
      "epoch: 473 step: 1, loss is 0.24033494293689728\n",
      "epoch: 474 step: 1, loss is 0.24025686085224152\n",
      "epoch: 475 step: 1, loss is 0.24017851054668427\n",
      "epoch: 476 step: 1, loss is 0.24009984731674194\n",
      "epoch: 477 step: 1, loss is 0.24002088606357574\n",
      "epoch: 478 step: 1, loss is 0.23994162678718567\n",
      "epoch: 479 step: 1, loss is 0.23986205458641052\n",
      "epoch: 480 step: 1, loss is 0.2397821843624115\n",
      "epoch: 481 step: 1, loss is 0.2397020012140274\n",
      "epoch: 482 step: 1, loss is 0.23962152004241943\n",
      "epoch: 483 step: 1, loss is 0.23954074084758759\n",
      "epoch: 484 step: 1, loss is 0.23945967853069305\n",
      "epoch: 485 step: 1, loss is 0.23937827348709106\n",
      "epoch: 486 step: 1, loss is 0.23929661512374878\n",
      "epoch: 487 step: 1, loss is 0.23921462893486023\n",
      "epoch: 488 step: 1, loss is 0.23913231492042542\n",
      "epoch: 489 step: 1, loss is 0.2390497624874115\n",
      "epoch: 490 step: 1, loss is 0.23896685242652893\n",
      "epoch: 491 step: 1, loss is 0.23888367414474487\n",
      "epoch: 492 step: 1, loss is 0.23880016803741455\n",
      "epoch: 493 step: 1, loss is 0.23871636390686035\n",
      "epoch: 494 step: 1, loss is 0.23863227665424347\n",
      "epoch: 495 step: 1, loss is 0.23854786157608032\n",
      "epoch: 496 step: 1, loss is 0.2384631633758545\n",
      "epoch: 497 step: 1, loss is 0.2383781224489212\n",
      "epoch: 498 step: 1, loss is 0.23829282820224762\n",
      "epoch: 499 step: 1, loss is 0.23820719122886658\n",
      "epoch: 500 step: 1, loss is 0.23812124133110046\n",
      "epoch: 501 step: 1, loss is 0.23803500831127167\n",
      "epoch: 502 step: 1, loss is 0.2379484474658966\n",
      "epoch: 503 step: 1, loss is 0.23786158859729767\n",
      "epoch: 504 step: 1, loss is 0.23777443170547485\n",
      "epoch: 505 step: 1, loss is 0.23768693208694458\n",
      "epoch: 506 step: 1, loss is 0.237599179148674\n",
      "epoch: 507 step: 1, loss is 0.23751108348369598\n",
      "epoch: 508 step: 1, loss is 0.23742267489433289\n",
      "epoch: 509 step: 1, loss is 0.23733395338058472\n",
      "epoch: 510 step: 1, loss is 0.23724494874477386\n",
      "epoch: 511 step: 1, loss is 0.23715558648109436\n",
      "epoch: 512 step: 1, loss is 0.23706594109535217\n",
      "epoch: 513 step: 1, loss is 0.23697596788406372\n",
      "epoch: 514 step: 1, loss is 0.2368856966495514\n",
      "epoch: 515 step: 1, loss is 0.2367950975894928\n",
      "epoch: 516 step: 1, loss is 0.23670417070388794\n",
      "epoch: 517 step: 1, loss is 0.2366129755973816\n",
      "epoch: 518 step: 1, loss is 0.2365214079618454\n",
      "epoch: 519 step: 1, loss is 0.2364295870065689\n",
      "epoch: 520 step: 1, loss is 0.23633740842342377\n",
      "epoch: 521 step: 1, loss is 0.23624491691589355\n",
      "epoch: 522 step: 1, loss is 0.23615212738513947\n",
      "epoch: 523 step: 1, loss is 0.2360590249300003\n",
      "epoch: 524 step: 1, loss is 0.2359655648469925\n",
      "epoch: 525 step: 1, loss is 0.235871821641922\n",
      "epoch: 526 step: 1, loss is 0.23577775061130524\n",
      "epoch: 527 step: 1, loss is 0.2356833517551422\n",
      "epoch: 528 step: 1, loss is 0.2355886846780777\n",
      "epoch: 529 step: 1, loss is 0.23549367487430573\n",
      "epoch: 530 step: 1, loss is 0.23539836704730988\n",
      "epoch: 531 step: 1, loss is 0.23530271649360657\n",
      "epoch: 532 step: 1, loss is 0.23520681262016296\n",
      "epoch: 533 step: 1, loss is 0.2351105660200119\n",
      "epoch: 534 step: 1, loss is 0.23501399159431458\n",
      "epoch: 535 step: 1, loss is 0.23491714894771576\n",
      "epoch: 536 step: 1, loss is 0.23481997847557068\n",
      "epoch: 537 step: 1, loss is 0.2347225397825241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 538 step: 1, loss is 0.23462477326393127\n",
      "epoch: 539 step: 1, loss is 0.23452669382095337\n",
      "epoch: 540 step: 1, loss is 0.2344283163547516\n",
      "epoch: 541 step: 1, loss is 0.23432967066764832\n",
      "epoch: 542 step: 1, loss is 0.23423074185848236\n",
      "epoch: 543 step: 1, loss is 0.23413145542144775\n",
      "epoch: 544 step: 1, loss is 0.23403193056583405\n",
      "epoch: 545 step: 1, loss is 0.23393210768699646\n",
      "epoch: 546 step: 1, loss is 0.2338319718837738\n",
      "epoch: 547 step: 1, loss is 0.23373159766197205\n",
      "epoch: 548 step: 1, loss is 0.2336309254169464\n",
      "epoch: 549 step: 1, loss is 0.2335299402475357\n",
      "epoch: 550 step: 1, loss is 0.2334287166595459\n",
      "epoch: 551 step: 1, loss is 0.2333272099494934\n",
      "epoch: 552 step: 1, loss is 0.23322543501853943\n",
      "epoch: 553 step: 1, loss is 0.23312337696552277\n",
      "epoch: 554 step: 1, loss is 0.23302102088928223\n",
      "epoch: 555 step: 1, loss is 0.2329184114933014\n",
      "epoch: 556 step: 1, loss is 0.23281557857990265\n",
      "epoch: 557 step: 1, loss is 0.23271243274211884\n",
      "epoch: 558 step: 1, loss is 0.23260904848575592\n",
      "epoch: 559 step: 1, loss is 0.2325054109096527\n",
      "epoch: 560 step: 1, loss is 0.23240147531032562\n",
      "epoch: 561 step: 1, loss is 0.23229731619358063\n",
      "epoch: 562 step: 1, loss is 0.23219290375709534\n",
      "epoch: 563 step: 1, loss is 0.23208823800086975\n",
      "epoch: 564 step: 1, loss is 0.23198331892490387\n",
      "epoch: 565 step: 1, loss is 0.23187817633152008\n",
      "epoch: 566 step: 1, loss is 0.23177273571491241\n",
      "epoch: 567 step: 1, loss is 0.23166710138320923\n",
      "epoch: 568 step: 1, loss is 0.23156119883060455\n",
      "epoch: 569 step: 1, loss is 0.23145507276058197\n",
      "epoch: 570 step: 1, loss is 0.2313486933708191\n",
      "epoch: 571 step: 1, loss is 0.2312420904636383\n",
      "epoch: 572 step: 1, loss is 0.23113521933555603\n",
      "epoch: 573 step: 1, loss is 0.23102818429470062\n",
      "epoch: 574 step: 1, loss is 0.23092086613178253\n",
      "epoch: 575 step: 1, loss is 0.23081335425376892\n",
      "epoch: 576 step: 1, loss is 0.23070558905601501\n",
      "epoch: 577 step: 1, loss is 0.2305976152420044\n",
      "epoch: 578 step: 1, loss is 0.23048943281173706\n",
      "epoch: 579 step: 1, loss is 0.23038096725940704\n",
      "epoch: 580 step: 1, loss is 0.23027236759662628\n",
      "epoch: 581 step: 1, loss is 0.23016351461410522\n",
      "epoch: 582 step: 1, loss is 0.23005445301532745\n",
      "epoch: 583 step: 1, loss is 0.22994515299797058\n",
      "epoch: 584 step: 1, loss is 0.2298356592655182\n",
      "epoch: 585 step: 1, loss is 0.22972595691680908\n",
      "epoch: 586 step: 1, loss is 0.22961607575416565\n",
      "epoch: 587 step: 1, loss is 0.22950595617294312\n",
      "epoch: 588 step: 1, loss is 0.22939564287662506\n",
      "epoch: 589 step: 1, loss is 0.2292851209640503\n",
      "epoch: 590 step: 1, loss is 0.2291743904352188\n",
      "epoch: 591 step: 1, loss is 0.2290634959936142\n",
      "epoch: 592 step: 1, loss is 0.22895239293575287\n",
      "epoch: 593 step: 1, loss is 0.22884108126163483\n",
      "epoch: 594 step: 1, loss is 0.22872957587242126\n",
      "epoch: 595 step: 1, loss is 0.22861789166927338\n",
      "epoch: 596 step: 1, loss is 0.22850598394870758\n",
      "epoch: 597 step: 1, loss is 0.22839391231536865\n",
      "epoch: 598 step: 1, loss is 0.2282816767692566\n",
      "epoch: 599 step: 1, loss is 0.22816920280456543\n",
      "epoch: 600 step: 1, loss is 0.22805657982826233\n",
      "epoch: 601 step: 1, loss is 0.2279437631368637\n",
      "epoch: 602 step: 1, loss is 0.22783076763153076\n",
      "epoch: 603 step: 1, loss is 0.22771760821342468\n",
      "epoch: 604 step: 1, loss is 0.2276042401790619\n",
      "epoch: 605 step: 1, loss is 0.22749066352844238\n",
      "epoch: 606 step: 1, loss is 0.2273769974708557\n",
      "epoch: 607 step: 1, loss is 0.22726312279701233\n",
      "epoch: 608 step: 1, loss is 0.22714905440807343\n",
      "epoch: 609 step: 1, loss is 0.2270348221063614\n",
      "epoch: 610 step: 1, loss is 0.22692042589187622\n",
      "epoch: 611 step: 1, loss is 0.22680586576461792\n",
      "epoch: 612 step: 1, loss is 0.2266911417245865\n",
      "epoch: 613 step: 1, loss is 0.22657623887062073\n",
      "epoch: 614 step: 1, loss is 0.22646117210388184\n",
      "epoch: 615 step: 1, loss is 0.2263459712266922\n",
      "epoch: 616 step: 1, loss is 0.22623057663440704\n",
      "epoch: 617 step: 1, loss is 0.22611504793167114\n",
      "epoch: 618 step: 1, loss is 0.22599931061267853\n",
      "epoch: 619 step: 1, loss is 0.22588343918323517\n",
      "epoch: 620 step: 1, loss is 0.22576743364334106\n",
      "epoch: 621 step: 1, loss is 0.22565123438835144\n",
      "epoch: 622 step: 1, loss is 0.22553488612174988\n",
      "epoch: 623 step: 1, loss is 0.22541843354701996\n",
      "epoch: 624 step: 1, loss is 0.22530175745487213\n",
      "epoch: 625 step: 1, loss is 0.22518496215343475\n",
      "epoch: 626 step: 1, loss is 0.22506804764270782\n",
      "epoch: 627 step: 1, loss is 0.224950909614563\n",
      "epoch: 628 step: 1, loss is 0.22483369708061218\n",
      "epoch: 629 step: 1, loss is 0.22471633553504944\n",
      "epoch: 630 step: 1, loss is 0.2245987504720688\n",
      "epoch: 631 step: 1, loss is 0.22448107600212097\n",
      "epoch: 632 step: 1, loss is 0.2243632674217224\n",
      "epoch: 633 step: 1, loss is 0.22424530982971191\n",
      "epoch: 634 step: 1, loss is 0.2241271734237671\n",
      "epoch: 635 step: 1, loss is 0.2240089476108551\n",
      "epoch: 636 step: 1, loss is 0.22389055788516998\n",
      "epoch: 637 step: 1, loss is 0.22377203404903412\n",
      "epoch: 638 step: 1, loss is 0.22365333139896393\n",
      "epoch: 639 step: 1, loss is 0.22353455424308777\n",
      "epoch: 640 step: 1, loss is 0.22341562807559967\n",
      "epoch: 641 step: 1, loss is 0.22329656779766083\n",
      "epoch: 642 step: 1, loss is 0.22317734360694885\n",
      "epoch: 643 step: 1, loss is 0.22305800020694733\n",
      "epoch: 644 step: 1, loss is 0.22293852269649506\n",
      "epoch: 645 step: 1, loss is 0.22281894087791443\n",
      "epoch: 646 step: 1, loss is 0.22269922494888306\n",
      "epoch: 647 step: 1, loss is 0.22257934510707855\n",
      "epoch: 648 step: 1, loss is 0.2224593460559845\n",
      "epoch: 649 step: 1, loss is 0.2223391979932785\n",
      "epoch: 650 step: 1, loss is 0.22221897542476654\n",
      "epoch: 651 step: 1, loss is 0.22209860384464264\n",
      "epoch: 652 step: 1, loss is 0.2219780832529068\n",
      "epoch: 653 step: 1, loss is 0.22185751795768738\n",
      "epoch: 654 step: 1, loss is 0.22173675894737244\n",
      "epoch: 655 step: 1, loss is 0.22161588072776794\n",
      "epoch: 656 step: 1, loss is 0.2214948832988739\n",
      "epoch: 657 step: 1, loss is 0.22137382626533508\n",
      "epoch: 658 step: 1, loss is 0.22125256061553955\n",
      "epoch: 659 step: 1, loss is 0.22113122045993805\n",
      "epoch: 660 step: 1, loss is 0.221009761095047\n",
      "epoch: 661 step: 1, loss is 0.2208881676197052\n",
      "epoch: 662 step: 1, loss is 0.22076646983623505\n",
      "epoch: 663 step: 1, loss is 0.22064465284347534\n",
      "epoch: 664 step: 1, loss is 0.2205227166414261\n",
      "epoch: 665 step: 1, loss is 0.22040067613124847\n",
      "epoch: 666 step: 1, loss is 0.2202785313129425\n",
      "epoch: 667 step: 1, loss is 0.2201562523841858\n",
      "epoch: 668 step: 1, loss is 0.22003386914730072\n",
      "epoch: 669 step: 1, loss is 0.2199113816022873\n",
      "epoch: 670 step: 1, loss is 0.21978875994682312\n",
      "epoch: 671 step: 1, loss is 0.21966606378555298\n",
      "epoch: 672 step: 1, loss is 0.2195432186126709\n",
      "epoch: 673 step: 1, loss is 0.21942032873630524\n",
      "epoch: 674 step: 1, loss is 0.21929724514484406\n",
      "epoch: 675 step: 1, loss is 0.2191741019487381\n",
      "epoch: 676 step: 1, loss is 0.21905085444450378\n",
      "epoch: 677 step: 1, loss is 0.21892747282981873\n",
      "epoch: 678 step: 1, loss is 0.2188040167093277\n",
      "epoch: 679 step: 1, loss is 0.21868042647838593\n",
      "epoch: 680 step: 1, loss is 0.21855676174163818\n",
      "epoch: 681 step: 1, loss is 0.2184329479932785\n",
      "epoch: 682 step: 1, loss is 0.21830910444259644\n",
      "epoch: 683 step: 1, loss is 0.21818509697914124\n",
      "epoch: 684 step: 1, loss is 0.21806102991104126\n",
      "epoch: 685 step: 1, loss is 0.21793681383132935\n",
      "epoch: 686 step: 1, loss is 0.21781255304813385\n",
      "epoch: 687 step: 1, loss is 0.2176881730556488\n",
      "epoch: 688 step: 1, loss is 0.217563658952713\n",
      "epoch: 689 step: 1, loss is 0.21743905544281006\n",
      "epoch: 690 step: 1, loss is 0.21731439232826233\n",
      "epoch: 691 step: 1, loss is 0.21718962490558624\n",
      "epoch: 692 step: 1, loss is 0.2170647382736206\n",
      "epoch: 693 step: 1, loss is 0.216939777135849\n",
      "epoch: 694 step: 1, loss is 0.21681466698646545\n",
      "epoch: 695 step: 1, loss is 0.21668954193592072\n",
      "epoch: 696 step: 1, loss is 0.21656428277492523\n",
      "epoch: 697 step: 1, loss is 0.2164389193058014\n",
      "epoch: 698 step: 1, loss is 0.21631348133087158\n",
      "epoch: 699 step: 1, loss is 0.21618792414665222\n",
      "epoch: 700 step: 1, loss is 0.21606232225894928\n",
      "epoch: 701 step: 1, loss is 0.2159366011619568\n",
      "epoch: 702 step: 1, loss is 0.21581074595451355\n",
      "epoch: 703 step: 1, loss is 0.21568484604358673\n",
      "epoch: 704 step: 1, loss is 0.21555885672569275\n",
      "epoch: 705 step: 1, loss is 0.2154327929019928\n",
      "epoch: 706 step: 1, loss is 0.2153065949678421\n",
      "epoch: 707 step: 1, loss is 0.21518033742904663\n",
      "epoch: 708 step: 1, loss is 0.21505402028560638\n",
      "epoch: 709 step: 1, loss is 0.2149275839328766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 710 step: 1, loss is 0.21480104327201843\n",
      "epoch: 711 step: 1, loss is 0.2146744430065155\n",
      "epoch: 712 step: 1, loss is 0.21454773843288422\n",
      "epoch: 713 step: 1, loss is 0.21442092955112457\n",
      "epoch: 714 step: 1, loss is 0.21429410576820374\n",
      "epoch: 715 step: 1, loss is 0.21416714787483215\n",
      "epoch: 716 step: 1, loss is 0.2140401005744934\n",
      "epoch: 717 step: 1, loss is 0.2139129638671875\n",
      "epoch: 718 step: 1, loss is 0.21378576755523682\n",
      "epoch: 719 step: 1, loss is 0.21365849673748016\n",
      "epoch: 720 step: 1, loss is 0.21353113651275635\n",
      "epoch: 721 step: 1, loss is 0.21340368688106537\n",
      "epoch: 722 step: 1, loss is 0.21327614784240723\n",
      "epoch: 723 step: 1, loss is 0.2131485641002655\n",
      "epoch: 724 step: 1, loss is 0.21302084624767303\n",
      "epoch: 725 step: 1, loss is 0.2128930687904358\n",
      "epoch: 726 step: 1, loss is 0.21276524662971497\n",
      "epoch: 727 step: 1, loss is 0.21263732016086578\n",
      "epoch: 728 step: 1, loss is 0.21250930428504944\n",
      "epoch: 729 step: 1, loss is 0.21238122880458832\n",
      "epoch: 730 step: 1, loss is 0.21225306391716003\n",
      "epoch: 731 step: 1, loss is 0.21212482452392578\n",
      "epoch: 732 step: 1, loss is 0.21199649572372437\n",
      "epoch: 733 step: 1, loss is 0.21186810731887817\n",
      "epoch: 734 step: 1, loss is 0.211739644408226\n",
      "epoch: 735 step: 1, loss is 0.21161110699176788\n",
      "epoch: 736 step: 1, loss is 0.2114824652671814\n",
      "epoch: 737 step: 1, loss is 0.21135377883911133\n",
      "epoch: 738 step: 1, loss is 0.2112250179052353\n",
      "epoch: 739 step: 1, loss is 0.2110961377620697\n",
      "epoch: 740 step: 1, loss is 0.21096724271774292\n",
      "epoch: 741 step: 1, loss is 0.21083825826644897\n",
      "epoch: 742 step: 1, loss is 0.21070918440818787\n",
      "epoch: 743 step: 1, loss is 0.21058005094528198\n",
      "epoch: 744 step: 1, loss is 0.21045084297657013\n",
      "epoch: 745 step: 1, loss is 0.2103215605020523\n",
      "epoch: 746 step: 1, loss is 0.21019220352172852\n",
      "epoch: 747 step: 1, loss is 0.21006278693675995\n",
      "epoch: 748 step: 1, loss is 0.20993328094482422\n",
      "epoch: 749 step: 1, loss is 0.2098037153482437\n",
      "epoch: 750 step: 1, loss is 0.20967409014701843\n",
      "epoch: 751 step: 1, loss is 0.20954439043998718\n",
      "epoch: 752 step: 1, loss is 0.20941463112831116\n",
      "epoch: 753 step: 1, loss is 0.20928476750850677\n",
      "epoch: 754 step: 1, loss is 0.20915487408638\n",
      "epoch: 755 step: 1, loss is 0.20902489125728607\n",
      "epoch: 756 step: 1, loss is 0.20889484882354736\n",
      "epoch: 757 step: 1, loss is 0.20876473188400269\n",
      "epoch: 758 step: 1, loss is 0.20863454043865204\n",
      "epoch: 759 step: 1, loss is 0.20850428938865662\n",
      "epoch: 760 step: 1, loss is 0.2083739936351776\n",
      "epoch: 761 step: 1, loss is 0.20824360847473145\n",
      "epoch: 762 step: 1, loss is 0.2081131786108017\n",
      "epoch: 763 step: 1, loss is 0.20798267424106598\n",
      "epoch: 764 step: 1, loss is 0.2078520953655243\n",
      "epoch: 765 step: 1, loss is 0.20772147178649902\n",
      "epoch: 766 step: 1, loss is 0.2075907289981842\n",
      "epoch: 767 step: 1, loss is 0.20745998620986938\n",
      "epoch: 768 step: 1, loss is 0.2073291540145874\n",
      "epoch: 769 step: 1, loss is 0.20719823241233826\n",
      "epoch: 770 step: 1, loss is 0.20706728100776672\n",
      "epoch: 771 step: 1, loss is 0.20693625509738922\n",
      "epoch: 772 step: 1, loss is 0.20680515468120575\n",
      "epoch: 773 step: 1, loss is 0.2066740095615387\n",
      "epoch: 774 step: 1, loss is 0.20654280483722687\n",
      "epoch: 775 step: 1, loss is 0.20641154050827026\n",
      "epoch: 776 step: 1, loss is 0.2062802016735077\n",
      "epoch: 777 step: 1, loss is 0.20614880323410034\n",
      "epoch: 778 step: 1, loss is 0.20601733028888702\n",
      "epoch: 779 step: 1, loss is 0.20588579773902893\n",
      "epoch: 780 step: 1, loss is 0.20575422048568726\n",
      "epoch: 781 step: 1, loss is 0.2056225836277008\n",
      "epoch: 782 step: 1, loss is 0.20549088716506958\n",
      "epoch: 783 step: 1, loss is 0.20535916090011597\n",
      "epoch: 784 step: 1, loss is 0.205227330327034\n",
      "epoch: 785 step: 1, loss is 0.20509544014930725\n",
      "epoch: 786 step: 1, loss is 0.20496350526809692\n",
      "epoch: 787 step: 1, loss is 0.20483151078224182\n",
      "epoch: 788 step: 1, loss is 0.20469945669174194\n",
      "epoch: 789 step: 1, loss is 0.2045673429965973\n",
      "epoch: 790 step: 1, loss is 0.20443516969680786\n",
      "epoch: 791 step: 1, loss is 0.20430292189121246\n",
      "epoch: 792 step: 1, loss is 0.20417064428329468\n",
      "epoch: 793 step: 1, loss is 0.20403829216957092\n",
      "epoch: 794 step: 1, loss is 0.20390591025352478\n",
      "epoch: 795 step: 1, loss is 0.20377345383167267\n",
      "epoch: 796 step: 1, loss is 0.20364095270633698\n",
      "epoch: 797 step: 1, loss is 0.20350836217403412\n",
      "epoch: 798 step: 1, loss is 0.20337574183940887\n",
      "epoch: 799 step: 1, loss is 0.20324306190013885\n",
      "epoch: 800 step: 1, loss is 0.20311030745506287\n",
      "epoch: 801 step: 1, loss is 0.2029775083065033\n",
      "epoch: 802 step: 1, loss is 0.20284464955329895\n",
      "epoch: 803 step: 1, loss is 0.20271174609661102\n",
      "epoch: 804 step: 1, loss is 0.20257878303527832\n",
      "epoch: 805 step: 1, loss is 0.20244577527046204\n",
      "epoch: 806 step: 1, loss is 0.20231270790100098\n",
      "epoch: 807 step: 1, loss is 0.20217958092689514\n",
      "epoch: 808 step: 1, loss is 0.20204639434814453\n",
      "epoch: 809 step: 1, loss is 0.20191317796707153\n",
      "epoch: 810 step: 1, loss is 0.20177987217903137\n",
      "epoch: 811 step: 1, loss is 0.20164650678634644\n",
      "epoch: 812 step: 1, loss is 0.2015131413936615\n",
      "epoch: 813 step: 1, loss is 0.2013797014951706\n",
      "epoch: 814 step: 1, loss is 0.2012462019920349\n",
      "epoch: 815 step: 1, loss is 0.20111262798309326\n",
      "epoch: 816 step: 1, loss is 0.20097903907299042\n",
      "epoch: 817 step: 1, loss is 0.2008453607559204\n",
      "epoch: 818 step: 1, loss is 0.2007116675376892\n",
      "epoch: 819 step: 1, loss is 0.20057788491249084\n",
      "epoch: 820 step: 1, loss is 0.2004440724849701\n",
      "epoch: 821 step: 1, loss is 0.20031021535396576\n",
      "epoch: 822 step: 1, loss is 0.20017626881599426\n",
      "epoch: 823 step: 1, loss is 0.20004230737686157\n",
      "epoch: 824 step: 1, loss is 0.1999082714319229\n",
      "epoch: 825 step: 1, loss is 0.19977419078350067\n",
      "epoch: 826 step: 1, loss is 0.19964006543159485\n",
      "epoch: 827 step: 1, loss is 0.19950588047504425\n",
      "epoch: 828 step: 1, loss is 0.19937163591384888\n",
      "epoch: 829 step: 1, loss is 0.19923736155033112\n",
      "epoch: 830 step: 1, loss is 0.19910304248332977\n",
      "epoch: 831 step: 1, loss is 0.19896863400936127\n",
      "epoch: 832 step: 1, loss is 0.19883418083190918\n",
      "epoch: 833 step: 1, loss is 0.1986997425556183\n",
      "epoch: 834 step: 1, loss is 0.19856519997119904\n",
      "epoch: 835 step: 1, loss is 0.19843058288097382\n",
      "epoch: 836 step: 1, loss is 0.1982959508895874\n",
      "epoch: 837 step: 1, loss is 0.1981612890958786\n",
      "epoch: 838 step: 1, loss is 0.19802650809288025\n",
      "epoch: 839 step: 1, loss is 0.1978917419910431\n",
      "epoch: 840 step: 1, loss is 0.19775688648223877\n",
      "epoch: 841 step: 1, loss is 0.19762198626995087\n",
      "epoch: 842 step: 1, loss is 0.19748707115650177\n",
      "epoch: 843 step: 1, loss is 0.1973520964384079\n",
      "epoch: 844 step: 1, loss is 0.19721701741218567\n",
      "epoch: 845 step: 1, loss is 0.19708196818828583\n",
      "epoch: 846 step: 1, loss is 0.19694679975509644\n",
      "epoch: 847 step: 1, loss is 0.19681161642074585\n",
      "epoch: 848 step: 1, loss is 0.19667640328407288\n",
      "epoch: 849 step: 1, loss is 0.19654114544391632\n",
      "epoch: 850 step: 1, loss is 0.1964057832956314\n",
      "epoch: 851 step: 1, loss is 0.1962704211473465\n",
      "epoch: 852 step: 1, loss is 0.19613498449325562\n",
      "epoch: 853 step: 1, loss is 0.19599950313568115\n",
      "epoch: 854 step: 1, loss is 0.19586396217346191\n",
      "epoch: 855 step: 1, loss is 0.1957283914089203\n",
      "epoch: 856 step: 1, loss is 0.19559279084205627\n",
      "epoch: 857 step: 1, loss is 0.1954571008682251\n",
      "epoch: 858 step: 1, loss is 0.19532139599323273\n",
      "epoch: 859 step: 1, loss is 0.1951856017112732\n",
      "epoch: 860 step: 1, loss is 0.19504980742931366\n",
      "epoch: 861 step: 1, loss is 0.19491390883922577\n",
      "epoch: 862 step: 1, loss is 0.19477802515029907\n",
      "epoch: 863 step: 1, loss is 0.1946420669555664\n",
      "epoch: 864 step: 1, loss is 0.19450606405735016\n",
      "epoch: 865 step: 1, loss is 0.19437000155448914\n",
      "epoch: 866 step: 1, loss is 0.19423392415046692\n",
      "epoch: 867 step: 1, loss is 0.19409772753715515\n",
      "epoch: 868 step: 1, loss is 0.19396154582500458\n",
      "epoch: 869 step: 1, loss is 0.19382527470588684\n",
      "epoch: 870 step: 1, loss is 0.1936890035867691\n",
      "epoch: 871 step: 1, loss is 0.1935526728630066\n",
      "epoch: 872 step: 1, loss is 0.19341625273227692\n",
      "epoch: 873 step: 1, loss is 0.19327984750270844\n",
      "epoch: 874 step: 1, loss is 0.1931433528661728\n",
      "epoch: 875 step: 1, loss is 0.19300682842731476\n",
      "epoch: 876 step: 1, loss is 0.19287024438381195\n",
      "epoch: 877 step: 1, loss is 0.19273363053798676\n",
      "epoch: 878 step: 1, loss is 0.19259697198867798\n",
      "epoch: 879 step: 1, loss is 0.19246023893356323\n",
      "epoch: 880 step: 1, loss is 0.1923234462738037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 881 step: 1, loss is 0.19218666851520538\n",
      "epoch: 882 step: 1, loss is 0.1920498013496399\n",
      "epoch: 883 step: 1, loss is 0.1919129192829132\n",
      "epoch: 884 step: 1, loss is 0.19177593290805817\n",
      "epoch: 885 step: 1, loss is 0.19163894653320312\n",
      "epoch: 886 step: 1, loss is 0.19150188565254211\n",
      "epoch: 887 step: 1, loss is 0.1913648098707199\n",
      "epoch: 888 step: 1, loss is 0.19122765958309174\n",
      "epoch: 889 step: 1, loss is 0.19109046459197998\n",
      "epoch: 890 step: 1, loss is 0.19095322489738464\n",
      "epoch: 891 step: 1, loss is 0.19081595540046692\n",
      "epoch: 892 step: 1, loss is 0.19067861139774323\n",
      "epoch: 893 step: 1, loss is 0.19054126739501953\n",
      "epoch: 894 step: 1, loss is 0.19040381908416748\n",
      "epoch: 895 step: 1, loss is 0.19026637077331543\n",
      "epoch: 896 step: 1, loss is 0.1901288628578186\n",
      "epoch: 897 step: 1, loss is 0.1899913251399994\n",
      "epoch: 898 step: 1, loss is 0.189853698015213\n",
      "epoch: 899 step: 1, loss is 0.18971605598926544\n",
      "epoch: 900 step: 1, loss is 0.1895783543586731\n",
      "epoch: 901 step: 1, loss is 0.18944062292575836\n",
      "epoch: 902 step: 1, loss is 0.18930283188819885\n",
      "epoch: 903 step: 1, loss is 0.18916502594947815\n",
      "epoch: 904 step: 1, loss is 0.18902716040611267\n",
      "epoch: 905 step: 1, loss is 0.18888923525810242\n",
      "epoch: 906 step: 1, loss is 0.18875126540660858\n",
      "epoch: 907 step: 1, loss is 0.18861325085163116\n",
      "epoch: 908 step: 1, loss is 0.18847519159317017\n",
      "epoch: 909 step: 1, loss is 0.18833708763122559\n",
      "epoch: 910 step: 1, loss is 0.18819892406463623\n",
      "epoch: 911 step: 1, loss is 0.1880607306957245\n",
      "epoch: 912 step: 1, loss is 0.18792246282100677\n",
      "epoch: 913 step: 1, loss is 0.18778422474861145\n",
      "epoch: 914 step: 1, loss is 0.18764588236808777\n",
      "epoch: 915 step: 1, loss is 0.1875075101852417\n",
      "epoch: 916 step: 1, loss is 0.18736907839775085\n",
      "epoch: 917 step: 1, loss is 0.18723061680793762\n",
      "epoch: 918 step: 1, loss is 0.18709209561347961\n",
      "epoch: 919 step: 1, loss is 0.18695354461669922\n",
      "epoch: 920 step: 1, loss is 0.18681494891643524\n",
      "epoch: 921 step: 1, loss is 0.18667632341384888\n",
      "epoch: 922 step: 1, loss is 0.18653762340545654\n",
      "epoch: 923 step: 1, loss is 0.18639889359474182\n",
      "epoch: 924 step: 1, loss is 0.18626010417938232\n",
      "epoch: 925 step: 1, loss is 0.18612127006053925\n",
      "epoch: 926 step: 1, loss is 0.18598242104053497\n",
      "epoch: 927 step: 1, loss is 0.18584351241588593\n",
      "epoch: 928 step: 1, loss is 0.1857045590877533\n",
      "epoch: 929 step: 1, loss is 0.18556556105613708\n",
      "epoch: 930 step: 1, loss is 0.1854265034198761\n",
      "epoch: 931 step: 1, loss is 0.18528741598129272\n",
      "epoch: 932 step: 1, loss is 0.18514828383922577\n",
      "epoch: 933 step: 1, loss is 0.18500912189483643\n",
      "epoch: 934 step: 1, loss is 0.1848699152469635\n",
      "epoch: 935 step: 1, loss is 0.1847306489944458\n",
      "epoch: 936 step: 1, loss is 0.18459133803844452\n",
      "epoch: 937 step: 1, loss is 0.18445199728012085\n",
      "epoch: 938 step: 1, loss is 0.1843126118183136\n",
      "epoch: 939 step: 1, loss is 0.18417316675186157\n",
      "epoch: 940 step: 1, loss is 0.18403367698192596\n",
      "epoch: 941 step: 1, loss is 0.18389417231082916\n",
      "epoch: 942 step: 1, loss is 0.1837545931339264\n",
      "epoch: 943 step: 1, loss is 0.18361498415470123\n",
      "epoch: 944 step: 1, loss is 0.1834753453731537\n",
      "epoch: 945 step: 1, loss is 0.18333564698696136\n",
      "epoch: 946 step: 1, loss is 0.18319593369960785\n",
      "epoch: 947 step: 1, loss is 0.18305614590644836\n",
      "epoch: 948 step: 1, loss is 0.18291634321212769\n",
      "epoch: 949 step: 1, loss is 0.18277648091316223\n",
      "epoch: 950 step: 1, loss is 0.182636559009552\n",
      "epoch: 951 step: 1, loss is 0.18249662220478058\n",
      "epoch: 952 step: 1, loss is 0.18235662579536438\n",
      "epoch: 953 step: 1, loss is 0.182216614484787\n",
      "epoch: 954 step: 1, loss is 0.18207654356956482\n",
      "epoch: 955 step: 1, loss is 0.18193642795085907\n",
      "epoch: 956 step: 1, loss is 0.18179625272750854\n",
      "epoch: 957 step: 1, loss is 0.18165606260299683\n",
      "epoch: 958 step: 1, loss is 0.18151582777500153\n",
      "epoch: 959 step: 1, loss is 0.18137553334236145\n",
      "epoch: 960 step: 1, loss is 0.18123523890972137\n",
      "epoch: 961 step: 1, loss is 0.18109486997127533\n",
      "epoch: 962 step: 1, loss is 0.1809544712305069\n",
      "epoch: 963 step: 1, loss is 0.18081402778625488\n",
      "epoch: 964 step: 1, loss is 0.18067356944084167\n",
      "epoch: 965 step: 1, loss is 0.1805330365896225\n",
      "epoch: 966 step: 1, loss is 0.18039245903491974\n",
      "epoch: 967 step: 1, loss is 0.1802518665790558\n",
      "epoch: 968 step: 1, loss is 0.18011124432086945\n",
      "epoch: 969 step: 1, loss is 0.17997054755687714\n",
      "epoch: 970 step: 1, loss is 0.17982983589172363\n",
      "epoch: 971 step: 1, loss is 0.17968907952308655\n",
      "epoch: 972 step: 1, loss is 0.1795482635498047\n",
      "epoch: 973 step: 1, loss is 0.17940743267536163\n",
      "epoch: 974 step: 1, loss is 0.1792665719985962\n",
      "epoch: 975 step: 1, loss is 0.17912563681602478\n",
      "epoch: 976 step: 1, loss is 0.17898470163345337\n",
      "epoch: 977 step: 1, loss is 0.1788436770439148\n",
      "epoch: 978 step: 1, loss is 0.17870263755321503\n",
      "epoch: 979 step: 1, loss is 0.17856155335903168\n",
      "epoch: 980 step: 1, loss is 0.17842046916484833\n",
      "epoch: 981 step: 1, loss is 0.178279310464859\n",
      "epoch: 982 step: 1, loss is 0.1781381219625473\n",
      "epoch: 983 step: 1, loss is 0.17799688875675201\n",
      "epoch: 984 step: 1, loss is 0.17785564064979553\n",
      "epoch: 985 step: 1, loss is 0.1777143031358719\n",
      "epoch: 986 step: 1, loss is 0.17757299542427063\n",
      "epoch: 987 step: 1, loss is 0.1774316281080246\n",
      "epoch: 988 step: 1, loss is 0.17729021608829498\n",
      "epoch: 989 step: 1, loss is 0.1771487593650818\n",
      "epoch: 990 step: 1, loss is 0.1770073026418686\n",
      "epoch: 991 step: 1, loss is 0.17686577141284943\n",
      "epoch: 992 step: 1, loss is 0.17672419548034668\n",
      "epoch: 993 step: 1, loss is 0.17658261954784393\n",
      "epoch: 994 step: 1, loss is 0.1764409989118576\n",
      "epoch: 995 step: 1, loss is 0.1762993037700653\n",
      "epoch: 996 step: 1, loss is 0.17615759372711182\n",
      "epoch: 997 step: 1, loss is 0.17601586878299713\n",
      "epoch: 998 step: 1, loss is 0.17587408423423767\n",
      "epoch: 999 step: 1, loss is 0.17573228478431702\n",
      "epoch: 1000 step: 1, loss is 0.1755904108285904\n",
      "epoch: 1001 step: 1, loss is 0.17544855177402496\n",
      "epoch: 1002 step: 1, loss is 0.17530664801597595\n",
      "epoch: 1003 step: 1, loss is 0.17516469955444336\n",
      "epoch: 1004 step: 1, loss is 0.17502273619174957\n",
      "epoch: 1005 step: 1, loss is 0.17488069832324982\n",
      "epoch: 1006 step: 1, loss is 0.17473864555358887\n",
      "epoch: 1007 step: 1, loss is 0.17459657788276672\n",
      "epoch: 1008 step: 1, loss is 0.1744544357061386\n",
      "epoch: 1009 step: 1, loss is 0.1743122935295105\n",
      "epoch: 1010 step: 1, loss is 0.1741701066493988\n",
      "epoch: 1011 step: 1, loss is 0.17402790486812592\n",
      "epoch: 1012 step: 1, loss is 0.17388564348220825\n",
      "epoch: 1013 step: 1, loss is 0.1737433671951294\n",
      "epoch: 1014 step: 1, loss is 0.17360106110572815\n",
      "epoch: 1015 step: 1, loss is 0.17345866560935974\n",
      "epoch: 1016 step: 1, loss is 0.17331631481647491\n",
      "epoch: 1017 step: 1, loss is 0.1731739044189453\n",
      "epoch: 1018 step: 1, loss is 0.17303144931793213\n",
      "epoch: 1019 step: 1, loss is 0.17288897931575775\n",
      "epoch: 1020 step: 1, loss is 0.1727464646100998\n",
      "epoch: 1021 step: 1, loss is 0.17260390520095825\n",
      "epoch: 1022 step: 1, loss is 0.1724613606929779\n",
      "epoch: 1023 step: 1, loss is 0.1723187267780304\n",
      "epoch: 1024 step: 1, loss is 0.17217612266540527\n",
      "epoch: 1025 step: 1, loss is 0.17203345894813538\n",
      "epoch: 1026 step: 1, loss is 0.1718907356262207\n",
      "epoch: 1027 step: 1, loss is 0.17174802720546722\n",
      "epoch: 1028 step: 1, loss is 0.17160527408123016\n",
      "epoch: 1029 step: 1, loss is 0.17146247625350952\n",
      "epoch: 1030 step: 1, loss is 0.17131966352462769\n",
      "epoch: 1031 step: 1, loss is 0.17117683589458466\n",
      "epoch: 1032 step: 1, loss is 0.17103394865989685\n",
      "epoch: 1033 step: 1, loss is 0.17089104652404785\n",
      "epoch: 1034 step: 1, loss is 0.17074811458587646\n",
      "epoch: 1035 step: 1, loss is 0.17060516774654388\n",
      "epoch: 1036 step: 1, loss is 0.17046219110488892\n",
      "epoch: 1037 step: 1, loss is 0.17031915485858917\n",
      "epoch: 1038 step: 1, loss is 0.17017611861228943\n",
      "epoch: 1039 step: 1, loss is 0.1700330376625061\n",
      "epoch: 1040 step: 1, loss is 0.16988994181156158\n",
      "epoch: 1041 step: 1, loss is 0.1697467863559723\n",
      "epoch: 1042 step: 1, loss is 0.16960366070270538\n",
      "epoch: 1043 step: 1, loss is 0.1694604903459549\n",
      "epoch: 1044 step: 1, loss is 0.16931724548339844\n",
      "epoch: 1045 step: 1, loss is 0.16917400062084198\n",
      "epoch: 1046 step: 1, loss is 0.16903072595596313\n",
      "epoch: 1047 step: 1, loss is 0.16888746619224548\n",
      "epoch: 1048 step: 1, loss is 0.16874414682388306\n",
      "epoch: 1049 step: 1, loss is 0.16860082745552063\n",
      "epoch: 1050 step: 1, loss is 0.16845744848251343\n",
      "epoch: 1051 step: 1, loss is 0.16831405460834503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1052 step: 1, loss is 0.16817061603069305\n",
      "epoch: 1053 step: 1, loss is 0.16802720725536346\n",
      "epoch: 1054 step: 1, loss is 0.1678837239742279\n",
      "epoch: 1055 step: 1, loss is 0.16774024069309235\n",
      "epoch: 1056 step: 1, loss is 0.167596697807312\n",
      "epoch: 1057 step: 1, loss is 0.16745316982269287\n",
      "epoch: 1058 step: 1, loss is 0.16730962693691254\n",
      "epoch: 1059 step: 1, loss is 0.16716600954532623\n",
      "epoch: 1060 step: 1, loss is 0.16702236235141754\n",
      "epoch: 1061 step: 1, loss is 0.16687878966331482\n",
      "epoch: 1062 step: 1, loss is 0.16673509776592255\n",
      "epoch: 1063 step: 1, loss is 0.16659143567085266\n",
      "epoch: 1064 step: 1, loss is 0.1664477288722992\n",
      "epoch: 1065 step: 1, loss is 0.16630399227142334\n",
      "epoch: 1066 step: 1, loss is 0.1661602407693863\n",
      "epoch: 1067 step: 1, loss is 0.16601647436618805\n",
      "epoch: 1068 step: 1, loss is 0.16587267816066742\n",
      "epoch: 1069 step: 1, loss is 0.1657288670539856\n",
      "epoch: 1070 step: 1, loss is 0.1655849814414978\n",
      "epoch: 1071 step: 1, loss is 0.1654411405324936\n",
      "epoch: 1072 step: 1, loss is 0.16529728472232819\n",
      "epoch: 1073 step: 1, loss is 0.1651533842086792\n",
      "epoch: 1074 step: 1, loss is 0.16500946879386902\n",
      "epoch: 1075 step: 1, loss is 0.16486550867557526\n",
      "epoch: 1076 step: 1, loss is 0.1647215336561203\n",
      "epoch: 1077 step: 1, loss is 0.16457755863666534\n",
      "epoch: 1078 step: 1, loss is 0.164433553814888\n",
      "epoch: 1079 step: 1, loss is 0.16428950428962708\n",
      "epoch: 1080 step: 1, loss is 0.16414543986320496\n",
      "epoch: 1081 step: 1, loss is 0.16400139033794403\n",
      "epoch: 1082 step: 1, loss is 0.1638573259115219\n",
      "epoch: 1083 step: 1, loss is 0.16371320188045502\n",
      "epoch: 1084 step: 1, loss is 0.16356906294822693\n",
      "epoch: 1085 step: 1, loss is 0.16342495381832123\n",
      "epoch: 1086 step: 1, loss is 0.16328077018260956\n",
      "epoch: 1087 step: 1, loss is 0.16313661634922028\n",
      "epoch: 1088 step: 1, loss is 0.16299240291118622\n",
      "epoch: 1089 step: 1, loss is 0.16284820437431335\n",
      "epoch: 1090 step: 1, loss is 0.16270394623279572\n",
      "epoch: 1091 step: 1, loss is 0.16255968809127808\n",
      "epoch: 1092 step: 1, loss is 0.16241542994976044\n",
      "epoch: 1093 step: 1, loss is 0.1622711420059204\n",
      "epoch: 1094 step: 1, loss is 0.16212685406208038\n",
      "epoch: 1095 step: 1, loss is 0.16198252141475677\n",
      "epoch: 1096 step: 1, loss is 0.16183820366859436\n",
      "epoch: 1097 step: 1, loss is 0.16169385612010956\n",
      "epoch: 1098 step: 1, loss is 0.16154947876930237\n",
      "epoch: 1099 step: 1, loss is 0.16140510141849518\n",
      "epoch: 1100 step: 1, loss is 0.1612606942653656\n",
      "epoch: 1101 step: 1, loss is 0.16111628711223602\n",
      "epoch: 1102 step: 1, loss is 0.16097185015678406\n",
      "epoch: 1103 step: 1, loss is 0.1608274132013321\n",
      "epoch: 1104 step: 1, loss is 0.16068293154239655\n",
      "epoch: 1105 step: 1, loss is 0.1605384647846222\n",
      "epoch: 1106 step: 1, loss is 0.16039395332336426\n",
      "epoch: 1107 step: 1, loss is 0.16024945676326752\n",
      "epoch: 1108 step: 1, loss is 0.16010494530200958\n",
      "epoch: 1109 step: 1, loss is 0.15996041893959045\n",
      "epoch: 1110 step: 1, loss is 0.15981586277484894\n",
      "epoch: 1111 step: 1, loss is 0.15967132151126862\n",
      "epoch: 1112 step: 1, loss is 0.1595267504453659\n",
      "epoch: 1113 step: 1, loss is 0.1593821495771408\n",
      "epoch: 1114 step: 1, loss is 0.1592375636100769\n",
      "epoch: 1115 step: 1, loss is 0.15909293293952942\n",
      "epoch: 1116 step: 1, loss is 0.15894828736782074\n",
      "epoch: 1117 step: 1, loss is 0.15880367159843445\n",
      "epoch: 1118 step: 1, loss is 0.15865899622440338\n",
      "epoch: 1119 step: 1, loss is 0.1585143655538559\n",
      "epoch: 1120 step: 1, loss is 0.15836969017982483\n",
      "epoch: 1121 step: 1, loss is 0.15822497010231018\n",
      "epoch: 1122 step: 1, loss is 0.15808026492595673\n",
      "epoch: 1123 step: 1, loss is 0.15793555974960327\n",
      "epoch: 1124 step: 1, loss is 0.15779083967208862\n",
      "epoch: 1125 step: 1, loss is 0.1576460748910904\n",
      "epoch: 1126 step: 1, loss is 0.15750136971473694\n",
      "epoch: 1127 step: 1, loss is 0.1573566049337387\n",
      "epoch: 1128 step: 1, loss is 0.15721184015274048\n",
      "epoch: 1129 step: 1, loss is 0.15706704556941986\n",
      "epoch: 1130 step: 1, loss is 0.15692226588726044\n",
      "epoch: 1131 step: 1, loss is 0.15677747130393982\n",
      "epoch: 1132 step: 1, loss is 0.15663264691829681\n",
      "epoch: 1133 step: 1, loss is 0.1564878225326538\n",
      "epoch: 1134 step: 1, loss is 0.1563429981470108\n",
      "epoch: 1135 step: 1, loss is 0.1561981737613678\n",
      "epoch: 1136 step: 1, loss is 0.1560533195734024\n",
      "epoch: 1137 step: 1, loss is 0.1559084802865982\n",
      "epoch: 1138 step: 1, loss is 0.15576359629631042\n",
      "epoch: 1139 step: 1, loss is 0.15561871230602264\n",
      "epoch: 1140 step: 1, loss is 0.15547382831573486\n",
      "epoch: 1141 step: 1, loss is 0.15532894432544708\n",
      "epoch: 1142 step: 1, loss is 0.15518403053283691\n",
      "epoch: 1143 step: 1, loss is 0.15503911674022675\n",
      "epoch: 1144 step: 1, loss is 0.15489418804645538\n",
      "epoch: 1145 step: 1, loss is 0.1547493040561676\n",
      "epoch: 1146 step: 1, loss is 0.15460430085659027\n",
      "epoch: 1147 step: 1, loss is 0.1544593721628189\n",
      "epoch: 1148 step: 1, loss is 0.15431442856788635\n",
      "epoch: 1149 step: 1, loss is 0.1541694700717926\n",
      "epoch: 1150 step: 1, loss is 0.15402449667453766\n",
      "epoch: 1151 step: 1, loss is 0.1538795530796051\n",
      "epoch: 1152 step: 1, loss is 0.15373453497886658\n",
      "epoch: 1153 step: 1, loss is 0.15358954668045044\n",
      "epoch: 1154 step: 1, loss is 0.1534445732831955\n",
      "epoch: 1155 step: 1, loss is 0.15329954028129578\n",
      "epoch: 1156 step: 1, loss is 0.15315452218055725\n",
      "epoch: 1157 step: 1, loss is 0.15300950407981873\n",
      "epoch: 1158 step: 1, loss is 0.1528645008802414\n",
      "epoch: 1159 step: 1, loss is 0.15271946787834167\n",
      "epoch: 1160 step: 1, loss is 0.15257443487644196\n",
      "epoch: 1161 step: 1, loss is 0.15242938697338104\n",
      "epoch: 1162 step: 1, loss is 0.15228432416915894\n",
      "epoch: 1163 step: 1, loss is 0.15213927626609802\n",
      "epoch: 1164 step: 1, loss is 0.15199419856071472\n",
      "epoch: 1165 step: 1, loss is 0.1518491506576538\n",
      "epoch: 1166 step: 1, loss is 0.15170405805110931\n",
      "epoch: 1167 step: 1, loss is 0.151558980345726\n",
      "epoch: 1168 step: 1, loss is 0.1514139175415039\n",
      "epoch: 1169 step: 1, loss is 0.15126879513263702\n",
      "epoch: 1170 step: 1, loss is 0.15112370252609253\n",
      "epoch: 1171 step: 1, loss is 0.15097856521606445\n",
      "epoch: 1172 step: 1, loss is 0.15083347260951996\n",
      "epoch: 1173 step: 1, loss is 0.15068835020065308\n",
      "epoch: 1174 step: 1, loss is 0.15054327249526978\n",
      "epoch: 1175 step: 1, loss is 0.1503981202840805\n",
      "epoch: 1176 step: 1, loss is 0.15025298297405243\n",
      "epoch: 1177 step: 1, loss is 0.15010784566402435\n",
      "epoch: 1178 step: 1, loss is 0.14996270835399628\n",
      "epoch: 1179 step: 1, loss is 0.1498175412416458\n",
      "epoch: 1180 step: 1, loss is 0.14967240393161774\n",
      "epoch: 1181 step: 1, loss is 0.14952725172042847\n",
      "epoch: 1182 step: 1, loss is 0.14938205480575562\n",
      "epoch: 1183 step: 1, loss is 0.14923691749572754\n",
      "epoch: 1184 step: 1, loss is 0.14909173548221588\n",
      "epoch: 1185 step: 1, loss is 0.14894653856754303\n",
      "epoch: 1186 step: 1, loss is 0.14880135655403137\n",
      "epoch: 1187 step: 1, loss is 0.14865615963935852\n",
      "epoch: 1188 step: 1, loss is 0.14851097762584686\n",
      "epoch: 1189 step: 1, loss is 0.14836575090885162\n",
      "epoch: 1190 step: 1, loss is 0.14822055399417877\n",
      "epoch: 1191 step: 1, loss is 0.14807537198066711\n",
      "epoch: 1192 step: 1, loss is 0.14793013036251068\n",
      "epoch: 1193 step: 1, loss is 0.14778491854667664\n",
      "epoch: 1194 step: 1, loss is 0.147639662027359\n",
      "epoch: 1195 step: 1, loss is 0.14749445021152496\n",
      "epoch: 1196 step: 1, loss is 0.14734920859336853\n",
      "epoch: 1197 step: 1, loss is 0.1472039520740509\n",
      "epoch: 1198 step: 1, loss is 0.14705872535705566\n",
      "epoch: 1199 step: 1, loss is 0.14691346883773804\n",
      "epoch: 1200 step: 1, loss is 0.14676819741725922\n",
      "epoch: 1201 step: 1, loss is 0.1466229259967804\n",
      "epoch: 1202 step: 1, loss is 0.14647766947746277\n",
      "epoch: 1203 step: 1, loss is 0.14633239805698395\n",
      "epoch: 1204 step: 1, loss is 0.14618709683418274\n",
      "epoch: 1205 step: 1, loss is 0.14604181051254272\n",
      "epoch: 1206 step: 1, loss is 0.1458965241909027\n",
      "epoch: 1207 step: 1, loss is 0.1457512080669403\n",
      "epoch: 1208 step: 1, loss is 0.1456059217453003\n",
      "epoch: 1209 step: 1, loss is 0.1454605907201767\n",
      "epoch: 1210 step: 1, loss is 0.1453152894973755\n",
      "epoch: 1211 step: 1, loss is 0.1451699584722519\n",
      "epoch: 1212 step: 1, loss is 0.1450245976448059\n",
      "epoch: 1213 step: 1, loss is 0.1448792964220047\n",
      "epoch: 1214 step: 1, loss is 0.14473393559455872\n",
      "epoch: 1215 step: 1, loss is 0.14458860456943512\n",
      "epoch: 1216 step: 1, loss is 0.14444322884082794\n",
      "epoch: 1217 step: 1, loss is 0.14429783821105957\n",
      "epoch: 1218 step: 1, loss is 0.1441524773836136\n",
      "epoch: 1219 step: 1, loss is 0.1440071016550064\n",
      "epoch: 1220 step: 1, loss is 0.14386169612407684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1221 step: 1, loss is 0.14371630549430847\n",
      "epoch: 1222 step: 1, loss is 0.1435708999633789\n",
      "epoch: 1223 step: 1, loss is 0.14342547953128815\n",
      "epoch: 1224 step: 1, loss is 0.14328007400035858\n",
      "epoch: 1225 step: 1, loss is 0.14313465356826782\n",
      "epoch: 1226 step: 1, loss is 0.14298921823501587\n",
      "epoch: 1227 step: 1, loss is 0.14284375309944153\n",
      "epoch: 1228 step: 1, loss is 0.14269830286502838\n",
      "epoch: 1229 step: 1, loss is 0.14255285263061523\n",
      "epoch: 1230 step: 1, loss is 0.1424073576927185\n",
      "epoch: 1231 step: 1, loss is 0.14226187765598297\n",
      "epoch: 1232 step: 1, loss is 0.14211638271808624\n",
      "epoch: 1233 step: 1, loss is 0.1419709026813507\n",
      "epoch: 1234 step: 1, loss is 0.1418253779411316\n",
      "epoch: 1235 step: 1, loss is 0.14167988300323486\n",
      "epoch: 1236 step: 1, loss is 0.14153432846069336\n",
      "epoch: 1237 step: 1, loss is 0.14138880372047424\n",
      "epoch: 1238 step: 1, loss is 0.14124324917793274\n",
      "epoch: 1239 step: 1, loss is 0.14109763503074646\n",
      "epoch: 1240 step: 1, loss is 0.14095209538936615\n",
      "epoch: 1241 step: 1, loss is 0.14080651104450226\n",
      "epoch: 1242 step: 1, loss is 0.14066089689731598\n",
      "epoch: 1243 step: 1, loss is 0.1405152976512909\n",
      "epoch: 1244 step: 1, loss is 0.14036966860294342\n",
      "epoch: 1245 step: 1, loss is 0.14022403955459595\n",
      "epoch: 1246 step: 1, loss is 0.1400783807039261\n",
      "epoch: 1247 step: 1, loss is 0.13993272185325623\n",
      "epoch: 1248 step: 1, loss is 0.13978706300258636\n",
      "epoch: 1249 step: 1, loss is 0.13964135944843292\n",
      "epoch: 1250 step: 1, loss is 0.1394956409931183\n",
      "epoch: 1251 step: 1, loss is 0.13934992253780365\n",
      "epoch: 1252 step: 1, loss is 0.13920418918132782\n",
      "epoch: 1253 step: 1, loss is 0.139058455824852\n",
      "epoch: 1254 step: 1, loss is 0.13891270756721497\n",
      "epoch: 1255 step: 1, loss is 0.13876691460609436\n",
      "epoch: 1256 step: 1, loss is 0.13862113654613495\n",
      "epoch: 1257 step: 1, loss is 0.13847532868385315\n",
      "epoch: 1258 step: 1, loss is 0.13832952082157135\n",
      "epoch: 1259 step: 1, loss is 0.13818365335464478\n",
      "epoch: 1260 step: 1, loss is 0.13803783059120178\n",
      "epoch: 1261 step: 1, loss is 0.13789191842079163\n",
      "epoch: 1262 step: 1, loss is 0.13774605095386505\n",
      "epoch: 1263 step: 1, loss is 0.13760018348693848\n",
      "epoch: 1264 step: 1, loss is 0.13745422661304474\n",
      "epoch: 1265 step: 1, loss is 0.1373082995414734\n",
      "epoch: 1266 step: 1, loss is 0.13716232776641846\n",
      "epoch: 1267 step: 1, loss is 0.13701635599136353\n",
      "epoch: 1268 step: 1, loss is 0.1368703544139862\n",
      "epoch: 1269 step: 1, loss is 0.1367243528366089\n",
      "epoch: 1270 step: 1, loss is 0.13657830655574799\n",
      "epoch: 1271 step: 1, loss is 0.1364322304725647\n",
      "epoch: 1272 step: 1, loss is 0.1362861841917038\n",
      "epoch: 1273 step: 1, loss is 0.13614006340503693\n",
      "epoch: 1274 step: 1, loss is 0.13599392771720886\n",
      "epoch: 1275 step: 1, loss is 0.135847806930542\n",
      "epoch: 1276 step: 1, loss is 0.13570164144039154\n",
      "epoch: 1277 step: 1, loss is 0.1355554610490799\n",
      "epoch: 1278 step: 1, loss is 0.13540923595428467\n",
      "epoch: 1279 step: 1, loss is 0.13526302576065063\n",
      "epoch: 1280 step: 1, loss is 0.13511675596237183\n",
      "epoch: 1281 step: 1, loss is 0.1349705010652542\n",
      "epoch: 1282 step: 1, loss is 0.13482418656349182\n",
      "epoch: 1283 step: 1, loss is 0.13467787206172943\n",
      "epoch: 1284 step: 1, loss is 0.13453152775764465\n",
      "epoch: 1285 step: 1, loss is 0.1343851536512375\n",
      "epoch: 1286 step: 1, loss is 0.13423876464366913\n",
      "epoch: 1287 step: 1, loss is 0.134092316031456\n",
      "epoch: 1288 step: 1, loss is 0.13394589722156525\n",
      "epoch: 1289 step: 1, loss is 0.13379940390586853\n",
      "epoch: 1290 step: 1, loss is 0.133652925491333\n",
      "epoch: 1291 step: 1, loss is 0.1335063874721527\n",
      "epoch: 1292 step: 1, loss is 0.13335981965065002\n",
      "epoch: 1293 step: 1, loss is 0.13321325182914734\n",
      "epoch: 1294 step: 1, loss is 0.13306663930416107\n",
      "epoch: 1295 step: 1, loss is 0.1329200267791748\n",
      "epoch: 1296 step: 1, loss is 0.13277332484722137\n",
      "epoch: 1297 step: 1, loss is 0.13262666761875153\n",
      "epoch: 1298 step: 1, loss is 0.13247992098331451\n",
      "epoch: 1299 step: 1, loss is 0.1323331743478775\n",
      "epoch: 1300 step: 1, loss is 0.1321863830089569\n",
      "epoch: 1301 step: 1, loss is 0.13203953206539154\n",
      "epoch: 1302 step: 1, loss is 0.13189272582530975\n",
      "epoch: 1303 step: 1, loss is 0.131745845079422\n",
      "epoch: 1304 step: 1, loss is 0.13159893453121185\n",
      "epoch: 1305 step: 1, loss is 0.13145197927951813\n",
      "epoch: 1306 step: 1, loss is 0.1313050091266632\n",
      "epoch: 1307 step: 1, loss is 0.1311580091714859\n",
      "epoch: 1308 step: 1, loss is 0.131010964512825\n",
      "epoch: 1309 step: 1, loss is 0.13086387515068054\n",
      "epoch: 1310 step: 1, loss is 0.13071678578853607\n",
      "epoch: 1311 step: 1, loss is 0.13056965172290802\n",
      "epoch: 1312 step: 1, loss is 0.13042250275611877\n",
      "epoch: 1313 step: 1, loss is 0.13027527928352356\n",
      "epoch: 1314 step: 1, loss is 0.13012804090976715\n",
      "epoch: 1315 step: 1, loss is 0.12998075783252716\n",
      "epoch: 1316 step: 1, loss is 0.1298334300518036\n",
      "epoch: 1317 step: 1, loss is 0.12968608736991882\n",
      "epoch: 1318 step: 1, loss is 0.1295386701822281\n",
      "epoch: 1319 step: 1, loss is 0.12939128279685974\n",
      "epoch: 1320 step: 1, loss is 0.12924377620220184\n",
      "epoch: 1321 step: 1, loss is 0.12909631431102753\n",
      "epoch: 1322 step: 1, loss is 0.12894876301288605\n",
      "epoch: 1323 step: 1, loss is 0.12880121171474457\n",
      "epoch: 1324 step: 1, loss is 0.12865358591079712\n",
      "epoch: 1325 step: 1, loss is 0.1285059154033661\n",
      "epoch: 1326 step: 1, loss is 0.12835824489593506\n",
      "epoch: 1327 step: 1, loss is 0.12821049988269806\n",
      "epoch: 1328 step: 1, loss is 0.12806272506713867\n",
      "epoch: 1329 step: 1, loss is 0.1279149353504181\n",
      "epoch: 1330 step: 1, loss is 0.12776707112789154\n",
      "epoch: 1331 step: 1, loss is 0.12761914730072021\n",
      "epoch: 1332 step: 1, loss is 0.1274712085723877\n",
      "epoch: 1333 step: 1, loss is 0.1273232400417328\n",
      "epoch: 1334 step: 1, loss is 0.1271752119064331\n",
      "epoch: 1335 step: 1, loss is 0.12702715396881104\n",
      "epoch: 1336 step: 1, loss is 0.1268790364265442\n",
      "epoch: 1337 step: 1, loss is 0.12673088908195496\n",
      "epoch: 1338 step: 1, loss is 0.12658271193504333\n",
      "epoch: 1339 step: 1, loss is 0.12643446028232574\n",
      "epoch: 1340 step: 1, loss is 0.12628617882728577\n",
      "epoch: 1341 step: 1, loss is 0.1261378675699234\n",
      "epoch: 1342 step: 1, loss is 0.12598948180675507\n",
      "epoch: 1343 step: 1, loss is 0.12584108114242554\n",
      "epoch: 1344 step: 1, loss is 0.12569260597229004\n",
      "epoch: 1345 step: 1, loss is 0.12554411590099335\n",
      "epoch: 1346 step: 1, loss is 0.1253955215215683\n",
      "epoch: 1347 step: 1, loss is 0.12524692714214325\n",
      "epoch: 1348 step: 1, loss is 0.12509827315807343\n",
      "epoch: 1349 step: 1, loss is 0.12494956701993942\n",
      "epoch: 1350 step: 1, loss is 0.12480084598064423\n",
      "epoch: 1351 step: 1, loss is 0.12465206533670425\n",
      "epoch: 1352 step: 1, loss is 0.12450321018695831\n",
      "epoch: 1353 step: 1, loss is 0.12435433268547058\n",
      "epoch: 1354 step: 1, loss is 0.12420538067817688\n",
      "epoch: 1355 step: 1, loss is 0.1240563914179802\n",
      "epoch: 1356 step: 1, loss is 0.12390735745429993\n",
      "epoch: 1357 step: 1, loss is 0.12375827878713608\n",
      "epoch: 1358 step: 1, loss is 0.12360915541648865\n",
      "epoch: 1359 step: 1, loss is 0.12345996499061584\n",
      "epoch: 1360 step: 1, loss is 0.12331072986125946\n",
      "epoch: 1361 step: 1, loss is 0.12316145747900009\n",
      "epoch: 1362 step: 1, loss is 0.12301212549209595\n",
      "epoch: 1363 step: 1, loss is 0.12286273390054703\n",
      "epoch: 1364 step: 1, loss is 0.12271331250667572\n",
      "epoch: 1365 step: 1, loss is 0.12256385385990143\n",
      "epoch: 1366 step: 1, loss is 0.12241430580615997\n",
      "epoch: 1367 step: 1, loss is 0.12226472049951553\n",
      "epoch: 1368 step: 1, loss is 0.12211509048938751\n",
      "epoch: 1369 step: 1, loss is 0.1219654232263565\n",
      "epoch: 1370 step: 1, loss is 0.12181568145751953\n",
      "epoch: 1371 step: 1, loss is 0.12166590988636017\n",
      "epoch: 1372 step: 1, loss is 0.12151607125997543\n",
      "epoch: 1373 step: 1, loss is 0.12136621028184891\n",
      "epoch: 1374 step: 1, loss is 0.12121626734733582\n",
      "epoch: 1375 step: 1, loss is 0.12106630206108093\n",
      "epoch: 1376 step: 1, loss is 0.12091625481843948\n",
      "epoch: 1377 step: 1, loss is 0.12076617777347565\n",
      "epoch: 1378 step: 1, loss is 0.12061605602502823\n",
      "epoch: 1379 step: 1, loss is 0.12046587467193604\n",
      "epoch: 1380 step: 1, loss is 0.12031562626361847\n",
      "epoch: 1381 step: 1, loss is 0.1201653778553009\n",
      "epoch: 1382 step: 1, loss is 0.12001505494117737\n",
      "epoch: 1383 step: 1, loss is 0.11986467242240906\n",
      "epoch: 1384 step: 1, loss is 0.11971426010131836\n",
      "epoch: 1385 step: 1, loss is 0.11956380307674408\n",
      "epoch: 1386 step: 1, loss is 0.11941328644752502\n",
      "epoch: 1387 step: 1, loss is 0.1192627102136612\n",
      "epoch: 1388 step: 1, loss is 0.11911213397979736\n",
      "epoch: 1389 step: 1, loss is 0.11896146833896637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1390 step: 1, loss is 0.11881080269813538\n",
      "epoch: 1391 step: 1, loss is 0.11866004019975662\n",
      "epoch: 1392 step: 1, loss is 0.11850928515195847\n",
      "epoch: 1393 step: 1, loss is 0.11835846304893494\n",
      "epoch: 1394 step: 1, loss is 0.11820758879184723\n",
      "epoch: 1395 step: 1, loss is 0.11805671453475952\n",
      "epoch: 1396 step: 1, loss is 0.11790575832128525\n",
      "epoch: 1397 step: 1, loss is 0.11775479465723038\n",
      "epoch: 1398 step: 1, loss is 0.11760376393795013\n",
      "epoch: 1399 step: 1, loss is 0.1174527108669281\n",
      "epoch: 1400 step: 1, loss is 0.11730162054300308\n",
      "epoch: 1401 step: 1, loss is 0.11715049296617508\n",
      "epoch: 1402 step: 1, loss is 0.11699932813644409\n",
      "epoch: 1403 step: 1, loss is 0.11684813350439072\n",
      "epoch: 1404 step: 1, loss is 0.11669690907001495\n",
      "epoch: 1405 step: 1, loss is 0.1165456473827362\n",
      "epoch: 1406 step: 1, loss is 0.11639434844255447\n",
      "epoch: 1407 step: 1, loss is 0.11624303460121155\n",
      "epoch: 1408 step: 1, loss is 0.11609169840812683\n",
      "epoch: 1409 step: 1, loss is 0.11594034731388092\n",
      "epoch: 1410 step: 1, loss is 0.11578896641731262\n",
      "epoch: 1411 step: 1, loss is 0.11563755571842194\n",
      "epoch: 1412 step: 1, loss is 0.11548611521720886\n",
      "epoch: 1413 step: 1, loss is 0.11533467471599579\n",
      "epoch: 1414 step: 1, loss is 0.11518318951129913\n",
      "epoch: 1415 step: 1, loss is 0.11503170430660248\n",
      "epoch: 1416 step: 1, loss is 0.11488020420074463\n",
      "epoch: 1417 step: 1, loss is 0.11472868919372559\n",
      "epoch: 1418 step: 1, loss is 0.11457718163728714\n",
      "epoch: 1419 step: 1, loss is 0.1144256591796875\n",
      "epoch: 1420 step: 1, loss is 0.11427409201860428\n",
      "epoch: 1421 step: 1, loss is 0.11412256211042404\n",
      "epoch: 1422 step: 1, loss is 0.11397100985050201\n",
      "epoch: 1423 step: 1, loss is 0.11381947249174118\n",
      "epoch: 1424 step: 1, loss is 0.11366792023181915\n",
      "epoch: 1425 step: 1, loss is 0.11351637542247772\n",
      "epoch: 1426 step: 1, loss is 0.11336484551429749\n",
      "epoch: 1427 step: 1, loss is 0.11321334540843964\n",
      "epoch: 1428 step: 1, loss is 0.1130618005990982\n",
      "epoch: 1429 step: 1, loss is 0.11291030794382095\n",
      "epoch: 1430 step: 1, loss is 0.1127588152885437\n",
      "epoch: 1431 step: 1, loss is 0.11260738223791122\n",
      "epoch: 1432 step: 1, loss is 0.11245589703321457\n",
      "epoch: 1433 step: 1, loss is 0.1123044565320015\n",
      "epoch: 1434 step: 1, loss is 0.11215309053659439\n",
      "epoch: 1435 step: 1, loss is 0.11200171709060669\n",
      "epoch: 1436 step: 1, loss is 0.11185035109519958\n",
      "epoch: 1437 step: 1, loss is 0.11169904470443726\n",
      "epoch: 1438 step: 1, loss is 0.11154776066541672\n",
      "epoch: 1439 step: 1, loss is 0.11139649897813797\n",
      "epoch: 1440 step: 1, loss is 0.1112452894449234\n",
      "epoch: 1441 step: 1, loss is 0.11109412461519241\n",
      "epoch: 1442 step: 1, loss is 0.1109430268406868\n",
      "epoch: 1443 step: 1, loss is 0.11079196631908417\n",
      "epoch: 1444 step: 1, loss is 0.11064096540212631\n",
      "epoch: 1445 step: 1, loss is 0.11048998683691025\n",
      "epoch: 1446 step: 1, loss is 0.11033909022808075\n",
      "epoch: 1447 step: 1, loss is 0.11018823087215424\n",
      "epoch: 1448 step: 1, loss is 0.11003746837377548\n",
      "epoch: 1449 step: 1, loss is 0.10988673567771912\n",
      "epoch: 1450 step: 1, loss is 0.10973608493804932\n",
      "epoch: 1451 step: 1, loss is 0.10958549380302429\n",
      "epoch: 1452 step: 1, loss is 0.10943498462438583\n",
      "epoch: 1453 step: 1, loss is 0.10928454995155334\n",
      "epoch: 1454 step: 1, loss is 0.10913421213626862\n",
      "epoch: 1455 step: 1, loss is 0.10898392647504807\n",
      "epoch: 1456 step: 1, loss is 0.10883375257253647\n",
      "epoch: 1457 step: 1, loss is 0.10868365317583084\n",
      "epoch: 1458 step: 1, loss is 0.10853365063667297\n",
      "epoch: 1459 step: 1, loss is 0.10838373005390167\n",
      "epoch: 1460 step: 1, loss is 0.10823390632867813\n",
      "epoch: 1461 step: 1, loss is 0.10808420926332474\n",
      "epoch: 1462 step: 1, loss is 0.10793459415435791\n",
      "epoch: 1463 step: 1, loss is 0.10778509080410004\n",
      "epoch: 1464 step: 1, loss is 0.10763567686080933\n",
      "epoch: 1465 step: 1, loss is 0.10748638212680817\n",
      "epoch: 1466 step: 1, loss is 0.10733723640441895\n",
      "epoch: 1467 step: 1, loss is 0.1071881502866745\n",
      "epoch: 1468 step: 1, loss is 0.1070392057299614\n",
      "epoch: 1469 step: 1, loss is 0.10689039528369904\n",
      "epoch: 1470 step: 1, loss is 0.10674167424440384\n",
      "epoch: 1471 step: 1, loss is 0.10659311711788177\n",
      "epoch: 1472 step: 1, loss is 0.10644468665122986\n",
      "epoch: 1473 step: 1, loss is 0.10629638284444809\n",
      "epoch: 1474 step: 1, loss is 0.10614821314811707\n",
      "epoch: 1475 step: 1, loss is 0.10600018501281738\n",
      "epoch: 1476 step: 1, loss is 0.10585230588912964\n",
      "epoch: 1477 step: 1, loss is 0.10570456087589264\n",
      "epoch: 1478 step: 1, loss is 0.10555695742368698\n",
      "epoch: 1479 step: 1, loss is 0.10540948808193207\n",
      "epoch: 1480 step: 1, loss is 0.10526219010353088\n",
      "epoch: 1481 step: 1, loss is 0.10511504113674164\n",
      "epoch: 1482 step: 1, loss is 0.10496805608272552\n",
      "epoch: 1483 step: 1, loss is 0.10482123494148254\n",
      "epoch: 1484 step: 1, loss is 0.10467454791069031\n",
      "epoch: 1485 step: 1, loss is 0.10452805459499359\n",
      "epoch: 1486 step: 1, loss is 0.10438168793916702\n",
      "epoch: 1487 step: 1, loss is 0.10423552244901657\n",
      "epoch: 1488 step: 1, loss is 0.10408953577280045\n",
      "epoch: 1489 step: 1, loss is 0.10394367575645447\n",
      "epoch: 1490 step: 1, loss is 0.10379800945520401\n",
      "epoch: 1491 step: 1, loss is 0.10365252941846848\n",
      "epoch: 1492 step: 1, loss is 0.10350722819566727\n",
      "epoch: 1493 step: 1, loss is 0.10336209833621979\n",
      "epoch: 1494 step: 1, loss is 0.10321713984012604\n",
      "epoch: 1495 step: 1, loss is 0.103072389960289\n",
      "epoch: 1496 step: 1, loss is 0.10292781889438629\n",
      "epoch: 1497 step: 1, loss is 0.10278341174125671\n",
      "epoch: 1498 step: 1, loss is 0.10263922065496445\n",
      "epoch: 1499 step: 1, loss is 0.10249520093202591\n",
      "epoch: 1500 step: 1, loss is 0.10235139727592468\n",
      "epoch: 1501 step: 1, loss is 0.10220779478549957\n",
      "epoch: 1502 step: 1, loss is 0.102064348757267\n",
      "epoch: 1503 step: 1, loss is 0.10192111879587173\n",
      "epoch: 1504 step: 1, loss is 0.10177808254957199\n",
      "epoch: 1505 step: 1, loss is 0.10163524746894836\n",
      "epoch: 1506 step: 1, loss is 0.10149261355400085\n",
      "epoch: 1507 step: 1, loss is 0.10135018825531006\n",
      "epoch: 1508 step: 1, loss is 0.10120797902345657\n",
      "epoch: 1509 step: 1, loss is 0.1010659784078598\n",
      "epoch: 1510 step: 1, loss is 0.10092413425445557\n",
      "epoch: 1511 step: 1, loss is 0.10078255087137222\n",
      "epoch: 1512 step: 1, loss is 0.1006411463022232\n",
      "epoch: 1513 step: 1, loss is 0.10049998015165329\n",
      "epoch: 1514 step: 1, loss is 0.10035901516675949\n",
      "epoch: 1515 step: 1, loss is 0.10021825134754181\n",
      "epoch: 1516 step: 1, loss is 0.10007770359516144\n",
      "epoch: 1517 step: 1, loss is 0.09993739426136017\n",
      "epoch: 1518 step: 1, loss is 0.09979726374149323\n",
      "epoch: 1519 step: 1, loss is 0.09965738654136658\n",
      "epoch: 1520 step: 1, loss is 0.09951768815517426\n",
      "epoch: 1521 step: 1, loss is 0.09937822073698044\n",
      "epoch: 1522 step: 1, loss is 0.09923899173736572\n",
      "epoch: 1523 step: 1, loss is 0.09909993410110474\n",
      "epoch: 1524 step: 1, loss is 0.09896113723516464\n",
      "epoch: 1525 step: 1, loss is 0.09882254898548126\n",
      "epoch: 1526 step: 1, loss is 0.0986841693520546\n",
      "epoch: 1527 step: 1, loss is 0.09854601323604584\n",
      "epoch: 1528 step: 1, loss is 0.09840810298919678\n",
      "epoch: 1529 step: 1, loss is 0.09827039390802383\n",
      "epoch: 1530 step: 1, loss is 0.0981329008936882\n",
      "epoch: 1531 step: 1, loss is 0.09799563139677048\n",
      "epoch: 1532 step: 1, loss is 0.09785860031843185\n",
      "epoch: 1533 step: 1, loss is 0.09772176295518875\n",
      "epoch: 1534 step: 1, loss is 0.09758517891168594\n",
      "epoch: 1535 step: 1, loss is 0.09744880348443985\n",
      "epoch: 1536 step: 1, loss is 0.09731265902519226\n",
      "epoch: 1537 step: 1, loss is 0.09717671573162079\n",
      "epoch: 1538 step: 1, loss is 0.0970410406589508\n",
      "epoch: 1539 step: 1, loss is 0.09690555930137634\n",
      "epoch: 1540 step: 1, loss is 0.09677029401063919\n",
      "epoch: 1541 step: 1, loss is 0.09663526713848114\n",
      "epoch: 1542 step: 1, loss is 0.0965004563331604\n",
      "epoch: 1543 step: 1, loss is 0.09636586159467697\n",
      "epoch: 1544 step: 1, loss is 0.09623148292303085\n",
      "epoch: 1545 step: 1, loss is 0.09609735012054443\n",
      "epoch: 1546 step: 1, loss is 0.09596342593431473\n",
      "epoch: 1547 step: 1, loss is 0.09582972526550293\n",
      "epoch: 1548 step: 1, loss is 0.09569624811410904\n",
      "epoch: 1549 step: 1, loss is 0.09556297957897186\n",
      "epoch: 1550 step: 1, loss is 0.09542995691299438\n",
      "epoch: 1551 step: 1, loss is 0.09529711306095123\n",
      "epoch: 1552 step: 1, loss is 0.09516452997922897\n",
      "epoch: 1553 step: 1, loss is 0.09503215551376343\n",
      "epoch: 1554 step: 1, loss is 0.09490000456571579\n",
      "epoch: 1555 step: 1, loss is 0.09476803243160248\n",
      "epoch: 1556 step: 1, loss is 0.09463632851839066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1557 step: 1, loss is 0.09450481832027435\n",
      "epoch: 1558 step: 1, loss is 0.09437353163957596\n",
      "epoch: 1559 step: 1, loss is 0.09424245357513428\n",
      "epoch: 1560 step: 1, loss is 0.0941116139292717\n",
      "epoch: 1561 step: 1, loss is 0.09398096799850464\n",
      "epoch: 1562 step: 1, loss is 0.09385053813457489\n",
      "epoch: 1563 step: 1, loss is 0.09372033178806305\n",
      "epoch: 1564 step: 1, loss is 0.09359032660722733\n",
      "epoch: 1565 step: 1, loss is 0.09346053749322891\n",
      "epoch: 1566 step: 1, loss is 0.09333096444606781\n",
      "epoch: 1567 step: 1, loss is 0.09320160746574402\n",
      "epoch: 1568 step: 1, loss is 0.09307245165109634\n",
      "epoch: 1569 step: 1, loss is 0.09294350445270538\n",
      "epoch: 1570 step: 1, loss is 0.09281477332115173\n",
      "epoch: 1571 step: 1, loss is 0.0926862359046936\n",
      "epoch: 1572 step: 1, loss is 0.09255792200565338\n",
      "epoch: 1573 step: 1, loss is 0.09242980927228928\n",
      "epoch: 1574 step: 1, loss is 0.09230190515518188\n",
      "epoch: 1575 step: 1, loss is 0.09217419475317001\n",
      "epoch: 1576 step: 1, loss is 0.09204667806625366\n",
      "epoch: 1577 step: 1, loss is 0.09191939234733582\n",
      "epoch: 1578 step: 1, loss is 0.09179230034351349\n",
      "epoch: 1579 step: 1, loss is 0.09166540205478668\n",
      "epoch: 1580 step: 1, loss is 0.09153872728347778\n",
      "epoch: 1581 step: 1, loss is 0.0914122462272644\n",
      "epoch: 1582 step: 1, loss is 0.09128593653440475\n",
      "epoch: 1583 step: 1, loss is 0.09115983545780182\n",
      "epoch: 1584 step: 1, loss is 0.0910339429974556\n",
      "epoch: 1585 step: 1, loss is 0.0909082368016243\n",
      "epoch: 1586 step: 1, loss is 0.09078272432088852\n",
      "epoch: 1587 step: 1, loss is 0.09065741300582886\n",
      "epoch: 1588 step: 1, loss is 0.09053230285644531\n",
      "epoch: 1589 step: 1, loss is 0.0904073640704155\n",
      "epoch: 1590 step: 1, loss is 0.0902826339006424\n",
      "epoch: 1591 step: 1, loss is 0.09015808254480362\n",
      "epoch: 1592 step: 1, loss is 0.09003371745347977\n",
      "epoch: 1593 step: 1, loss is 0.08990956842899323\n",
      "epoch: 1594 step: 1, loss is 0.08978556096553802\n",
      "epoch: 1595 step: 1, loss is 0.08966179192066193\n",
      "epoch: 1596 step: 1, loss is 0.08953817933797836\n",
      "epoch: 1597 step: 1, loss is 0.08941475301980972\n",
      "epoch: 1598 step: 1, loss is 0.0892915204167366\n",
      "epoch: 1599 step: 1, loss is 0.08916845172643661\n",
      "epoch: 1600 step: 1, loss is 0.08904556930065155\n",
      "epoch: 1601 step: 1, loss is 0.0889228880405426\n",
      "epoch: 1602 step: 1, loss is 0.0888003557920456\n",
      "epoch: 1603 step: 1, loss is 0.0886780396103859\n",
      "epoch: 1604 step: 1, loss is 0.08855588734149933\n",
      "epoch: 1605 step: 1, loss is 0.08843390643596649\n",
      "epoch: 1606 step: 1, loss is 0.08831208199262619\n",
      "epoch: 1607 step: 1, loss is 0.08819044381380081\n",
      "epoch: 1608 step: 1, loss is 0.08806899935007095\n",
      "epoch: 1609 step: 1, loss is 0.08794770389795303\n",
      "epoch: 1610 step: 1, loss is 0.08782659471035004\n",
      "epoch: 1611 step: 1, loss is 0.08770565688610077\n",
      "epoch: 1612 step: 1, loss is 0.08758486807346344\n",
      "epoch: 1613 step: 1, loss is 0.08746428042650223\n",
      "epoch: 1614 step: 1, loss is 0.08734384924173355\n",
      "epoch: 1615 step: 1, loss is 0.087223581969738\n",
      "epoch: 1616 step: 1, loss is 0.08710348606109619\n",
      "epoch: 1617 step: 1, loss is 0.08698354661464691\n",
      "epoch: 1618 step: 1, loss is 0.08686379343271255\n",
      "epoch: 1619 step: 1, loss is 0.08674418181180954\n",
      "epoch: 1620 step: 1, loss is 0.08662474900484085\n",
      "epoch: 1621 step: 1, loss is 0.0865054652094841\n",
      "epoch: 1622 step: 1, loss is 0.08638635277748108\n",
      "epoch: 1623 step: 1, loss is 0.08626740425825119\n",
      "epoch: 1624 step: 1, loss is 0.08614860475063324\n",
      "epoch: 1625 step: 1, loss is 0.08602995425462723\n",
      "epoch: 1626 step: 1, loss is 0.08591148257255554\n",
      "epoch: 1627 step: 1, loss is 0.0857931524515152\n",
      "epoch: 1628 step: 1, loss is 0.08567499369382858\n",
      "epoch: 1629 step: 1, loss is 0.08555697649717331\n",
      "epoch: 1630 step: 1, loss is 0.08543913066387177\n",
      "epoch: 1631 step: 1, loss is 0.08532140403985977\n",
      "epoch: 1632 step: 1, loss is 0.08520388603210449\n",
      "epoch: 1633 step: 1, loss is 0.08508647233247757\n",
      "epoch: 1634 step: 1, loss is 0.08496921509504318\n",
      "epoch: 1635 step: 1, loss is 0.08485213667154312\n",
      "epoch: 1636 step: 1, loss is 0.08473518490791321\n",
      "epoch: 1637 step: 1, loss is 0.08461838960647583\n",
      "epoch: 1638 step: 1, loss is 0.08450175821781158\n",
      "epoch: 1639 step: 1, loss is 0.08438525348901749\n",
      "epoch: 1640 step: 1, loss is 0.08426889777183533\n",
      "epoch: 1641 step: 1, loss is 0.0841526910662651\n",
      "epoch: 1642 step: 1, loss is 0.08403662592172623\n",
      "epoch: 1643 step: 1, loss is 0.08392071723937988\n",
      "epoch: 1644 step: 1, loss is 0.08380495756864548\n",
      "epoch: 1645 step: 1, loss is 0.08368929475545883\n",
      "epoch: 1646 step: 1, loss is 0.0835738405585289\n",
      "epoch: 1647 step: 1, loss is 0.08345848321914673\n",
      "epoch: 1648 step: 1, loss is 0.08334328234195709\n",
      "epoch: 1649 step: 1, loss is 0.0832282081246376\n",
      "epoch: 1650 step: 1, loss is 0.08311330527067184\n",
      "epoch: 1651 step: 1, loss is 0.08299851417541504\n",
      "epoch: 1652 step: 1, loss is 0.08288389444351196\n",
      "epoch: 1653 step: 1, loss is 0.08276937901973724\n",
      "epoch: 1654 step: 1, loss is 0.08265502005815506\n",
      "epoch: 1655 step: 1, loss is 0.08254079520702362\n",
      "epoch: 1656 step: 1, loss is 0.08242668211460114\n",
      "epoch: 1657 step: 1, loss is 0.08231276273727417\n",
      "epoch: 1658 step: 1, loss is 0.08219892531633377\n",
      "epoch: 1659 step: 1, loss is 0.08208524435758591\n",
      "epoch: 1660 step: 1, loss is 0.08197169005870819\n",
      "epoch: 1661 step: 1, loss is 0.08185827732086182\n",
      "epoch: 1662 step: 1, loss is 0.08174499124288559\n",
      "epoch: 1663 step: 1, loss is 0.08163183927536011\n",
      "epoch: 1664 step: 1, loss is 0.08151881396770477\n",
      "epoch: 1665 step: 1, loss is 0.08140593767166138\n",
      "epoch: 1666 step: 1, loss is 0.08129316568374634\n",
      "epoch: 1667 step: 1, loss is 0.08118055760860443\n",
      "epoch: 1668 step: 1, loss is 0.08106806129217148\n",
      "epoch: 1669 step: 1, loss is 0.08095569908618927\n",
      "epoch: 1670 step: 1, loss is 0.08084346354007721\n",
      "epoch: 1671 step: 1, loss is 0.0807313621044159\n",
      "epoch: 1672 step: 1, loss is 0.08061938732862473\n",
      "epoch: 1673 step: 1, loss is 0.0805075466632843\n",
      "epoch: 1674 step: 1, loss is 0.08039581775665283\n",
      "epoch: 1675 step: 1, loss is 0.0802842229604721\n",
      "epoch: 1676 step: 1, loss is 0.08017275482416153\n",
      "epoch: 1677 step: 1, loss is 0.0800614133477211\n",
      "epoch: 1678 step: 1, loss is 0.07995020598173141\n",
      "epoch: 1679 step: 1, loss is 0.07983911037445068\n",
      "epoch: 1680 step: 1, loss is 0.0797281414270401\n",
      "epoch: 1681 step: 1, loss is 0.07961730659008026\n",
      "epoch: 1682 step: 1, loss is 0.07950660586357117\n",
      "epoch: 1683 step: 1, loss is 0.07939603179693222\n",
      "epoch: 1684 step: 1, loss is 0.07928555458784103\n",
      "epoch: 1685 step: 1, loss is 0.07917521893978119\n",
      "epoch: 1686 step: 1, loss is 0.0790650025010109\n",
      "epoch: 1687 step: 1, loss is 0.07895489782094955\n",
      "epoch: 1688 step: 1, loss is 0.07884490489959717\n",
      "epoch: 1689 step: 1, loss is 0.07873506844043732\n",
      "epoch: 1690 step: 1, loss is 0.07862535119056702\n",
      "epoch: 1691 step: 1, loss is 0.07851573079824448\n",
      "epoch: 1692 step: 1, loss is 0.07840625941753387\n",
      "epoch: 1693 step: 1, loss is 0.07829690724611282\n",
      "epoch: 1694 step: 1, loss is 0.07818765193223953\n",
      "epoch: 1695 step: 1, loss is 0.07807852327823639\n",
      "epoch: 1696 step: 1, loss is 0.07796954363584518\n",
      "epoch: 1697 step: 1, loss is 0.07786064594984055\n",
      "epoch: 1698 step: 1, loss is 0.07775190472602844\n",
      "epoch: 1699 step: 1, loss is 0.0776432454586029\n",
      "epoch: 1700 step: 1, loss is 0.0775347501039505\n",
      "epoch: 1701 step: 1, loss is 0.07742632180452347\n",
      "epoch: 1702 step: 1, loss is 0.07731804251670837\n",
      "epoch: 1703 step: 1, loss is 0.07720988988876343\n",
      "epoch: 1704 step: 1, loss is 0.07710183411836624\n",
      "epoch: 1705 step: 1, loss is 0.0769939124584198\n",
      "epoch: 1706 step: 1, loss is 0.07688609510660172\n",
      "epoch: 1707 step: 1, loss is 0.07677841931581497\n",
      "epoch: 1708 step: 1, loss is 0.07667084038257599\n",
      "epoch: 1709 step: 1, loss is 0.07656337320804596\n",
      "epoch: 1710 step: 1, loss is 0.07645605504512787\n",
      "epoch: 1711 step: 1, loss is 0.07634882628917694\n",
      "epoch: 1712 step: 1, loss is 0.07624173909425735\n",
      "epoch: 1713 step: 1, loss is 0.07613474130630493\n",
      "epoch: 1714 step: 1, loss is 0.07602787762880325\n",
      "epoch: 1715 step: 1, loss is 0.07592111080884933\n",
      "epoch: 1716 step: 1, loss is 0.07581448554992676\n",
      "epoch: 1717 step: 1, loss is 0.07570797950029373\n",
      "epoch: 1718 step: 1, loss is 0.07560157030820847\n",
      "epoch: 1719 step: 1, loss is 0.07549531012773514\n",
      "epoch: 1720 step: 1, loss is 0.07538911700248718\n",
      "epoch: 1721 step: 1, loss is 0.07528307288885117\n",
      "epoch: 1722 step: 1, loss is 0.07517712563276291\n",
      "epoch: 1723 step: 1, loss is 0.075071319937706\n",
      "epoch: 1724 step: 1, loss is 0.07496561110019684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1725 step: 1, loss is 0.07486002892255783\n",
      "epoch: 1726 step: 1, loss is 0.07475455105304718\n",
      "epoch: 1727 step: 1, loss is 0.07464919239282608\n",
      "epoch: 1728 step: 1, loss is 0.07454395294189453\n",
      "epoch: 1729 step: 1, loss is 0.07443882524967194\n",
      "epoch: 1730 step: 1, loss is 0.07433382421731949\n",
      "epoch: 1731 step: 1, loss is 0.07422894239425659\n",
      "epoch: 1732 step: 1, loss is 0.07412412762641907\n",
      "epoch: 1733 step: 1, loss is 0.07401946932077408\n",
      "epoch: 1734 step: 1, loss is 0.07391493022441864\n",
      "epoch: 1735 step: 1, loss is 0.07381051033735275\n",
      "epoch: 1736 step: 1, loss is 0.07370617985725403\n",
      "epoch: 1737 step: 1, loss is 0.07360197603702545\n",
      "epoch: 1738 step: 1, loss is 0.07349789142608643\n",
      "epoch: 1739 step: 1, loss is 0.07339388877153397\n",
      "epoch: 1740 step: 1, loss is 0.07329005748033524\n",
      "epoch: 1741 step: 1, loss is 0.07318630814552307\n",
      "epoch: 1742 step: 1, loss is 0.07308267802000046\n",
      "epoch: 1743 step: 1, loss is 0.07297918200492859\n",
      "epoch: 1744 step: 1, loss is 0.07287576049566269\n",
      "epoch: 1745 step: 1, loss is 0.07277247309684753\n",
      "epoch: 1746 step: 1, loss is 0.07266931235790253\n",
      "epoch: 1747 step: 1, loss is 0.07256625592708588\n",
      "epoch: 1748 step: 1, loss is 0.07246331870555878\n",
      "epoch: 1749 step: 1, loss is 0.07236049324274063\n",
      "epoch: 1750 step: 1, loss is 0.07225776463747025\n",
      "epoch: 1751 step: 1, loss is 0.0721551775932312\n",
      "epoch: 1752 step: 1, loss is 0.07205270230770111\n",
      "epoch: 1753 step: 1, loss is 0.07195032387971878\n",
      "epoch: 1754 step: 1, loss is 0.07184808701276779\n",
      "epoch: 1755 step: 1, loss is 0.07174594700336456\n",
      "epoch: 1756 step: 1, loss is 0.07164391130208969\n",
      "epoch: 1757 step: 1, loss is 0.07154202461242676\n",
      "epoch: 1758 step: 1, loss is 0.07144022732973099\n",
      "epoch: 1759 step: 1, loss is 0.07133854180574417\n",
      "epoch: 1760 step: 1, loss is 0.0712369829416275\n",
      "epoch: 1761 step: 1, loss is 0.07113553583621979\n",
      "epoch: 1762 step: 1, loss is 0.07103420794010162\n",
      "epoch: 1763 step: 1, loss is 0.07093297690153122\n",
      "epoch: 1764 step: 1, loss is 0.07083187252283096\n",
      "epoch: 1765 step: 1, loss is 0.07073090225458145\n",
      "epoch: 1766 step: 1, loss is 0.0706300139427185\n",
      "epoch: 1767 step: 1, loss is 0.0705292671918869\n",
      "epoch: 1768 step: 1, loss is 0.07042860239744186\n",
      "epoch: 1769 step: 1, loss is 0.07032809406518936\n",
      "epoch: 1770 step: 1, loss is 0.07022766768932343\n",
      "epoch: 1771 step: 1, loss is 0.07012736797332764\n",
      "epoch: 1772 step: 1, loss is 0.0700271874666214\n",
      "epoch: 1773 step: 1, loss is 0.06992710381746292\n",
      "epoch: 1774 step: 1, loss is 0.06982715427875519\n",
      "epoch: 1775 step: 1, loss is 0.0697273388504982\n",
      "epoch: 1776 step: 1, loss is 0.06962760537862778\n",
      "epoch: 1777 step: 1, loss is 0.0695279911160469\n",
      "epoch: 1778 step: 1, loss is 0.06942848861217499\n",
      "epoch: 1779 step: 1, loss is 0.06932912766933441\n",
      "epoch: 1780 step: 1, loss is 0.0692298635840416\n",
      "epoch: 1781 step: 1, loss is 0.06913071870803833\n",
      "epoch: 1782 step: 1, loss is 0.06903168559074402\n",
      "epoch: 1783 step: 1, loss is 0.06893275678157806\n",
      "epoch: 1784 step: 1, loss is 0.06883399188518524\n",
      "epoch: 1785 step: 1, loss is 0.06873529404401779\n",
      "epoch: 1786 step: 1, loss is 0.06863671541213989\n",
      "epoch: 1787 step: 1, loss is 0.06853825598955154\n",
      "epoch: 1788 step: 1, loss is 0.06843993812799454\n",
      "epoch: 1789 step: 1, loss is 0.06834172457456589\n",
      "epoch: 1790 step: 1, loss is 0.0682436153292656\n",
      "epoch: 1791 step: 1, loss is 0.06814564764499664\n",
      "epoch: 1792 step: 1, loss is 0.06804774701595306\n",
      "epoch: 1793 step: 1, loss is 0.06795000284910202\n",
      "epoch: 1794 step: 1, loss is 0.06785236299037933\n",
      "epoch: 1795 step: 1, loss is 0.067754827439785\n",
      "epoch: 1796 step: 1, loss is 0.06765744090080261\n",
      "epoch: 1797 step: 1, loss is 0.06756015121936798\n",
      "epoch: 1798 step: 1, loss is 0.0674629732966423\n",
      "epoch: 1799 step: 1, loss is 0.06736589968204498\n",
      "epoch: 1800 step: 1, loss is 0.067268967628479\n",
      "epoch: 1801 step: 1, loss is 0.06717215478420258\n",
      "epoch: 1802 step: 1, loss is 0.06707543134689331\n",
      "epoch: 1803 step: 1, loss is 0.06697884947061539\n",
      "epoch: 1804 step: 1, loss is 0.06688237190246582\n",
      "epoch: 1805 step: 1, loss is 0.066786028444767\n",
      "epoch: 1806 step: 1, loss is 0.06668977439403534\n",
      "epoch: 1807 step: 1, loss is 0.06659366190433502\n",
      "epoch: 1808 step: 1, loss is 0.06649766862392426\n",
      "epoch: 1809 step: 1, loss is 0.06640177965164185\n",
      "epoch: 1810 step: 1, loss is 0.06630600243806839\n",
      "epoch: 1811 step: 1, loss is 0.06621033698320389\n",
      "epoch: 1812 step: 1, loss is 0.06611481308937073\n",
      "epoch: 1813 step: 1, loss is 0.06601938605308533\n",
      "epoch: 1814 step: 1, loss is 0.06592407822608948\n",
      "epoch: 1815 step: 1, loss is 0.06582888215780258\n",
      "epoch: 1816 step: 1, loss is 0.06573382019996643\n",
      "epoch: 1817 step: 1, loss is 0.06563887000083923\n",
      "epoch: 1818 step: 1, loss is 0.06554403156042099\n",
      "epoch: 1819 step: 1, loss is 0.0654493123292923\n",
      "epoch: 1820 step: 1, loss is 0.06535471230745316\n",
      "epoch: 1821 step: 1, loss is 0.06526023894548416\n",
      "epoch: 1822 step: 1, loss is 0.06516586989164352\n",
      "epoch: 1823 step: 1, loss is 0.06507162004709244\n",
      "epoch: 1824 step: 1, loss is 0.0649774894118309\n",
      "epoch: 1825 step: 1, loss is 0.06488350033760071\n",
      "epoch: 1826 step: 1, loss is 0.06478960067033768\n",
      "epoch: 1827 step: 1, loss is 0.0646958276629448\n",
      "epoch: 1828 step: 1, loss is 0.06460217386484146\n",
      "epoch: 1829 step: 1, loss is 0.06450863927602768\n",
      "epoch: 1830 step: 1, loss is 0.06441522389650345\n",
      "epoch: 1831 step: 1, loss is 0.06432190537452698\n",
      "epoch: 1832 step: 1, loss is 0.06422872841358185\n",
      "epoch: 1833 step: 1, loss is 0.06413567066192627\n",
      "epoch: 1834 step: 1, loss is 0.06404271721839905\n",
      "epoch: 1835 step: 1, loss is 0.06394989788532257\n",
      "epoch: 1836 step: 1, loss is 0.06385717540979385\n",
      "epoch: 1837 step: 1, loss is 0.06376460194587708\n",
      "epoch: 1838 step: 1, loss is 0.06367212533950806\n",
      "epoch: 1839 step: 1, loss is 0.06357976049184799\n",
      "epoch: 1840 step: 1, loss is 0.06348752975463867\n",
      "epoch: 1841 step: 1, loss is 0.0633954256772995\n",
      "epoch: 1842 step: 1, loss is 0.06330342590808868\n",
      "epoch: 1843 step: 1, loss is 0.06321153044700623\n",
      "epoch: 1844 step: 1, loss is 0.06311977654695511\n",
      "epoch: 1845 step: 1, loss is 0.06302814930677414\n",
      "epoch: 1846 step: 1, loss is 0.06293661147356033\n",
      "epoch: 1847 step: 1, loss is 0.06284520775079727\n",
      "epoch: 1848 step: 1, loss is 0.06275393813848495\n",
      "epoch: 1849 step: 1, loss is 0.062662772834301\n",
      "epoch: 1850 step: 1, loss is 0.06257172673940659\n",
      "epoch: 1851 step: 1, loss is 0.062480781227350235\n",
      "epoch: 1852 step: 1, loss is 0.062389981001615524\n",
      "epoch: 1853 step: 1, loss is 0.06229928880929947\n",
      "epoch: 1854 step: 1, loss is 0.06220871955156326\n",
      "epoch: 1855 step: 1, loss is 0.06211826950311661\n",
      "epoch: 1856 step: 1, loss is 0.0620279461145401\n",
      "epoch: 1857 step: 1, loss is 0.06193773075938225\n",
      "epoch: 1858 step: 1, loss is 0.06184764578938484\n",
      "epoch: 1859 step: 1, loss is 0.061757661402225494\n",
      "epoch: 1860 step: 1, loss is 0.06166781112551689\n",
      "epoch: 1861 step: 1, loss is 0.06157808378338814\n",
      "epoch: 1862 step: 1, loss is 0.06148847937583923\n",
      "epoch: 1863 step: 1, loss is 0.06139897555112839\n",
      "epoch: 1864 step: 1, loss is 0.06130959838628769\n",
      "epoch: 1865 step: 1, loss is 0.06122033670544624\n",
      "epoch: 1866 step: 1, loss is 0.06113120913505554\n",
      "epoch: 1867 step: 1, loss is 0.061042189598083496\n",
      "epoch: 1868 step: 1, loss is 0.0609532967209816\n",
      "epoch: 1869 step: 1, loss is 0.060864515602588654\n",
      "epoch: 1870 step: 1, loss is 0.06077584624290466\n",
      "epoch: 1871 step: 1, loss is 0.06068731099367142\n",
      "epoch: 1872 step: 1, loss is 0.06059889495372772\n",
      "epoch: 1873 step: 1, loss is 0.06051059812307358\n",
      "epoch: 1874 step: 1, loss is 0.06042240560054779\n",
      "epoch: 1875 step: 1, loss is 0.06033434718847275\n",
      "epoch: 1876 step: 1, loss is 0.06024641543626785\n",
      "epoch: 1877 step: 1, loss is 0.06015859171748161\n",
      "epoch: 1878 step: 1, loss is 0.06007089838385582\n",
      "epoch: 1879 step: 1, loss is 0.05998329818248749\n",
      "epoch: 1880 step: 1, loss is 0.059895873069763184\n",
      "epoch: 1881 step: 1, loss is 0.059808533638715744\n",
      "epoch: 1882 step: 1, loss is 0.059721313416957855\n",
      "epoch: 1883 step: 1, loss is 0.05963420122861862\n",
      "epoch: 1884 step: 1, loss is 0.05954723060131073\n",
      "epoch: 1885 step: 1, loss is 0.0594603605568409\n",
      "epoch: 1886 step: 1, loss is 0.05937362462282181\n",
      "epoch: 1887 step: 1, loss is 0.059286996722221375\n",
      "epoch: 1888 step: 1, loss is 0.059200502932071686\n",
      "epoch: 1889 step: 1, loss is 0.05911412462592125\n",
      "epoch: 1890 step: 1, loss is 0.059027861803770065\n",
      "epoch: 1891 step: 1, loss is 0.058941714465618134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1892 step: 1, loss is 0.058855704963207245\n",
      "epoch: 1893 step: 1, loss is 0.05876979976892471\n",
      "epoch: 1894 step: 1, loss is 0.05868401378393173\n",
      "epoch: 1895 step: 1, loss is 0.05859836935997009\n",
      "epoch: 1896 step: 1, loss is 0.05851280689239502\n",
      "epoch: 1897 step: 1, loss is 0.05842738598585129\n",
      "epoch: 1898 step: 1, loss is 0.058342110365629196\n",
      "epoch: 1899 step: 1, loss is 0.05825691670179367\n",
      "epoch: 1900 step: 1, loss is 0.05817185714840889\n",
      "epoch: 1901 step: 1, loss is 0.058086905628442764\n",
      "epoch: 1902 step: 1, loss is 0.05800210312008858\n",
      "epoch: 1903 step: 1, loss is 0.05791739374399185\n",
      "epoch: 1904 step: 1, loss is 0.05783279985189438\n",
      "epoch: 1905 step: 1, loss is 0.05774834379553795\n",
      "epoch: 1906 step: 1, loss is 0.05766400694847107\n",
      "epoch: 1907 step: 1, loss is 0.057579778134822845\n",
      "epoch: 1908 step: 1, loss is 0.057495687156915665\n",
      "epoch: 1909 step: 1, loss is 0.05741170421242714\n",
      "epoch: 1910 step: 1, loss is 0.057327840477228165\n",
      "epoch: 1911 step: 1, loss is 0.05724409222602844\n",
      "epoch: 1912 step: 1, loss is 0.057160474359989166\n",
      "epoch: 1913 step: 1, loss is 0.05707696080207825\n",
      "epoch: 1914 step: 1, loss is 0.056993573904037476\n",
      "epoch: 1915 step: 1, loss is 0.05691031366586685\n",
      "epoch: 1916 step: 1, loss is 0.05682716518640518\n",
      "epoch: 1917 step: 1, loss is 0.05674413591623306\n",
      "epoch: 1918 step: 1, loss is 0.05666122958064079\n",
      "epoch: 1919 step: 1, loss is 0.05657844990491867\n",
      "epoch: 1920 step: 1, loss is 0.056495796889066696\n",
      "epoch: 1921 step: 1, loss is 0.05641324445605278\n",
      "epoch: 1922 step: 1, loss is 0.05633081495761871\n",
      "epoch: 1923 step: 1, loss is 0.0562484972178936\n",
      "epoch: 1924 step: 1, loss is 0.05616632476449013\n",
      "epoch: 1925 step: 1, loss is 0.05608426034450531\n",
      "epoch: 1926 step: 1, loss is 0.05600229650735855\n",
      "epoch: 1927 step: 1, loss is 0.05592046678066254\n",
      "epoch: 1928 step: 1, loss is 0.05583875998854637\n",
      "epoch: 1929 step: 1, loss is 0.05575718730688095\n",
      "epoch: 1930 step: 1, loss is 0.055675700306892395\n",
      "epoch: 1931 step: 1, loss is 0.05559433996677399\n",
      "epoch: 1932 step: 1, loss is 0.055513106286525726\n",
      "epoch: 1933 step: 1, loss is 0.055431999266147614\n",
      "epoch: 1934 step: 1, loss is 0.05535101145505905\n",
      "epoch: 1935 step: 1, loss is 0.055270131677389145\n",
      "epoch: 1936 step: 1, loss is 0.055189378559589386\n",
      "epoch: 1937 step: 1, loss is 0.055108729749917984\n",
      "epoch: 1938 step: 1, loss is 0.055028218775987625\n",
      "epoch: 1939 step: 1, loss is 0.054947830736637115\n",
      "epoch: 1940 step: 1, loss is 0.054867539554834366\n",
      "epoch: 1941 step: 1, loss is 0.05478736758232117\n",
      "epoch: 1942 step: 1, loss is 0.05470733344554901\n",
      "epoch: 1943 step: 1, loss is 0.05462741106748581\n",
      "epoch: 1944 step: 1, loss is 0.05454758554697037\n",
      "epoch: 1945 step: 1, loss is 0.05446791648864746\n",
      "epoch: 1946 step: 1, loss is 0.05438835173845291\n",
      "epoch: 1947 step: 1, loss is 0.054308898746967316\n",
      "epoch: 1948 step: 1, loss is 0.05422956123948097\n",
      "epoch: 1949 step: 1, loss is 0.054150357842445374\n",
      "epoch: 1950 step: 1, loss is 0.05407125875353813\n",
      "epoch: 1951 step: 1, loss is 0.05399227887392044\n",
      "epoch: 1952 step: 1, loss is 0.05391344055533409\n",
      "epoch: 1953 step: 1, loss is 0.05383468419313431\n",
      "epoch: 1954 step: 1, loss is 0.053756069391965866\n",
      "epoch: 1955 step: 1, loss is 0.053677573800086975\n",
      "epoch: 1956 step: 1, loss is 0.05359918251633644\n",
      "epoch: 1957 step: 1, loss is 0.053520917892456055\n",
      "epoch: 1958 step: 1, loss is 0.05344277247786522\n",
      "epoch: 1959 step: 1, loss is 0.05336473882198334\n",
      "epoch: 1960 step: 1, loss is 0.0532868355512619\n",
      "epoch: 1961 step: 1, loss is 0.05320904031395912\n",
      "epoch: 1962 step: 1, loss is 0.05313136801123619\n",
      "epoch: 1963 step: 1, loss is 0.05305379629135132\n",
      "epoch: 1964 step: 1, loss is 0.052976347506046295\n",
      "epoch: 1965 step: 1, loss is 0.052899040281772614\n",
      "epoch: 1966 step: 1, loss is 0.05282182618975639\n",
      "epoch: 1967 step: 1, loss is 0.052744731307029724\n",
      "epoch: 1968 step: 1, loss is 0.0526677705347538\n",
      "epoch: 1969 step: 1, loss is 0.05259091779589653\n",
      "epoch: 1970 step: 1, loss is 0.05251418799161911\n",
      "epoch: 1971 step: 1, loss is 0.05243756249547005\n",
      "epoch: 1972 step: 1, loss is 0.052361052483320236\n",
      "epoch: 1973 step: 1, loss is 0.05228465795516968\n",
      "epoch: 1974 step: 1, loss is 0.052208397537469864\n",
      "epoch: 1975 step: 1, loss is 0.0521322563290596\n",
      "epoch: 1976 step: 1, loss is 0.0520562045276165\n",
      "epoch: 1977 step: 1, loss is 0.05198030546307564\n",
      "epoch: 1978 step: 1, loss is 0.05190448835492134\n",
      "epoch: 1979 step: 1, loss is 0.051828816533088684\n",
      "epoch: 1980 step: 1, loss is 0.051753245294094086\n",
      "epoch: 1981 step: 1, loss is 0.05167779326438904\n",
      "epoch: 1982 step: 1, loss is 0.05160247161984444\n",
      "epoch: 1983 step: 1, loss is 0.0515272282063961\n",
      "epoch: 1984 step: 1, loss is 0.051452141255140305\n",
      "epoch: 1985 step: 1, loss is 0.05137715861201286\n",
      "epoch: 1986 step: 1, loss is 0.05130230262875557\n",
      "epoch: 1987 step: 1, loss is 0.05122752487659454\n",
      "epoch: 1988 step: 1, loss is 0.05115289241075516\n",
      "epoch: 1989 step: 1, loss is 0.05107835680246353\n",
      "epoch: 1990 step: 1, loss is 0.05100396275520325\n",
      "epoch: 1991 step: 1, loss is 0.05092967301607132\n",
      "epoch: 1992 step: 1, loss is 0.05085550621151924\n",
      "epoch: 1993 step: 1, loss is 0.05078144371509552\n",
      "epoch: 1994 step: 1, loss is 0.05070749297738075\n",
      "epoch: 1995 step: 1, loss is 0.050633661448955536\n",
      "epoch: 1996 step: 1, loss is 0.050559937953948975\n",
      "epoch: 1997 step: 1, loss is 0.05048634856939316\n",
      "epoch: 1998 step: 1, loss is 0.050412874668836594\n",
      "epoch: 1999 step: 1, loss is 0.05033949762582779\n",
      "epoch: 2000 step: 1, loss is 0.05026624724268913\n",
      "epoch: 2001 step: 1, loss is 0.05019310489296913\n",
      "epoch: 2002 step: 1, loss is 0.05012008175253868\n",
      "epoch: 2003 step: 1, loss is 0.05004717782139778\n",
      "epoch: 2004 step: 1, loss is 0.049974385648965836\n",
      "epoch: 2005 step: 1, loss is 0.04990169033408165\n",
      "epoch: 2006 step: 1, loss is 0.04982913285493851\n",
      "epoch: 2007 step: 1, loss is 0.04975668713450432\n",
      "epoch: 2008 step: 1, loss is 0.049684349447488785\n",
      "epoch: 2009 step: 1, loss is 0.0496121309697628\n",
      "epoch: 2010 step: 1, loss is 0.049540016800165176\n",
      "epoch: 2011 step: 1, loss is 0.0494680218398571\n",
      "epoch: 2012 step: 1, loss is 0.04939613863825798\n",
      "epoch: 2013 step: 1, loss is 0.049324363470077515\n",
      "epoch: 2014 step: 1, loss is 0.0492527149617672\n",
      "epoch: 2015 step: 1, loss is 0.049181174486875534\n",
      "epoch: 2016 step: 1, loss is 0.049109749495983124\n",
      "epoch: 2017 step: 1, loss is 0.049038439989089966\n",
      "epoch: 2018 step: 1, loss is 0.048967234790325165\n",
      "epoch: 2019 step: 1, loss is 0.048896148800849915\n",
      "epoch: 2020 step: 1, loss is 0.04882516711950302\n",
      "epoch: 2021 step: 1, loss is 0.048754315823316574\n",
      "epoch: 2022 step: 1, loss is 0.04868356138467789\n",
      "epoch: 2023 step: 1, loss is 0.04861292243003845\n",
      "epoch: 2024 step: 1, loss is 0.04854240268468857\n",
      "epoch: 2025 step: 1, loss is 0.048471979796886444\n",
      "epoch: 2026 step: 1, loss is 0.04840168356895447\n",
      "epoch: 2027 step: 1, loss is 0.04833151027560234\n",
      "epoch: 2028 step: 1, loss is 0.04826143756508827\n",
      "epoch: 2029 step: 1, loss is 0.048191461712121964\n",
      "epoch: 2030 step: 1, loss is 0.048121605068445206\n",
      "epoch: 2031 step: 1, loss is 0.048051878809928894\n",
      "epoch: 2032 step: 1, loss is 0.04798223823308945\n",
      "epoch: 2033 step: 1, loss is 0.047912731766700745\n",
      "epoch: 2034 step: 1, loss is 0.0478433258831501\n",
      "epoch: 2035 step: 1, loss is 0.04777403548359871\n",
      "epoch: 2036 step: 1, loss is 0.04770485311746597\n",
      "epoch: 2037 step: 1, loss is 0.04763578623533249\n",
      "epoch: 2038 step: 1, loss is 0.047566819936037064\n",
      "epoch: 2039 step: 1, loss is 0.047497980296611786\n",
      "epoch: 2040 step: 1, loss is 0.04742924124002457\n",
      "epoch: 2041 step: 1, loss is 0.0473606213927269\n",
      "epoch: 2042 step: 1, loss is 0.04729209840297699\n",
      "epoch: 2043 step: 1, loss is 0.047223687171936035\n",
      "epoch: 2044 step: 1, loss is 0.047155410051345825\n",
      "epoch: 2045 step: 1, loss is 0.04708722233772278\n",
      "epoch: 2046 step: 1, loss is 0.047019146382808685\n",
      "epoch: 2047 step: 1, loss is 0.046951182186603546\n",
      "epoch: 2048 step: 1, loss is 0.04688332602381706\n",
      "epoch: 2049 step: 1, loss is 0.04681559279561043\n",
      "epoch: 2050 step: 1, loss is 0.04674794152379036\n",
      "epoch: 2051 step: 1, loss is 0.04668042063713074\n",
      "epoch: 2052 step: 1, loss is 0.04661300405859947\n",
      "epoch: 2053 step: 1, loss is 0.046545710414648056\n",
      "epoch: 2054 step: 1, loss is 0.046478502452373505\n",
      "epoch: 2055 step: 1, loss is 0.04641139879822731\n",
      "epoch: 2056 step: 1, loss is 0.04634442180395126\n",
      "epoch: 2057 step: 1, loss is 0.04627756401896477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2058 step: 1, loss is 0.046210817992687225\n",
      "epoch: 2059 step: 1, loss is 0.04614414647221565\n",
      "epoch: 2060 step: 1, loss is 0.04607759788632393\n",
      "epoch: 2061 step: 1, loss is 0.04601117968559265\n",
      "epoch: 2062 step: 1, loss is 0.045944854617118835\n",
      "epoch: 2063 step: 1, loss is 0.04587861895561218\n",
      "epoch: 2064 step: 1, loss is 0.04581250995397568\n",
      "epoch: 2065 step: 1, loss is 0.04574650526046753\n",
      "epoch: 2066 step: 1, loss is 0.04568062350153923\n",
      "epoch: 2067 step: 1, loss is 0.045614827424287796\n",
      "epoch: 2068 step: 1, loss is 0.04554913938045502\n",
      "epoch: 2069 step: 1, loss is 0.04548356682062149\n",
      "epoch: 2070 step: 1, loss is 0.04541809484362602\n",
      "epoch: 2071 step: 1, loss is 0.045352738350629807\n",
      "epoch: 2072 step: 1, loss is 0.04528748616576195\n",
      "epoch: 2073 step: 1, loss is 0.04522235319018364\n",
      "epoch: 2074 step: 1, loss is 0.0451572984457016\n",
      "epoch: 2075 step: 1, loss is 0.045092374086380005\n",
      "epoch: 2076 step: 1, loss is 0.04502755030989647\n",
      "epoch: 2077 step: 1, loss is 0.04496283084154129\n",
      "epoch: 2078 step: 1, loss is 0.044898226857185364\n",
      "epoch: 2079 step: 1, loss is 0.0448337122797966\n",
      "epoch: 2080 step: 1, loss is 0.04476931691169739\n",
      "epoch: 2081 step: 1, loss is 0.044705022126436234\n",
      "epoch: 2082 step: 1, loss is 0.04464083909988403\n",
      "epoch: 2083 step: 1, loss is 0.04457676038146019\n",
      "epoch: 2084 step: 1, loss is 0.04451277479529381\n",
      "epoch: 2085 step: 1, loss is 0.04444890469312668\n",
      "epoch: 2086 step: 1, loss is 0.0443851463496685\n",
      "epoch: 2087 step: 1, loss is 0.04432146996259689\n",
      "epoch: 2088 step: 1, loss is 0.04425791651010513\n",
      "epoch: 2089 step: 1, loss is 0.044194482266902924\n",
      "epoch: 2090 step: 1, loss is 0.044131144881248474\n",
      "epoch: 2091 step: 1, loss is 0.04406789690256119\n",
      "epoch: 2092 step: 1, loss is 0.04400475695729256\n",
      "epoch: 2093 step: 1, loss is 0.04394172504544258\n",
      "epoch: 2094 step: 1, loss is 0.04387880861759186\n",
      "epoch: 2095 step: 1, loss is 0.043815966695547104\n",
      "epoch: 2096 step: 1, loss is 0.0437532477080822\n",
      "epoch: 2097 step: 1, loss is 0.04369062930345535\n",
      "epoch: 2098 step: 1, loss is 0.04362811893224716\n",
      "epoch: 2099 step: 1, loss is 0.04356571286916733\n",
      "epoch: 2100 step: 1, loss is 0.04350339248776436\n",
      "epoch: 2101 step: 1, loss is 0.04344119876623154\n",
      "epoch: 2102 step: 1, loss is 0.043379101902246475\n",
      "epoch: 2103 step: 1, loss is 0.043317101895809174\n",
      "epoch: 2104 step: 1, loss is 0.043255213648080826\n",
      "epoch: 2105 step: 1, loss is 0.04319342225790024\n",
      "epoch: 2106 step: 1, loss is 0.04313173145055771\n",
      "epoch: 2107 step: 1, loss is 0.043070148676633835\n",
      "epoch: 2108 step: 1, loss is 0.04300867021083832\n",
      "epoch: 2109 step: 1, loss is 0.04294727370142937\n",
      "epoch: 2110 step: 1, loss is 0.042886003851890564\n",
      "epoch: 2111 step: 1, loss is 0.04282483458518982\n",
      "epoch: 2112 step: 1, loss is 0.04276374354958534\n",
      "epoch: 2113 step: 1, loss is 0.04270279034972191\n",
      "epoch: 2114 step: 1, loss is 0.042641907930374146\n",
      "epoch: 2115 step: 1, loss is 0.04258112981915474\n",
      "epoch: 2116 step: 1, loss is 0.04252046346664429\n",
      "epoch: 2117 step: 1, loss is 0.042459893971681595\n",
      "epoch: 2118 step: 1, loss is 0.042399439960718155\n",
      "epoch: 2119 step: 1, loss is 0.04233906418085098\n",
      "epoch: 2120 step: 1, loss is 0.04227880761027336\n",
      "epoch: 2121 step: 1, loss is 0.0422186478972435\n",
      "epoch: 2122 step: 1, loss is 0.0421585850417614\n",
      "epoch: 2123 step: 1, loss is 0.042098622769117355\n",
      "epoch: 2124 step: 1, loss is 0.042038749903440475\n",
      "epoch: 2125 step: 1, loss is 0.04197899252176285\n",
      "epoch: 2126 step: 1, loss is 0.04191932454705238\n",
      "epoch: 2127 step: 1, loss is 0.04185976833105087\n",
      "epoch: 2128 step: 1, loss is 0.041800301522016525\n",
      "epoch: 2129 step: 1, loss is 0.04174092411994934\n",
      "epoch: 2130 step: 1, loss is 0.04168166220188141\n",
      "epoch: 2131 step: 1, loss is 0.04162249714136124\n",
      "epoch: 2132 step: 1, loss is 0.04156343266367912\n",
      "epoch: 2133 step: 1, loss is 0.04150446876883507\n",
      "epoch: 2134 step: 1, loss is 0.04144560545682907\n",
      "epoch: 2135 step: 1, loss is 0.041386835277080536\n",
      "epoch: 2136 step: 1, loss is 0.041328154504299164\n",
      "epoch: 2137 step: 1, loss is 0.04126958176493645\n",
      "epoch: 2138 step: 1, loss is 0.04121110215783119\n",
      "epoch: 2139 step: 1, loss is 0.041152723133563995\n",
      "epoch: 2140 step: 1, loss is 0.041094452142715454\n",
      "epoch: 2141 step: 1, loss is 0.04103626683354378\n",
      "epoch: 2142 step: 1, loss is 0.04097817838191986\n",
      "epoch: 2143 step: 1, loss is 0.0409201942384243\n",
      "epoch: 2144 step: 1, loss is 0.040862295776605606\n",
      "epoch: 2145 step: 1, loss is 0.040804509073495865\n",
      "epoch: 2146 step: 1, loss is 0.040746815502643585\n",
      "epoch: 2147 step: 1, loss is 0.040689222514629364\n",
      "epoch: 2148 step: 1, loss is 0.040631722658872604\n",
      "epoch: 2149 step: 1, loss is 0.04057431221008301\n",
      "epoch: 2150 step: 1, loss is 0.04051700979471207\n",
      "epoch: 2151 step: 1, loss is 0.04045980051159859\n",
      "epoch: 2152 step: 1, loss is 0.04040268436074257\n",
      "epoch: 2153 step: 1, loss is 0.040345657616853714\n",
      "epoch: 2154 step: 1, loss is 0.04028873145580292\n",
      "epoch: 2155 step: 1, loss is 0.04023190215229988\n",
      "epoch: 2156 step: 1, loss is 0.0401751734316349\n",
      "epoch: 2157 step: 1, loss is 0.040118537843227386\n",
      "epoch: 2158 step: 1, loss is 0.04006199538707733\n",
      "epoch: 2159 step: 1, loss is 0.040005557239055634\n",
      "epoch: 2160 step: 1, loss is 0.039949189871549606\n",
      "epoch: 2161 step: 1, loss is 0.03989294171333313\n",
      "epoch: 2162 step: 1, loss is 0.03983679041266441\n",
      "epoch: 2163 step: 1, loss is 0.03978072106838226\n",
      "epoch: 2164 step: 1, loss is 0.03972474858164787\n",
      "epoch: 2165 step: 1, loss is 0.03966887295246124\n",
      "epoch: 2166 step: 1, loss is 0.039613086730241776\n",
      "epoch: 2167 step: 1, loss is 0.03955739736557007\n",
      "epoch: 2168 step: 1, loss is 0.03950180485844612\n",
      "epoch: 2169 step: 1, loss is 0.03944630175828934\n",
      "epoch: 2170 step: 1, loss is 0.03939089924097061\n",
      "epoch: 2171 step: 1, loss is 0.03933557868003845\n",
      "epoch: 2172 step: 1, loss is 0.03928036987781525\n",
      "epoch: 2173 step: 1, loss is 0.03922524303197861\n",
      "epoch: 2174 step: 1, loss is 0.039170198142528534\n",
      "epoch: 2175 step: 1, loss is 0.03911527246236801\n",
      "epoch: 2176 step: 1, loss is 0.03906041383743286\n",
      "epoch: 2177 step: 1, loss is 0.03900565207004547\n",
      "epoch: 2178 step: 1, loss is 0.03895098716020584\n",
      "epoch: 2179 step: 1, loss is 0.038896434009075165\n",
      "epoch: 2180 step: 1, loss is 0.03884194791316986\n",
      "epoch: 2181 step: 1, loss is 0.03878756985068321\n",
      "epoch: 2182 step: 1, loss is 0.03873327374458313\n",
      "epoch: 2183 step: 1, loss is 0.03867907449603081\n",
      "epoch: 2184 step: 1, loss is 0.03862495720386505\n",
      "epoch: 2185 step: 1, loss is 0.038570936769247055\n",
      "epoch: 2186 step: 1, loss is 0.03851700574159622\n",
      "epoch: 2187 step: 1, loss is 0.03846317157149315\n",
      "epoch: 2188 step: 1, loss is 0.03840942680835724\n",
      "epoch: 2189 step: 1, loss is 0.03835577517747879\n",
      "epoch: 2190 step: 1, loss is 0.0383022166788578\n",
      "epoch: 2191 step: 1, loss is 0.03824874013662338\n",
      "epoch: 2192 step: 1, loss is 0.03819534182548523\n",
      "epoch: 2193 step: 1, loss is 0.038142070174217224\n",
      "epoch: 2194 step: 1, loss is 0.038088858127593994\n",
      "epoch: 2195 step: 1, loss is 0.03803575411438942\n",
      "epoch: 2196 step: 1, loss is 0.03798273205757141\n",
      "epoch: 2197 step: 1, loss is 0.03792980685830116\n",
      "epoch: 2198 step: 1, loss is 0.03787695989012718\n",
      "epoch: 2199 step: 1, loss is 0.037824202328920364\n",
      "epoch: 2200 step: 1, loss is 0.03777153789997101\n",
      "epoch: 2201 step: 1, loss is 0.03771895915269852\n",
      "epoch: 2202 step: 1, loss is 0.037666477262973785\n",
      "epoch: 2203 step: 1, loss is 0.037614088505506516\n",
      "epoch: 2204 step: 1, loss is 0.037561774253845215\n",
      "epoch: 2205 step: 1, loss is 0.03750956431031227\n",
      "epoch: 2206 step: 1, loss is 0.0374574288725853\n",
      "epoch: 2207 step: 1, loss is 0.03740537911653519\n",
      "epoch: 2208 step: 1, loss is 0.037353429943323135\n",
      "epoch: 2209 step: 1, loss is 0.03730155527591705\n",
      "epoch: 2210 step: 1, loss is 0.03724978491663933\n",
      "epoch: 2211 step: 1, loss is 0.03719809278845787\n",
      "epoch: 2212 step: 1, loss is 0.03714650869369507\n",
      "epoch: 2213 step: 1, loss is 0.03709498420357704\n",
      "epoch: 2214 step: 1, loss is 0.037043578922748566\n",
      "epoch: 2215 step: 1, loss is 0.03699222579598427\n",
      "epoch: 2216 step: 1, loss is 0.03694098815321922\n",
      "epoch: 2217 step: 1, loss is 0.03688981384038925\n",
      "epoch: 2218 step: 1, loss is 0.03683873265981674\n",
      "epoch: 2219 step: 1, loss is 0.0367877334356308\n",
      "epoch: 2220 step: 1, loss is 0.03673683479428291\n",
      "epoch: 2221 step: 1, loss is 0.036686014384031296\n",
      "epoch: 2222 step: 1, loss is 0.03663529083132744\n",
      "epoch: 2223 step: 1, loss is 0.03658464923501015\n",
      "epoch: 2224 step: 1, loss is 0.03653407841920853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2225 step: 1, loss is 0.03648360073566437\n",
      "epoch: 2226 step: 1, loss is 0.03643321245908737\n",
      "epoch: 2227 step: 1, loss is 0.03638290613889694\n",
      "epoch: 2228 step: 1, loss is 0.03633268177509308\n",
      "epoch: 2229 step: 1, loss is 0.03628255054354668\n",
      "epoch: 2230 step: 1, loss is 0.03623250871896744\n",
      "epoch: 2231 step: 1, loss is 0.036182548850774765\n",
      "epoch: 2232 step: 1, loss is 0.03613266721367836\n",
      "epoch: 2233 step: 1, loss is 0.03608287498354912\n",
      "epoch: 2234 step: 1, loss is 0.03603317588567734\n",
      "epoch: 2235 step: 1, loss is 0.03598354011774063\n",
      "epoch: 2236 step: 1, loss is 0.035933997482061386\n",
      "epoch: 2237 step: 1, loss is 0.035884540528059006\n",
      "epoch: 2238 step: 1, loss is 0.03583517298102379\n",
      "epoch: 2239 step: 1, loss is 0.03578588739037514\n",
      "epoch: 2240 step: 1, loss is 0.03573668375611305\n",
      "epoch: 2241 step: 1, loss is 0.03568756952881813\n",
      "epoch: 2242 step: 1, loss is 0.03563851863145828\n",
      "epoch: 2243 step: 1, loss is 0.03558956831693649\n",
      "epoch: 2244 step: 1, loss is 0.035540685057640076\n",
      "epoch: 2245 step: 1, loss is 0.035491906106472015\n",
      "epoch: 2246 step: 1, loss is 0.03544320538640022\n",
      "epoch: 2247 step: 1, loss is 0.0353945754468441\n",
      "epoch: 2248 step: 1, loss is 0.03534603863954544\n",
      "epoch: 2249 step: 1, loss is 0.03529758378863335\n",
      "epoch: 2250 step: 1, loss is 0.03524921089410782\n",
      "epoch: 2251 step: 1, loss is 0.03520091250538826\n",
      "epoch: 2252 step: 1, loss is 0.03515270724892616\n",
      "epoch: 2253 step: 1, loss is 0.035104576498270035\n",
      "epoch: 2254 step: 1, loss is 0.03505653515458107\n",
      "epoch: 2255 step: 1, loss is 0.03500857204198837\n",
      "epoch: 2256 step: 1, loss is 0.034960683435201645\n",
      "epoch: 2257 step: 1, loss is 0.03491287678480148\n",
      "epoch: 2258 step: 1, loss is 0.034865159541368484\n",
      "epoch: 2259 step: 1, loss is 0.03481751307845116\n",
      "epoch: 2260 step: 1, loss is 0.03476995974779129\n",
      "epoch: 2261 step: 1, loss is 0.03472248092293739\n",
      "epoch: 2262 step: 1, loss is 0.034675076603889465\n",
      "epoch: 2263 step: 1, loss is 0.034627765417099\n",
      "epoch: 2264 step: 1, loss is 0.0345805287361145\n",
      "epoch: 2265 step: 1, loss is 0.03453337028622627\n",
      "epoch: 2266 step: 1, loss is 0.03448629751801491\n",
      "epoch: 2267 step: 1, loss is 0.034439295530319214\n",
      "epoch: 2268 step: 1, loss is 0.034392375499010086\n",
      "epoch: 2269 step: 1, loss is 0.034345533698797226\n",
      "epoch: 2270 step: 1, loss is 0.03429877758026123\n",
      "epoch: 2271 step: 1, loss is 0.0342520996928215\n",
      "epoch: 2272 step: 1, loss is 0.03420550376176834\n",
      "epoch: 2273 step: 1, loss is 0.03415897861123085\n",
      "epoch: 2274 step: 1, loss is 0.034112535417079926\n",
      "epoch: 2275 step: 1, loss is 0.03406618535518646\n",
      "epoch: 2276 step: 1, loss is 0.034019894897937775\n",
      "epoch: 2277 step: 1, loss is 0.03397369012236595\n",
      "epoch: 2278 step: 1, loss is 0.0339275561273098\n",
      "epoch: 2279 step: 1, loss is 0.033881522715091705\n",
      "epoch: 2280 step: 1, loss is 0.03383553773164749\n",
      "epoch: 2281 step: 1, loss is 0.03378964960575104\n",
      "epoch: 2282 step: 1, loss is 0.033743828535079956\n",
      "epoch: 2283 step: 1, loss is 0.03369809314608574\n",
      "epoch: 2284 step: 1, loss is 0.03365242853760719\n",
      "epoch: 2285 step: 1, loss is 0.03360684588551521\n",
      "epoch: 2286 step: 1, loss is 0.0335613489151001\n",
      "epoch: 2287 step: 1, loss is 0.033515915274620056\n",
      "epoch: 2288 step: 1, loss is 0.033470556139945984\n",
      "epoch: 2289 step: 1, loss is 0.03342529013752937\n",
      "epoch: 2290 step: 1, loss is 0.03338009491562843\n",
      "epoch: 2291 step: 1, loss is 0.03333498165011406\n",
      "epoch: 2292 step: 1, loss is 0.03328993171453476\n",
      "epoch: 2293 step: 1, loss is 0.033244963735342026\n",
      "epoch: 2294 step: 1, loss is 0.03320007026195526\n",
      "epoch: 2295 step: 1, loss is 0.03315524011850357\n",
      "epoch: 2296 step: 1, loss is 0.03311050683259964\n",
      "epoch: 2297 step: 1, loss is 0.03306584060192108\n",
      "epoch: 2298 step: 1, loss is 0.033021267503499985\n",
      "epoch: 2299 step: 1, loss is 0.03297674283385277\n",
      "epoch: 2300 step: 1, loss is 0.032932307571172714\n",
      "epoch: 2301 step: 1, loss is 0.03288794681429863\n",
      "epoch: 2302 step: 1, loss is 0.032843656837940216\n",
      "epoch: 2303 step: 1, loss is 0.03279944509267807\n",
      "epoch: 2304 step: 1, loss is 0.03275531157851219\n",
      "epoch: 2305 step: 1, loss is 0.03271123394370079\n",
      "epoch: 2306 step: 1, loss is 0.03266724944114685\n",
      "epoch: 2307 step: 1, loss is 0.032623354345560074\n",
      "epoch: 2308 step: 1, loss is 0.03257950395345688\n",
      "epoch: 2309 step: 1, loss is 0.03253573924303055\n",
      "epoch: 2310 step: 1, loss is 0.032492052763700485\n",
      "epoch: 2311 step: 1, loss is 0.032448433339595795\n",
      "epoch: 2312 step: 1, loss is 0.03240489214658737\n",
      "epoch: 2313 step: 1, loss is 0.03236142173409462\n",
      "epoch: 2314 step: 1, loss is 0.03231802210211754\n",
      "epoch: 2315 step: 1, loss is 0.032274700701236725\n",
      "epoch: 2316 step: 1, loss is 0.03223145753145218\n",
      "epoch: 2317 step: 1, loss is 0.03218827769160271\n",
      "epoch: 2318 step: 1, loss is 0.032145168632268906\n",
      "epoch: 2319 step: 1, loss is 0.03210214152932167\n",
      "epoch: 2320 step: 1, loss is 0.032059188932180405\n",
      "epoch: 2321 step: 1, loss is 0.03201630711555481\n",
      "epoch: 2322 step: 1, loss is 0.03197348490357399\n",
      "epoch: 2323 step: 1, loss is 0.03193075582385063\n",
      "epoch: 2324 step: 1, loss is 0.03188807889819145\n",
      "epoch: 2325 step: 1, loss is 0.031845491379499435\n",
      "epoch: 2326 step: 1, loss is 0.03180297091603279\n",
      "epoch: 2327 step: 1, loss is 0.031760524958372116\n",
      "epoch: 2328 step: 1, loss is 0.031718142330646515\n",
      "epoch: 2329 step: 1, loss is 0.03167583420872688\n",
      "epoch: 2330 step: 1, loss is 0.031633589416742325\n",
      "epoch: 2331 step: 1, loss is 0.03159142658114433\n",
      "epoch: 2332 step: 1, loss is 0.03154933080077171\n",
      "epoch: 2333 step: 1, loss is 0.031507305800914764\n",
      "epoch: 2334 step: 1, loss is 0.031465351581573486\n",
      "epoch: 2335 step: 1, loss is 0.03142347186803818\n",
      "epoch: 2336 step: 1, loss is 0.03138165548443794\n",
      "epoch: 2337 step: 1, loss is 0.031339921057224274\n",
      "epoch: 2338 step: 1, loss is 0.03129824250936508\n",
      "epoch: 2339 step: 1, loss is 0.03125665336847305\n",
      "epoch: 2340 step: 1, loss is 0.03121512196958065\n",
      "epoch: 2341 step: 1, loss is 0.03117365762591362\n",
      "epoch: 2342 step: 1, loss is 0.03113226592540741\n",
      "epoch: 2343 step: 1, loss is 0.031090952455997467\n",
      "epoch: 2344 step: 1, loss is 0.031049707904458046\n",
      "epoch: 2345 step: 1, loss is 0.0310085266828537\n",
      "epoch: 2346 step: 1, loss is 0.03096739947795868\n",
      "epoch: 2347 step: 1, loss is 0.030926372855901718\n",
      "epoch: 2348 step: 1, loss is 0.030885392799973488\n",
      "epoch: 2349 step: 1, loss is 0.030844498425722122\n",
      "epoch: 2350 step: 1, loss is 0.030803654342889786\n",
      "epoch: 2351 step: 1, loss is 0.03076288104057312\n",
      "epoch: 2352 step: 1, loss is 0.03072219528257847\n",
      "epoch: 2353 step: 1, loss is 0.030681557953357697\n",
      "epoch: 2354 step: 1, loss is 0.030641011893749237\n",
      "epoch: 2355 step: 1, loss is 0.03060051053762436\n",
      "epoch: 2356 step: 1, loss is 0.030560094863176346\n",
      "epoch: 2357 step: 1, loss is 0.030519738793373108\n",
      "epoch: 2358 step: 1, loss is 0.030479442328214645\n",
      "epoch: 2359 step: 1, loss is 0.0304392222315073\n",
      "epoch: 2360 step: 1, loss is 0.030399063602089882\n",
      "epoch: 2361 step: 1, loss is 0.030358972027897835\n",
      "epoch: 2362 step: 1, loss is 0.030318960547447205\n",
      "epoch: 2363 step: 1, loss is 0.0302790105342865\n",
      "epoch: 2364 step: 1, loss is 0.030239135026931763\n",
      "epoch: 2365 step: 1, loss is 0.03019932098686695\n",
      "epoch: 2366 step: 1, loss is 0.03015957586467266\n",
      "epoch: 2367 step: 1, loss is 0.030119886621832848\n",
      "epoch: 2368 step: 1, loss is 0.030080264434218407\n",
      "epoch: 2369 step: 1, loss is 0.030040714889764786\n",
      "epoch: 2370 step: 1, loss is 0.030001234263181686\n",
      "epoch: 2371 step: 1, loss is 0.02996182255446911\n",
      "epoch: 2372 step: 1, loss is 0.029922472313046455\n",
      "epoch: 2373 step: 1, loss is 0.029883187264204025\n",
      "epoch: 2374 step: 1, loss is 0.029843967407941818\n",
      "epoch: 2375 step: 1, loss is 0.029804812744259834\n",
      "epoch: 2376 step: 1, loss is 0.029765726998448372\n",
      "epoch: 2377 step: 1, loss is 0.029726702719926834\n",
      "epoch: 2378 step: 1, loss is 0.029687751084566116\n",
      "epoch: 2379 step: 1, loss is 0.029648849740624428\n",
      "epoch: 2380 step: 1, loss is 0.029610026627779007\n",
      "epoch: 2381 step: 1, loss is 0.02957126870751381\n",
      "epoch: 2382 step: 1, loss is 0.02953256480395794\n",
      "epoch: 2383 step: 1, loss is 0.02949393540620804\n",
      "epoch: 2384 step: 1, loss is 0.029455378651618958\n",
      "epoch: 2385 step: 1, loss is 0.029416868463158607\n",
      "epoch: 2386 step: 1, loss is 0.029378434643149376\n",
      "epoch: 2387 step: 1, loss is 0.029340064153075218\n",
      "epoch: 2388 step: 1, loss is 0.029301755130290985\n",
      "epoch: 2389 step: 1, loss is 0.029263503849506378\n",
      "epoch: 2390 step: 1, loss is 0.02922532707452774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2391 step: 1, loss is 0.029187215492129326\n",
      "epoch: 2392 step: 1, loss is 0.029149165377020836\n",
      "epoch: 2393 step: 1, loss is 0.029111165553331375\n",
      "epoch: 2394 step: 1, loss is 0.029073242098093033\n",
      "epoch: 2395 step: 1, loss is 0.029035376384854317\n",
      "epoch: 2396 step: 1, loss is 0.028997574001550674\n",
      "epoch: 2397 step: 1, loss is 0.028959836810827255\n",
      "epoch: 2398 step: 1, loss is 0.028922175988554955\n",
      "epoch: 2399 step: 1, loss is 0.028884563595056534\n",
      "epoch: 2400 step: 1, loss is 0.028847014531493187\n",
      "epoch: 2401 step: 1, loss is 0.028809530660510063\n",
      "epoch: 2402 step: 1, loss is 0.02877209708094597\n",
      "epoch: 2403 step: 1, loss is 0.02873474732041359\n",
      "epoch: 2404 step: 1, loss is 0.028697440400719643\n",
      "epoch: 2405 step: 1, loss is 0.028660211712121964\n",
      "epoch: 2406 step: 1, loss is 0.028623031452298164\n",
      "epoch: 2407 step: 1, loss is 0.028585931286215782\n",
      "epoch: 2408 step: 1, loss is 0.02854887954890728\n",
      "epoch: 2409 step: 1, loss is 0.028511885553598404\n",
      "epoch: 2410 step: 1, loss is 0.02847496047616005\n",
      "epoch: 2411 step: 1, loss is 0.028438102453947067\n",
      "epoch: 2412 step: 1, loss is 0.028401300311088562\n",
      "epoch: 2413 step: 1, loss is 0.028364550322294235\n",
      "epoch: 2414 step: 1, loss is 0.028327874839305878\n",
      "epoch: 2415 step: 1, loss is 0.0282912515103817\n",
      "epoch: 2416 step: 1, loss is 0.028254691511392593\n",
      "epoch: 2417 step: 1, loss is 0.02821820043027401\n",
      "epoch: 2418 step: 1, loss is 0.028181754052639008\n",
      "epoch: 2419 step: 1, loss is 0.028145384043455124\n",
      "epoch: 2420 step: 1, loss is 0.02810906432569027\n",
      "epoch: 2421 step: 1, loss is 0.028072817251086235\n",
      "epoch: 2422 step: 1, loss is 0.028036607429385185\n",
      "epoch: 2423 step: 1, loss is 0.028000475838780403\n",
      "epoch: 2424 step: 1, loss is 0.027964407578110695\n",
      "epoch: 2425 step: 1, loss is 0.027928384020924568\n",
      "epoch: 2426 step: 1, loss is 0.02789243496954441\n",
      "epoch: 2427 step: 1, loss is 0.027856532484292984\n",
      "epoch: 2428 step: 1, loss is 0.02782069891691208\n",
      "epoch: 2429 step: 1, loss is 0.02778492122888565\n",
      "epoch: 2430 step: 1, loss is 0.027749210596084595\n",
      "epoch: 2431 step: 1, loss is 0.027713539078831673\n",
      "epoch: 2432 step: 1, loss is 0.027677949517965317\n",
      "epoch: 2433 step: 1, loss is 0.02764241397380829\n",
      "epoch: 2434 step: 1, loss is 0.02760692685842514\n",
      "epoch: 2435 step: 1, loss is 0.027571501210331917\n",
      "epoch: 2436 step: 1, loss is 0.027536148205399513\n",
      "epoch: 2437 step: 1, loss is 0.027500830590724945\n",
      "epoch: 2438 step: 1, loss is 0.027465587481856346\n",
      "epoch: 2439 step: 1, loss is 0.02743040956556797\n",
      "epoch: 2440 step: 1, loss is 0.02739526890218258\n",
      "epoch: 2441 step: 1, loss is 0.027360200881958008\n",
      "epoch: 2442 step: 1, loss is 0.027325190603733063\n",
      "epoch: 2443 step: 1, loss is 0.0272902250289917\n",
      "epoch: 2444 step: 1, loss is 0.027255333960056305\n",
      "epoch: 2445 step: 1, loss is 0.027220487594604492\n",
      "epoch: 2446 step: 1, loss is 0.027185706421732903\n",
      "epoch: 2447 step: 1, loss is 0.02715097926557064\n",
      "epoch: 2448 step: 1, loss is 0.027116309851408005\n",
      "epoch: 2449 step: 1, loss is 0.027081703767180443\n",
      "epoch: 2450 step: 1, loss is 0.02704714611172676\n",
      "epoch: 2451 step: 1, loss is 0.027012651786208153\n",
      "epoch: 2452 step: 1, loss is 0.02697821334004402\n",
      "epoch: 2453 step: 1, loss is 0.026943830773234367\n",
      "epoch: 2454 step: 1, loss is 0.02690950036048889\n",
      "epoch: 2455 step: 1, loss is 0.02687523514032364\n",
      "epoch: 2456 step: 1, loss is 0.026841016486287117\n",
      "epoch: 2457 step: 1, loss is 0.026806870475411415\n",
      "epoch: 2458 step: 1, loss is 0.026772771030664444\n",
      "epoch: 2459 step: 1, loss is 0.026738720014691353\n",
      "epoch: 2460 step: 1, loss is 0.026704737916588783\n",
      "epoch: 2461 step: 1, loss is 0.026670802384614944\n",
      "epoch: 2462 step: 1, loss is 0.026636919006705284\n",
      "epoch: 2463 step: 1, loss is 0.026603104546666145\n",
      "epoch: 2464 step: 1, loss is 0.026569347828626633\n",
      "epoch: 2465 step: 1, loss is 0.026535639539361\n",
      "epoch: 2466 step: 1, loss is 0.026501981541514397\n",
      "epoch: 2467 step: 1, loss is 0.026468390598893166\n",
      "epoch: 2468 step: 1, loss is 0.02643483690917492\n",
      "epoch: 2469 step: 1, loss is 0.026401352137327194\n",
      "epoch: 2470 step: 1, loss is 0.0263679176568985\n",
      "epoch: 2471 step: 1, loss is 0.026334542781114578\n",
      "epoch: 2472 step: 1, loss is 0.026301220059394836\n",
      "epoch: 2473 step: 1, loss is 0.026267947629094124\n",
      "epoch: 2474 step: 1, loss is 0.026234736666083336\n",
      "epoch: 2475 step: 1, loss is 0.026201575994491577\n",
      "epoch: 2476 step: 1, loss is 0.026168469339609146\n",
      "epoch: 2477 step: 1, loss is 0.02613542601466179\n",
      "epoch: 2478 step: 1, loss is 0.026102423667907715\n",
      "epoch: 2479 step: 1, loss is 0.026069480925798416\n",
      "epoch: 2480 step: 1, loss is 0.026036597788333893\n",
      "epoch: 2481 step: 1, loss is 0.0260037612169981\n",
      "epoch: 2482 step: 1, loss is 0.025970982387661934\n",
      "epoch: 2483 step: 1, loss is 0.02593824453651905\n",
      "epoch: 2484 step: 1, loss is 0.025905581191182137\n",
      "epoch: 2485 step: 1, loss is 0.025872958824038506\n",
      "epoch: 2486 step: 1, loss is 0.0258403941988945\n",
      "epoch: 2487 step: 1, loss is 0.025807876139879227\n",
      "epoch: 2488 step: 1, loss is 0.025775421410799026\n",
      "epoch: 2489 step: 1, loss is 0.025743013247847557\n",
      "epoch: 2490 step: 1, loss is 0.025710664689540863\n",
      "epoch: 2491 step: 1, loss is 0.02567836083471775\n",
      "epoch: 2492 step: 1, loss is 0.025646118447184563\n",
      "epoch: 2493 step: 1, loss is 0.02561391517519951\n",
      "epoch: 2494 step: 1, loss is 0.02558177337050438\n",
      "epoch: 2495 step: 1, loss is 0.02554967813193798\n",
      "epoch: 2496 step: 1, loss is 0.025517648085951805\n",
      "epoch: 2497 step: 1, loss is 0.02548566460609436\n",
      "epoch: 2498 step: 1, loss is 0.025453731417655945\n",
      "epoch: 2499 step: 1, loss is 0.025421850383281708\n",
      "epoch: 2500 step: 1, loss is 0.02539001777768135\n",
      "epoch: 2501 step: 1, loss is 0.02535824105143547\n",
      "epoch: 2502 step: 1, loss is 0.025326505303382874\n",
      "epoch: 2503 step: 1, loss is 0.025294845923781395\n",
      "epoch: 2504 step: 1, loss is 0.025263216346502304\n",
      "epoch: 2505 step: 1, loss is 0.025231650099158287\n",
      "epoch: 2506 step: 1, loss is 0.025200124830007553\n",
      "epoch: 2507 step: 1, loss is 0.025168653577566147\n",
      "epoch: 2508 step: 1, loss is 0.025137245655059814\n",
      "epoch: 2509 step: 1, loss is 0.025105873122811317\n",
      "epoch: 2510 step: 1, loss is 0.025074558332562447\n",
      "epoch: 2511 step: 1, loss is 0.025043295696377754\n",
      "epoch: 2512 step: 1, loss is 0.02501208335161209\n",
      "epoch: 2513 step: 1, loss is 0.024980925023555756\n",
      "epoch: 2514 step: 1, loss is 0.024949805811047554\n",
      "epoch: 2515 step: 1, loss is 0.024918748065829277\n",
      "epoch: 2516 step: 1, loss is 0.02488773688673973\n",
      "epoch: 2517 step: 1, loss is 0.02485678158700466\n",
      "epoch: 2518 step: 1, loss is 0.024825867265462875\n",
      "epoch: 2519 step: 1, loss is 0.02479500137269497\n",
      "epoch: 2520 step: 1, loss is 0.02476419322192669\n",
      "epoch: 2521 step: 1, loss is 0.02473343349993229\n",
      "epoch: 2522 step: 1, loss is 0.02470272406935692\n",
      "epoch: 2523 step: 1, loss is 0.024672066792845726\n",
      "epoch: 2524 step: 1, loss is 0.024641456082463264\n",
      "epoch: 2525 step: 1, loss is 0.02461089938879013\n",
      "epoch: 2526 step: 1, loss is 0.02458038553595543\n",
      "epoch: 2527 step: 1, loss is 0.024549921974539757\n",
      "epoch: 2528 step: 1, loss is 0.024519503116607666\n",
      "epoch: 2529 step: 1, loss is 0.02448914386332035\n",
      "epoch: 2530 step: 1, loss is 0.024458829313516617\n",
      "epoch: 2531 step: 1, loss is 0.024428561329841614\n",
      "epoch: 2532 step: 1, loss is 0.02439834736287594\n",
      "epoch: 2533 step: 1, loss is 0.024368170648813248\n",
      "epoch: 2534 step: 1, loss is 0.02433805726468563\n",
      "epoch: 2535 step: 1, loss is 0.024307990446686745\n",
      "epoch: 2536 step: 1, loss is 0.024277960881590843\n",
      "epoch: 2537 step: 1, loss is 0.024247990921139717\n",
      "epoch: 2538 step: 1, loss is 0.02421806938946247\n",
      "epoch: 2539 step: 1, loss is 0.024188188835978508\n",
      "epoch: 2540 step: 1, loss is 0.02415836602449417\n",
      "epoch: 2541 step: 1, loss is 0.024128582328557968\n",
      "epoch: 2542 step: 1, loss is 0.024098848924040794\n",
      "epoch: 2543 step: 1, loss is 0.02406916208565235\n",
      "epoch: 2544 step: 1, loss is 0.024039529263973236\n",
      "epoch: 2545 step: 1, loss is 0.024009937420487404\n",
      "epoch: 2546 step: 1, loss is 0.023980390280485153\n",
      "epoch: 2547 step: 1, loss is 0.02395089529454708\n",
      "epoch: 2548 step: 1, loss is 0.023921458050608635\n",
      "epoch: 2549 step: 1, loss is 0.023892056196928024\n",
      "epoch: 2550 step: 1, loss is 0.023862697184085846\n",
      "epoch: 2551 step: 1, loss is 0.02383340522646904\n",
      "epoch: 2552 step: 1, loss is 0.023804139345884323\n",
      "epoch: 2553 step: 1, loss is 0.023774921894073486\n",
      "epoch: 2554 step: 1, loss is 0.023745764046907425\n",
      "epoch: 2555 step: 1, loss is 0.023716645315289497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2556 step: 1, loss is 0.023687582463026047\n",
      "epoch: 2557 step: 1, loss is 0.023658541962504387\n",
      "epoch: 2558 step: 1, loss is 0.023629579693078995\n",
      "epoch: 2559 step: 1, loss is 0.02360064536333084\n",
      "epoch: 2560 step: 1, loss is 0.023571765050292015\n",
      "epoch: 2561 step: 1, loss is 0.02354292757809162\n",
      "epoch: 2562 step: 1, loss is 0.02351413667201996\n",
      "epoch: 2563 step: 1, loss is 0.02348538674414158\n",
      "epoch: 2564 step: 1, loss is 0.023456690832972527\n",
      "epoch: 2565 step: 1, loss is 0.02342803031206131\n",
      "epoch: 2566 step: 1, loss is 0.023399433121085167\n",
      "epoch: 2567 step: 1, loss is 0.023370865732431412\n",
      "epoch: 2568 step: 1, loss is 0.023342352360486984\n",
      "epoch: 2569 step: 1, loss is 0.02331387624144554\n",
      "epoch: 2570 step: 1, loss is 0.023285450413823128\n",
      "epoch: 2571 step: 1, loss is 0.023257073014974594\n",
      "epoch: 2572 step: 1, loss is 0.023228731006383896\n",
      "epoch: 2573 step: 1, loss is 0.023200444877147675\n",
      "epoch: 2574 step: 1, loss is 0.023172201588749886\n",
      "epoch: 2575 step: 1, loss is 0.023144003003835678\n",
      "epoch: 2576 step: 1, loss is 0.023115843534469604\n",
      "epoch: 2577 step: 1, loss is 0.02308773435652256\n",
      "epoch: 2578 step: 1, loss is 0.023059677332639694\n",
      "epoch: 2579 step: 1, loss is 0.023031655699014664\n",
      "epoch: 2580 step: 1, loss is 0.023003680631518364\n",
      "epoch: 2581 step: 1, loss is 0.02297574281692505\n",
      "epoch: 2582 step: 1, loss is 0.02294786274433136\n",
      "epoch: 2583 step: 1, loss is 0.02292001247406006\n",
      "epoch: 2584 step: 1, loss is 0.022892212495207787\n",
      "epoch: 2585 step: 1, loss is 0.022864464670419693\n",
      "epoch: 2586 step: 1, loss is 0.022836755961179733\n",
      "epoch: 2587 step: 1, loss is 0.022809099406003952\n",
      "epoch: 2588 step: 1, loss is 0.022781476378440857\n",
      "epoch: 2589 step: 1, loss is 0.022753892466425896\n",
      "epoch: 2590 step: 1, loss is 0.02272636443376541\n",
      "epoch: 2591 step: 1, loss is 0.02269887924194336\n",
      "epoch: 2592 step: 1, loss is 0.022671427577733994\n",
      "epoch: 2593 step: 1, loss is 0.022644028067588806\n",
      "epoch: 2594 step: 1, loss is 0.02261667139828205\n",
      "epoch: 2595 step: 1, loss is 0.022589365020394325\n",
      "epoch: 2596 step: 1, loss is 0.02256208285689354\n",
      "epoch: 2597 step: 1, loss is 0.02253485471010208\n",
      "epoch: 2598 step: 1, loss is 0.022507665678858757\n",
      "epoch: 2599 step: 1, loss is 0.02248053252696991\n",
      "epoch: 2600 step: 1, loss is 0.0224534273147583\n",
      "epoch: 2601 step: 1, loss is 0.022426364943385124\n",
      "epoch: 2602 step: 1, loss is 0.022399358451366425\n",
      "epoch: 2603 step: 1, loss is 0.022372383624315262\n",
      "epoch: 2604 step: 1, loss is 0.02234545722603798\n",
      "epoch: 2605 step: 1, loss is 0.022318581119179726\n",
      "epoch: 2606 step: 1, loss is 0.022291729226708412\n",
      "epoch: 2607 step: 1, loss is 0.02226492576301098\n",
      "epoch: 2608 step: 1, loss is 0.02223816141486168\n",
      "epoch: 2609 step: 1, loss is 0.02221144735813141\n",
      "epoch: 2610 step: 1, loss is 0.02218477427959442\n",
      "epoch: 2611 step: 1, loss is 0.022158145904541016\n",
      "epoch: 2612 step: 1, loss is 0.022131554782390594\n",
      "epoch: 2613 step: 1, loss is 0.022105006501078606\n",
      "epoch: 2614 step: 1, loss is 0.0220784954726696\n",
      "epoch: 2615 step: 1, loss is 0.022052036598324776\n",
      "epoch: 2616 step: 1, loss is 0.022025607526302338\n",
      "epoch: 2617 step: 1, loss is 0.021999230608344078\n",
      "epoch: 2618 step: 1, loss is 0.021972889080643654\n",
      "epoch: 2619 step: 1, loss is 0.02194659225642681\n",
      "epoch: 2620 step: 1, loss is 0.021920325234532356\n",
      "epoch: 2621 step: 1, loss is 0.021894117817282677\n",
      "epoch: 2622 step: 1, loss is 0.021867940202355385\n",
      "epoch: 2623 step: 1, loss is 0.02184181660413742\n",
      "epoch: 2624 step: 1, loss is 0.0218157097697258\n",
      "epoch: 2625 step: 1, loss is 0.021789666265249252\n",
      "epoch: 2626 step: 1, loss is 0.02176365815103054\n",
      "epoch: 2627 step: 1, loss is 0.021737683564424515\n",
      "epoch: 2628 step: 1, loss is 0.02171175181865692\n",
      "epoch: 2629 step: 1, loss is 0.021685868501663208\n",
      "epoch: 2630 step: 1, loss is 0.021660028025507927\n",
      "epoch: 2631 step: 1, loss is 0.021634215489029884\n",
      "epoch: 2632 step: 1, loss is 0.021608451381325722\n",
      "epoch: 2633 step: 1, loss is 0.021582722663879395\n",
      "epoch: 2634 step: 1, loss is 0.02155703864991665\n",
      "epoch: 2635 step: 1, loss is 0.021531397476792336\n",
      "epoch: 2636 step: 1, loss is 0.02150578796863556\n",
      "epoch: 2637 step: 1, loss is 0.021480223163962364\n",
      "epoch: 2638 step: 1, loss is 0.021454699337482452\n",
      "epoch: 2639 step: 1, loss is 0.021429212763905525\n",
      "epoch: 2640 step: 1, loss is 0.02140377089381218\n",
      "epoch: 2641 step: 1, loss is 0.02137836068868637\n",
      "epoch: 2642 step: 1, loss is 0.021352998912334442\n",
      "epoch: 2643 step: 1, loss is 0.0213276669383049\n",
      "epoch: 2644 step: 1, loss is 0.02130238153040409\n",
      "epoch: 2645 step: 1, loss is 0.021277133375406265\n",
      "epoch: 2646 step: 1, loss is 0.021251928061246872\n",
      "epoch: 2647 step: 1, loss is 0.021226759999990463\n",
      "epoch: 2648 step: 1, loss is 0.02120163105428219\n",
      "epoch: 2649 step: 1, loss is 0.021176539361476898\n",
      "epoch: 2650 step: 1, loss is 0.02115149050951004\n",
      "epoch: 2651 step: 1, loss is 0.021126477047801018\n",
      "epoch: 2652 step: 1, loss is 0.02110150456428528\n",
      "epoch: 2653 step: 1, loss is 0.021076573058962822\n",
      "epoch: 2654 step: 1, loss is 0.0210516769438982\n",
      "epoch: 2655 step: 1, loss is 0.021026818081736565\n",
      "epoch: 2656 step: 1, loss is 0.02100200019776821\n",
      "epoch: 2657 step: 1, loss is 0.02097722329199314\n",
      "epoch: 2658 step: 1, loss is 0.020952485501766205\n",
      "epoch: 2659 step: 1, loss is 0.020927779376506805\n",
      "epoch: 2660 step: 1, loss is 0.02090311609208584\n",
      "epoch: 2661 step: 1, loss is 0.020878486335277557\n",
      "epoch: 2662 step: 1, loss is 0.020853903144598007\n",
      "epoch: 2663 step: 1, loss is 0.020829355344176292\n",
      "epoch: 2664 step: 1, loss is 0.02080484665930271\n",
      "epoch: 2665 step: 1, loss is 0.020780369639396667\n",
      "epoch: 2666 step: 1, loss is 0.020755935460329056\n",
      "epoch: 2667 step: 1, loss is 0.020731531083583832\n",
      "epoch: 2668 step: 1, loss is 0.020707178860902786\n",
      "epoch: 2669 step: 1, loss is 0.020682858303189278\n",
      "epoch: 2670 step: 1, loss is 0.020658574998378754\n",
      "epoch: 2671 step: 1, loss is 0.020634327083826065\n",
      "epoch: 2672 step: 1, loss is 0.020610114559531212\n",
      "epoch: 2673 step: 1, loss is 0.020585941150784492\n",
      "epoch: 2674 step: 1, loss is 0.020561812445521355\n",
      "epoch: 2675 step: 1, loss is 0.0205377209931612\n",
      "epoch: 2676 step: 1, loss is 0.020513661205768585\n",
      "epoch: 2677 step: 1, loss is 0.020489633083343506\n",
      "epoch: 2678 step: 1, loss is 0.020465649664402008\n",
      "epoch: 2679 step: 1, loss is 0.020441699773073196\n",
      "epoch: 2680 step: 1, loss is 0.02041778899729252\n",
      "epoch: 2681 step: 1, loss is 0.020393913611769676\n",
      "epoch: 2682 step: 1, loss is 0.020370081067085266\n",
      "epoch: 2683 step: 1, loss is 0.020346272736787796\n",
      "epoch: 2684 step: 1, loss is 0.020322510972619057\n",
      "epoch: 2685 step: 1, loss is 0.020298782736063004\n",
      "epoch: 2686 step: 1, loss is 0.020275089889764786\n",
      "epoch: 2687 step: 1, loss is 0.020251432433724403\n",
      "epoch: 2688 step: 1, loss is 0.020227817818522453\n",
      "epoch: 2689 step: 1, loss is 0.02020423859357834\n",
      "epoch: 2690 step: 1, loss is 0.02018069103360176\n",
      "epoch: 2691 step: 1, loss is 0.02015717886388302\n",
      "epoch: 2692 step: 1, loss is 0.020133711397647858\n",
      "epoch: 2693 step: 1, loss is 0.020110273733735085\n",
      "epoch: 2694 step: 1, loss is 0.020086869597434998\n",
      "epoch: 2695 step: 1, loss is 0.020063508301973343\n",
      "epoch: 2696 step: 1, loss is 0.020040171220898628\n",
      "epoch: 2697 step: 1, loss is 0.020016884431242943\n",
      "epoch: 2698 step: 1, loss is 0.019993625581264496\n",
      "epoch: 2699 step: 1, loss is 0.019970405846834183\n",
      "epoch: 2700 step: 1, loss is 0.019947219640016556\n",
      "epoch: 2701 step: 1, loss is 0.019924068823456764\n",
      "epoch: 2702 step: 1, loss is 0.01990095153450966\n",
      "epoch: 2703 step: 1, loss is 0.019877873361110687\n",
      "epoch: 2704 step: 1, loss is 0.019854824990034103\n",
      "epoch: 2705 step: 1, loss is 0.0198318213224411\n",
      "epoch: 2706 step: 1, loss is 0.019808847457170486\n",
      "epoch: 2707 step: 1, loss is 0.019785908982157707\n",
      "epoch: 2708 step: 1, loss is 0.019762994721531868\n",
      "epoch: 2709 step: 1, loss is 0.01974012888967991\n",
      "epoch: 2710 step: 1, loss is 0.019717298448085785\n",
      "epoch: 2711 step: 1, loss is 0.019694503396749496\n",
      "epoch: 2712 step: 1, loss is 0.019671732559800148\n",
      "epoch: 2713 step: 1, loss is 0.019649002701044083\n",
      "epoch: 2714 step: 1, loss is 0.019626306369900703\n",
      "epoch: 2715 step: 1, loss is 0.019603649154305458\n",
      "epoch: 2716 step: 1, loss is 0.019581016153097153\n",
      "epoch: 2717 step: 1, loss is 0.019558433443307877\n",
      "epoch: 2718 step: 1, loss is 0.01953587308526039\n",
      "epoch: 2719 step: 1, loss is 0.01951335370540619\n",
      "epoch: 2720 step: 1, loss is 0.019490869715809822\n",
      "epoch: 2721 step: 1, loss is 0.019468406215310097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2722 step: 1, loss is 0.019445989280939102\n",
      "epoch: 2723 step: 1, loss is 0.019423602148890495\n",
      "epoch: 2724 step: 1, loss is 0.019401252269744873\n",
      "epoch: 2725 step: 1, loss is 0.01937893219292164\n",
      "epoch: 2726 step: 1, loss is 0.019356651231646538\n",
      "epoch: 2727 step: 1, loss is 0.019334400072693825\n",
      "epoch: 2728 step: 1, loss is 0.01931218057870865\n",
      "epoch: 2729 step: 1, loss is 0.019290000200271606\n",
      "epoch: 2730 step: 1, loss is 0.0192678552120924\n",
      "epoch: 2731 step: 1, loss is 0.019245736300945282\n",
      "epoch: 2732 step: 1, loss is 0.019223652780056\n",
      "epoch: 2733 step: 1, loss is 0.01920160837471485\n",
      "epoch: 2734 step: 1, loss is 0.019179590046405792\n",
      "epoch: 2735 step: 1, loss is 0.019157610833644867\n",
      "epoch: 2736 step: 1, loss is 0.01913566328585148\n",
      "epoch: 2737 step: 1, loss is 0.019113754853606224\n",
      "epoch: 2738 step: 1, loss is 0.01909187249839306\n",
      "epoch: 2739 step: 1, loss is 0.01907001994550228\n",
      "epoch: 2740 step: 1, loss is 0.01904820278286934\n",
      "epoch: 2741 step: 1, loss is 0.019026419147849083\n",
      "epoch: 2742 step: 1, loss is 0.01900467462837696\n",
      "epoch: 2743 step: 1, loss is 0.01898295246064663\n",
      "epoch: 2744 step: 1, loss is 0.018961265683174133\n",
      "epoch: 2745 step: 1, loss is 0.01893962174654007\n",
      "epoch: 2746 step: 1, loss is 0.018918000161647797\n",
      "epoch: 2747 step: 1, loss is 0.01889641396701336\n",
      "epoch: 2748 step: 1, loss is 0.01887485757470131\n",
      "epoch: 2749 step: 1, loss is 0.018853342160582542\n",
      "epoch: 2750 step: 1, loss is 0.018831850960850716\n",
      "epoch: 2751 step: 1, loss is 0.018810398876667023\n",
      "epoch: 2752 step: 1, loss is 0.01878896728157997\n",
      "epoch: 2753 step: 1, loss is 0.018767576664686203\n",
      "epoch: 2754 step: 1, loss is 0.018746215850114822\n",
      "epoch: 2755 step: 1, loss is 0.018724890425801277\n",
      "epoch: 2756 step: 1, loss is 0.01870359480381012\n",
      "epoch: 2757 step: 1, loss is 0.018682334572076797\n",
      "epoch: 2758 step: 1, loss is 0.018661100417375565\n",
      "epoch: 2759 step: 1, loss is 0.018639909103512764\n",
      "epoch: 2760 step: 1, loss is 0.018618736416101456\n",
      "epoch: 2761 step: 1, loss is 0.018597599118947983\n",
      "epoch: 2762 step: 1, loss is 0.018576493486762047\n",
      "epoch: 2763 step: 1, loss is 0.018555425107479095\n",
      "epoch: 2764 step: 1, loss is 0.018534382805228233\n",
      "epoch: 2765 step: 1, loss is 0.01851337030529976\n",
      "epoch: 2766 step: 1, loss is 0.01849239692091942\n",
      "epoch: 2767 step: 1, loss is 0.018471449613571167\n",
      "epoch: 2768 step: 1, loss is 0.018450528383255005\n",
      "epoch: 2769 step: 1, loss is 0.018429642543196678\n",
      "epoch: 2770 step: 1, loss is 0.018408797681331635\n",
      "epoch: 2771 step: 1, loss is 0.018387971445918083\n",
      "epoch: 2772 step: 1, loss is 0.018367182463407516\n",
      "epoch: 2773 step: 1, loss is 0.018346427008509636\n",
      "epoch: 2774 step: 1, loss is 0.018325693905353546\n",
      "epoch: 2775 step: 1, loss is 0.01830499805510044\n",
      "epoch: 2776 step: 1, loss is 0.018284333869814873\n",
      "epoch: 2777 step: 1, loss is 0.01826370693743229\n",
      "epoch: 2778 step: 1, loss is 0.018243098631501198\n",
      "epoch: 2779 step: 1, loss is 0.01822252757847309\n",
      "epoch: 2780 step: 1, loss is 0.018201977014541626\n",
      "epoch: 2781 step: 1, loss is 0.018181459978222847\n",
      "epoch: 2782 step: 1, loss is 0.01816098764538765\n",
      "epoch: 2783 step: 1, loss is 0.018140535801649094\n",
      "epoch: 2784 step: 1, loss is 0.018120113760232925\n",
      "epoch: 2785 step: 1, loss is 0.018099728971719742\n",
      "epoch: 2786 step: 1, loss is 0.018079359084367752\n",
      "epoch: 2787 step: 1, loss is 0.018059024587273598\n",
      "epoch: 2788 step: 1, loss is 0.018038732931017876\n",
      "epoch: 2789 step: 1, loss is 0.018018463626503944\n",
      "epoch: 2790 step: 1, loss is 0.017998218536376953\n",
      "epoch: 2791 step: 1, loss is 0.017978006973862648\n",
      "epoch: 2792 step: 1, loss is 0.017957834526896477\n",
      "epoch: 2793 step: 1, loss is 0.017937682569026947\n",
      "epoch: 2794 step: 1, loss is 0.017917566001415253\n",
      "epoch: 2795 step: 1, loss is 0.017897477373480797\n",
      "epoch: 2796 step: 1, loss is 0.017877422273159027\n",
      "epoch: 2797 step: 1, loss is 0.0178573876619339\n",
      "epoch: 2798 step: 1, loss is 0.017837392166256905\n",
      "epoch: 2799 step: 1, loss is 0.01781742088496685\n",
      "epoch: 2800 step: 1, loss is 0.017797479405999184\n",
      "epoch: 2801 step: 1, loss is 0.017777562141418457\n",
      "epoch: 2802 step: 1, loss is 0.017757683992385864\n",
      "epoch: 2803 step: 1, loss is 0.01773783192038536\n",
      "epoch: 2804 step: 1, loss is 0.017718007788062096\n",
      "epoch: 2805 step: 1, loss is 0.017698215320706367\n",
      "epoch: 2806 step: 1, loss is 0.017678450793027878\n",
      "epoch: 2807 step: 1, loss is 0.017658714205026627\n",
      "epoch: 2808 step: 1, loss is 0.01763901114463806\n",
      "epoch: 2809 step: 1, loss is 0.017619330435991287\n",
      "epoch: 2810 step: 1, loss is 0.01759968139231205\n",
      "epoch: 2811 step: 1, loss is 0.0175800658762455\n",
      "epoch: 2812 step: 1, loss is 0.017560472711920738\n",
      "epoch: 2813 step: 1, loss is 0.017540916800498962\n",
      "epoch: 2814 step: 1, loss is 0.017521381378173828\n",
      "epoch: 2815 step: 1, loss is 0.01750187948346138\n",
      "epoch: 2816 step: 1, loss is 0.017482398077845573\n",
      "epoch: 2817 step: 1, loss is 0.0174629557877779\n",
      "epoch: 2818 step: 1, loss is 0.017443539574742317\n",
      "epoch: 2819 step: 1, loss is 0.017424147576093674\n",
      "epoch: 2820 step: 1, loss is 0.017404789105057716\n",
      "epoch: 2821 step: 1, loss is 0.0173854548484087\n",
      "epoch: 2822 step: 1, loss is 0.01736615225672722\n",
      "epoch: 2823 step: 1, loss is 0.017346877604722977\n",
      "epoch: 2824 step: 1, loss is 0.017327630892395973\n",
      "epoch: 2825 step: 1, loss is 0.017308412119746208\n",
      "epoch: 2826 step: 1, loss is 0.01728922501206398\n",
      "epoch: 2827 step: 1, loss is 0.017270056530833244\n",
      "epoch: 2828 step: 1, loss is 0.017250923439860344\n",
      "epoch: 2829 step: 1, loss is 0.017231818288564682\n",
      "epoch: 2830 step: 1, loss is 0.01721273921430111\n",
      "epoch: 2831 step: 1, loss is 0.017193682491779327\n",
      "epoch: 2832 step: 1, loss is 0.017174668610095978\n",
      "epoch: 2833 step: 1, loss is 0.01715567335486412\n",
      "epoch: 2834 step: 1, loss is 0.01713670790195465\n",
      "epoch: 2835 step: 1, loss is 0.01711776852607727\n",
      "epoch: 2836 step: 1, loss is 0.01709885336458683\n",
      "epoch: 2837 step: 1, loss is 0.017079973593354225\n",
      "epoch: 2838 step: 1, loss is 0.017061108723282814\n",
      "epoch: 2839 step: 1, loss is 0.017042282968759537\n",
      "epoch: 2840 step: 1, loss is 0.017023487016558647\n",
      "epoch: 2841 step: 1, loss is 0.0170047115534544\n",
      "epoch: 2842 step: 1, loss is 0.016985967755317688\n",
      "epoch: 2843 step: 1, loss is 0.016967251896858215\n",
      "epoch: 2844 step: 1, loss is 0.01694856397807598\n",
      "epoch: 2845 step: 1, loss is 0.01692989282310009\n",
      "epoch: 2846 step: 1, loss is 0.016911257058382034\n",
      "epoch: 2847 step: 1, loss is 0.01689264364540577\n",
      "epoch: 2848 step: 1, loss is 0.01687406562268734\n",
      "epoch: 2849 step: 1, loss is 0.016855508089065552\n",
      "epoch: 2850 step: 1, loss is 0.01683698035776615\n",
      "epoch: 2851 step: 1, loss is 0.01681847684085369\n",
      "epoch: 2852 step: 1, loss is 0.01679999940097332\n",
      "epoch: 2853 step: 1, loss is 0.016781549900770187\n",
      "epoch: 2854 step: 1, loss is 0.016763128340244293\n",
      "epoch: 2855 step: 1, loss is 0.016744734719395638\n",
      "epoch: 2856 step: 1, loss is 0.016726363450288773\n",
      "epoch: 2857 step: 1, loss is 0.016708027571439743\n",
      "epoch: 2858 step: 1, loss is 0.016689712181687355\n",
      "epoch: 2859 step: 1, loss is 0.01667141541838646\n",
      "epoch: 2860 step: 1, loss is 0.016653157770633698\n",
      "epoch: 2861 step: 1, loss is 0.016634922474622726\n",
      "epoch: 2862 step: 1, loss is 0.016616716980934143\n",
      "epoch: 2863 step: 1, loss is 0.01659853756427765\n",
      "epoch: 2864 step: 1, loss is 0.016580376774072647\n",
      "epoch: 2865 step: 1, loss is 0.01656224951148033\n",
      "epoch: 2866 step: 1, loss is 0.016544148325920105\n",
      "epoch: 2867 step: 1, loss is 0.016526075080037117\n",
      "epoch: 2868 step: 1, loss is 0.01650802232325077\n",
      "epoch: 2869 step: 1, loss is 0.016489995643496513\n",
      "epoch: 2870 step: 1, loss is 0.016471995040774345\n",
      "epoch: 2871 step: 1, loss is 0.016454026103019714\n",
      "epoch: 2872 step: 1, loss is 0.016436072066426277\n",
      "epoch: 2873 step: 1, loss is 0.016418153420090675\n",
      "epoch: 2874 step: 1, loss is 0.016400253400206566\n",
      "epoch: 2875 step: 1, loss is 0.01638239249587059\n",
      "epoch: 2876 step: 1, loss is 0.01636454649269581\n",
      "epoch: 2877 step: 1, loss is 0.016346728429198265\n",
      "epoch: 2878 step: 1, loss is 0.016328934580087662\n",
      "epoch: 2879 step: 1, loss is 0.016311164945364\n",
      "epoch: 2880 step: 1, loss is 0.01629342883825302\n",
      "epoch: 2881 step: 1, loss is 0.016275711357593536\n",
      "epoch: 2882 step: 1, loss is 0.01625801809132099\n",
      "epoch: 2883 step: 1, loss is 0.016240354627370834\n",
      "epoch: 2884 step: 1, loss is 0.016222719103097916\n",
      "epoch: 2885 step: 1, loss is 0.016205107793211937\n",
      "epoch: 2886 step: 1, loss is 0.0161875132471323\n",
      "epoch: 2887 step: 1, loss is 0.0161699540913105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2888 step: 1, loss is 0.016152413561940193\n",
      "epoch: 2889 step: 1, loss is 0.016134902834892273\n",
      "epoch: 2890 step: 1, loss is 0.016117412596940994\n",
      "epoch: 2891 step: 1, loss is 0.016099948436021805\n",
      "epoch: 2892 step: 1, loss is 0.016082512214779854\n",
      "epoch: 2893 step: 1, loss is 0.016065100207924843\n",
      "epoch: 2894 step: 1, loss is 0.016047704964876175\n",
      "epoch: 2895 step: 1, loss is 0.01603035256266594\n",
      "epoch: 2896 step: 1, loss is 0.0160130113363266\n",
      "epoch: 2897 step: 1, loss is 0.015995703637599945\n",
      "epoch: 2898 step: 1, loss is 0.015978410840034485\n",
      "epoch: 2899 step: 1, loss is 0.015961147844791412\n",
      "epoch: 2900 step: 1, loss is 0.015943914651870728\n",
      "epoch: 2901 step: 1, loss is 0.015926694497466087\n",
      "epoch: 2902 step: 1, loss is 0.015909506008028984\n",
      "epoch: 2903 step: 1, loss is 0.01589234359562397\n",
      "epoch: 2904 step: 1, loss is 0.015875205397605896\n",
      "epoch: 2905 step: 1, loss is 0.015858085826039314\n",
      "epoch: 2906 step: 1, loss is 0.015840990468859673\n",
      "epoch: 2907 step: 1, loss is 0.01582392491400242\n",
      "epoch: 2908 step: 1, loss is 0.015806883573532104\n",
      "epoch: 2909 step: 1, loss is 0.015789860859513283\n",
      "epoch: 2910 step: 1, loss is 0.015772871673107147\n",
      "epoch: 2911 step: 1, loss is 0.015755897387862206\n",
      "epoch: 2912 step: 1, loss is 0.01573895290493965\n",
      "epoch: 2913 step: 1, loss is 0.015722032636404037\n",
      "epoch: 2914 step: 1, loss is 0.015705132856965065\n",
      "epoch: 2915 step: 1, loss is 0.01568826101720333\n",
      "epoch: 2916 step: 1, loss is 0.01567140780389309\n",
      "epoch: 2917 step: 1, loss is 0.015654586255550385\n",
      "epoch: 2918 step: 1, loss is 0.015637783333659172\n",
      "epoch: 2919 step: 1, loss is 0.015621010214090347\n",
      "epoch: 2920 step: 1, loss is 0.015604249201714993\n",
      "epoch: 2921 step: 1, loss is 0.015587519854307175\n",
      "epoch: 2922 step: 1, loss is 0.015570808202028275\n",
      "epoch: 2923 step: 1, loss is 0.01555413007736206\n",
      "epoch: 2924 step: 1, loss is 0.01553746685385704\n",
      "epoch: 2925 step: 1, loss is 0.015520834363996983\n",
      "epoch: 2926 step: 1, loss is 0.015504226088523865\n",
      "epoch: 2927 step: 1, loss is 0.01548763271421194\n",
      "epoch: 2928 step: 1, loss is 0.015471071936190128\n",
      "epoch: 2929 step: 1, loss is 0.01545452605932951\n",
      "epoch: 2930 step: 1, loss is 0.015438007190823555\n",
      "epoch: 2931 step: 1, loss is 0.015421516261994839\n",
      "epoch: 2932 step: 1, loss is 0.015405047684907913\n",
      "epoch: 2933 step: 1, loss is 0.015388593077659607\n",
      "epoch: 2934 step: 1, loss is 0.015372168272733688\n",
      "epoch: 2935 step: 1, loss is 0.015355771407485008\n",
      "epoch: 2936 step: 1, loss is 0.015339386649429798\n",
      "epoch: 2937 step: 1, loss is 0.015323037281632423\n",
      "epoch: 2938 step: 1, loss is 0.015306701883673668\n",
      "epoch: 2939 step: 1, loss is 0.015290390700101852\n",
      "epoch: 2940 step: 1, loss is 0.015274100005626678\n",
      "epoch: 2941 step: 1, loss is 0.015257839113473892\n",
      "epoch: 2942 step: 1, loss is 0.015241598710417747\n",
      "epoch: 2943 step: 1, loss is 0.015225379727780819\n",
      "epoch: 2944 step: 1, loss is 0.01520918495953083\n",
      "epoch: 2945 step: 1, loss is 0.015193015336990356\n",
      "epoch: 2946 step: 1, loss is 0.015176866203546524\n",
      "epoch: 2947 step: 1, loss is 0.015160731039941311\n",
      "epoch: 2948 step: 1, loss is 0.015144634060561657\n",
      "epoch: 2949 step: 1, loss is 0.01512855477631092\n",
      "epoch: 2950 step: 1, loss is 0.015112494118511677\n",
      "epoch: 2951 step: 1, loss is 0.015096455812454224\n",
      "epoch: 2952 step: 1, loss is 0.015080442652106285\n",
      "epoch: 2953 step: 1, loss is 0.015064451843500137\n",
      "epoch: 2954 step: 1, loss is 0.01504848524928093\n",
      "epoch: 2955 step: 1, loss is 0.01503253635019064\n",
      "epoch: 2956 step: 1, loss is 0.015016608871519566\n",
      "epoch: 2957 step: 1, loss is 0.015000708401203156\n",
      "epoch: 2958 step: 1, loss is 0.01498483121395111\n",
      "epoch: 2959 step: 1, loss is 0.014968974515795708\n",
      "epoch: 2960 step: 1, loss is 0.014953140169382095\n",
      "epoch: 2961 step: 1, loss is 0.014937326312065125\n",
      "epoch: 2962 step: 1, loss is 0.014921532943844795\n",
      "epoch: 2963 step: 1, loss is 0.01490576472133398\n",
      "epoch: 2964 step: 1, loss is 0.014890018850564957\n",
      "epoch: 2965 step: 1, loss is 0.014874289743602276\n",
      "epoch: 2966 step: 1, loss is 0.014858588576316833\n",
      "epoch: 2967 step: 1, loss is 0.014842912554740906\n",
      "epoch: 2968 step: 1, loss is 0.014827248640358448\n",
      "epoch: 2969 step: 1, loss is 0.01481160894036293\n",
      "epoch: 2970 step: 1, loss is 0.014795998111367226\n",
      "epoch: 2971 step: 1, loss is 0.014780404046177864\n",
      "epoch: 2972 step: 1, loss is 0.014764831401407719\n",
      "epoch: 2973 step: 1, loss is 0.014749283902347088\n",
      "epoch: 2974 step: 1, loss is 0.014733754098415375\n",
      "epoch: 2975 step: 1, loss is 0.014718248508870602\n",
      "epoch: 2976 step: 1, loss is 0.014702760614454746\n",
      "epoch: 2977 step: 1, loss is 0.014687297865748405\n",
      "epoch: 2978 step: 1, loss is 0.014671854674816132\n",
      "epoch: 2979 step: 1, loss is 0.014656434766948223\n",
      "epoch: 2980 step: 1, loss is 0.014641035348176956\n",
      "epoch: 2981 step: 1, loss is 0.014625653624534607\n",
      "epoch: 2982 step: 1, loss is 0.014610297977924347\n",
      "epoch: 2983 step: 1, loss is 0.014594962820410728\n",
      "epoch: 2984 step: 1, loss is 0.014579647220671177\n",
      "epoch: 2985 step: 1, loss is 0.014564359560608864\n",
      "epoch: 2986 step: 1, loss is 0.014549081213772297\n",
      "epoch: 2987 step: 1, loss is 0.014533836394548416\n",
      "epoch: 2988 step: 1, loss is 0.014518605545163155\n",
      "epoch: 2989 step: 1, loss is 0.014503397047519684\n",
      "epoch: 2990 step: 1, loss is 0.014488213695585728\n",
      "epoch: 2991 step: 1, loss is 0.01447304617613554\n",
      "epoch: 2992 step: 1, loss is 0.014457901939749718\n",
      "epoch: 2993 step: 1, loss is 0.014442774467170238\n",
      "epoch: 2994 step: 1, loss is 0.014427673071622849\n",
      "epoch: 2995 step: 1, loss is 0.014412587508559227\n",
      "epoch: 2996 step: 1, loss is 0.014397528022527695\n",
      "epoch: 2997 step: 1, loss is 0.014382481575012207\n",
      "epoch: 2998 step: 1, loss is 0.014367466792464256\n",
      "epoch: 2999 step: 1, loss is 0.014352464117109776\n",
      "epoch: 3000 step: 1, loss is 0.014337489381432533\n",
      "epoch: 3001 step: 1, loss is 0.014322531409561634\n",
      "epoch: 3002 step: 1, loss is 0.014307593926787376\n",
      "epoch: 3003 step: 1, loss is 0.014292681589722633\n",
      "epoch: 3004 step: 1, loss is 0.01427778135985136\n",
      "epoch: 3005 step: 1, loss is 0.0142629100009799\n",
      "epoch: 3006 step: 1, loss is 0.014248056337237358\n",
      "epoch: 3007 step: 1, loss is 0.014233218505978584\n",
      "epoch: 3008 step: 1, loss is 0.014218401163816452\n",
      "epoch: 3009 step: 1, loss is 0.014203607104718685\n",
      "epoch: 3010 step: 1, loss is 0.014188842847943306\n",
      "epoch: 3011 step: 1, loss is 0.014174084179103374\n",
      "epoch: 3012 step: 1, loss is 0.014159351587295532\n",
      "epoch: 3013 step: 1, loss is 0.01414464134722948\n",
      "epoch: 3014 step: 1, loss is 0.014129949733614922\n",
      "epoch: 3015 step: 1, loss is 0.014115273021161556\n",
      "epoch: 3016 step: 1, loss is 0.01410062238574028\n",
      "epoch: 3017 step: 1, loss is 0.014085992239415646\n",
      "epoch: 3018 step: 1, loss is 0.014071376994252205\n",
      "epoch: 3019 step: 1, loss is 0.014056788757443428\n",
      "epoch: 3020 step: 1, loss is 0.014042212627828121\n",
      "epoch: 3021 step: 1, loss is 0.014027664437890053\n",
      "epoch: 3022 step: 1, loss is 0.014013131149113178\n",
      "epoch: 3023 step: 1, loss is 0.01399861741811037\n",
      "epoch: 3024 step: 1, loss is 0.013984122313559055\n",
      "epoch: 3025 step: 1, loss is 0.013969650492072105\n",
      "epoch: 3026 step: 1, loss is 0.013955197297036648\n",
      "epoch: 3027 step: 1, loss is 0.01394076831638813\n",
      "epoch: 3028 step: 1, loss is 0.013926354236900806\n",
      "epoch: 3029 step: 1, loss is 0.01391195971518755\n",
      "epoch: 3030 step: 1, loss is 0.013897586613893509\n",
      "epoch: 3031 step: 1, loss is 0.01388323213905096\n",
      "epoch: 3032 step: 1, loss is 0.013868897221982479\n",
      "epoch: 3033 step: 1, loss is 0.013854581862688065\n",
      "epoch: 3034 step: 1, loss is 0.013840284198522568\n",
      "epoch: 3035 step: 1, loss is 0.01382601447403431\n",
      "epoch: 3036 step: 1, loss is 0.013811754062771797\n",
      "epoch: 3037 step: 1, loss is 0.013797514140605927\n",
      "epoch: 3038 step: 1, loss is 0.01378329936414957\n",
      "epoch: 3039 step: 1, loss is 0.013769100420176983\n",
      "epoch: 3040 step: 1, loss is 0.013754921033978462\n",
      "epoch: 3041 step: 1, loss is 0.01374075934290886\n",
      "epoch: 3042 step: 1, loss is 0.013726620003581047\n",
      "epoch: 3043 step: 1, loss is 0.013712496496737003\n",
      "epoch: 3044 step: 1, loss is 0.0136983972042799\n",
      "epoch: 3045 step: 1, loss is 0.01368431095033884\n",
      "epoch: 3046 step: 1, loss is 0.013670245185494423\n",
      "epoch: 3047 step: 1, loss is 0.013656203635036945\n",
      "epoch: 3048 step: 1, loss is 0.013642175123095512\n",
      "epoch: 3049 step: 1, loss is 0.013628172688186169\n",
      "epoch: 3050 step: 1, loss is 0.013614177703857422\n",
      "epoch: 3051 step: 1, loss is 0.013600213453173637\n",
      "epoch: 3052 step: 1, loss is 0.013586265034973621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3053 step: 1, loss is 0.01357232965528965\n",
      "epoch: 3054 step: 1, loss is 0.013558415696024895\n",
      "epoch: 3055 step: 1, loss is 0.013544529676437378\n",
      "epoch: 3056 step: 1, loss is 0.013530649244785309\n",
      "epoch: 3057 step: 1, loss is 0.013516797684133053\n",
      "epoch: 3058 step: 1, loss is 0.013502960093319416\n",
      "epoch: 3059 step: 1, loss is 0.013489142060279846\n",
      "epoch: 3060 step: 1, loss is 0.013475334271788597\n",
      "epoch: 3061 step: 1, loss is 0.013461556285619736\n",
      "epoch: 3062 step: 1, loss is 0.013447792269289494\n",
      "epoch: 3063 step: 1, loss is 0.013434055261313915\n",
      "epoch: 3064 step: 1, loss is 0.013420323841273785\n",
      "epoch: 3065 step: 1, loss is 0.013406622223556042\n",
      "epoch: 3066 step: 1, loss is 0.013392929919064045\n",
      "epoch: 3067 step: 1, loss is 0.013379260897636414\n",
      "epoch: 3068 step: 1, loss is 0.01336560770869255\n",
      "epoch: 3069 step: 1, loss is 0.013351976871490479\n",
      "epoch: 3070 step: 1, loss is 0.013338364660739899\n",
      "epoch: 3071 step: 1, loss is 0.01332476083189249\n",
      "epoch: 3072 step: 1, loss is 0.013311185874044895\n",
      "epoch: 3073 step: 1, loss is 0.013297625817358494\n",
      "epoch: 3074 step: 1, loss is 0.013284079730510712\n",
      "epoch: 3075 step: 1, loss is 0.013270562514662743\n",
      "epoch: 3076 step: 1, loss is 0.01325705461204052\n",
      "epoch: 3077 step: 1, loss is 0.013243565335869789\n",
      "epoch: 3078 step: 1, loss is 0.01323009468615055\n",
      "epoch: 3079 step: 1, loss is 0.013216644525527954\n",
      "epoch: 3080 step: 1, loss is 0.013203212060034275\n",
      "epoch: 3081 step: 1, loss is 0.013189797289669514\n",
      "epoch: 3082 step: 1, loss is 0.013176403939723969\n",
      "epoch: 3083 step: 1, loss is 0.01316301990300417\n",
      "epoch: 3084 step: 1, loss is 0.01314966008067131\n",
      "epoch: 3085 step: 1, loss is 0.013136317022144794\n",
      "epoch: 3086 step: 1, loss is 0.013122990727424622\n",
      "epoch: 3087 step: 1, loss is 0.01310968492180109\n",
      "epoch: 3088 step: 1, loss is 0.013096392154693604\n",
      "epoch: 3089 step: 1, loss is 0.013083124533295631\n",
      "epoch: 3090 step: 1, loss is 0.013069866225123405\n",
      "epoch: 3091 step: 1, loss is 0.01305663026869297\n",
      "epoch: 3092 step: 1, loss is 0.013043409213423729\n",
      "epoch: 3093 step: 1, loss is 0.013030212372541428\n",
      "epoch: 3094 step: 1, loss is 0.01301703043282032\n",
      "epoch: 3095 step: 1, loss is 0.013003864325582981\n",
      "epoch: 3096 step: 1, loss is 0.01299071591347456\n",
      "epoch: 3097 step: 1, loss is 0.012977584265172482\n",
      "epoch: 3098 step: 1, loss is 0.012964467518031597\n",
      "epoch: 3099 step: 1, loss is 0.012951375916600227\n",
      "epoch: 3100 step: 1, loss is 0.01293829083442688\n",
      "epoch: 3101 step: 1, loss is 0.012925230897963047\n",
      "epoch: 3102 step: 1, loss is 0.012912189587950706\n",
      "epoch: 3103 step: 1, loss is 0.012899166904389858\n",
      "epoch: 3104 step: 1, loss is 0.012886153534054756\n",
      "epoch: 3105 step: 1, loss is 0.01287316344678402\n",
      "epoch: 3106 step: 1, loss is 0.012860190123319626\n",
      "epoch: 3107 step: 1, loss is 0.012847233563661575\n",
      "epoch: 3108 step: 1, loss is 0.012834292836487293\n",
      "epoch: 3109 step: 1, loss is 0.012821368873119354\n",
      "epoch: 3110 step: 1, loss is 0.012808466330170631\n",
      "epoch: 3111 step: 1, loss is 0.012795571237802505\n",
      "epoch: 3112 step: 1, loss is 0.012782703153789043\n",
      "epoch: 3113 step: 1, loss is 0.0127698490396142\n",
      "epoch: 3114 step: 1, loss is 0.012757010757923126\n",
      "epoch: 3115 step: 1, loss is 0.012744192034006119\n",
      "epoch: 3116 step: 1, loss is 0.012731386348605156\n",
      "epoch: 3117 step: 1, loss is 0.012718597427010536\n",
      "epoch: 3118 step: 1, loss is 0.012705834582448006\n",
      "epoch: 3119 step: 1, loss is 0.012693080119788647\n",
      "epoch: 3120 step: 1, loss is 0.012680348940193653\n",
      "epoch: 3121 step: 1, loss is 0.01266762800514698\n",
      "epoch: 3122 step: 1, loss is 0.012654922902584076\n",
      "epoch: 3123 step: 1, loss is 0.012642238289117813\n",
      "epoch: 3124 step: 1, loss is 0.012629573233425617\n",
      "epoch: 3125 step: 1, loss is 0.012616919353604317\n",
      "epoch: 3126 step: 1, loss is 0.01260428037494421\n",
      "epoch: 3127 step: 1, loss is 0.012591667473316193\n",
      "epoch: 3128 step: 1, loss is 0.012579062022268772\n",
      "epoch: 3129 step: 1, loss is 0.012566478922963142\n",
      "epoch: 3130 step: 1, loss is 0.012553911656141281\n",
      "epoch: 3131 step: 1, loss is 0.012541357427835464\n",
      "epoch: 3132 step: 1, loss is 0.012528820894658566\n",
      "epoch: 3133 step: 1, loss is 0.012516304850578308\n",
      "epoch: 3134 step: 1, loss is 0.012503799051046371\n",
      "epoch: 3135 step: 1, loss is 0.012491314671933651\n",
      "epoch: 3136 step: 1, loss is 0.012478846125304699\n",
      "epoch: 3137 step: 1, loss is 0.012466395273804665\n",
      "epoch: 3138 step: 1, loss is 0.012453950941562653\n",
      "epoch: 3139 step: 1, loss is 0.012441535480320454\n",
      "epoch: 3140 step: 1, loss is 0.01242913119494915\n",
      "epoch: 3141 step: 1, loss is 0.012416744604706764\n",
      "epoch: 3142 step: 1, loss is 0.012404371052980423\n",
      "epoch: 3143 step: 1, loss is 0.012392016127705574\n",
      "epoch: 3144 step: 1, loss is 0.012379675172269344\n",
      "epoch: 3145 step: 1, loss is 0.012367349117994308\n",
      "epoch: 3146 step: 1, loss is 0.012355051003396511\n",
      "epoch: 3147 step: 1, loss is 0.012342754751443863\n",
      "epoch: 3148 step: 1, loss is 0.012330482713878155\n",
      "epoch: 3149 step: 1, loss is 0.012318224646151066\n",
      "epoch: 3150 step: 1, loss is 0.012305978685617447\n",
      "epoch: 3151 step: 1, loss is 0.01229375135153532\n",
      "epoch: 3152 step: 1, loss is 0.01228154357522726\n",
      "epoch: 3153 step: 1, loss is 0.012269346974790096\n",
      "epoch: 3154 step: 1, loss is 0.012257169932126999\n",
      "epoch: 3155 step: 1, loss is 0.012245011515915394\n",
      "epoch: 3156 step: 1, loss is 0.012232864275574684\n",
      "epoch: 3157 step: 1, loss is 0.012220727279782295\n",
      "epoch: 3158 step: 1, loss is 0.012208620086312294\n",
      "epoch: 3159 step: 1, loss is 0.012196514755487442\n",
      "epoch: 3160 step: 1, loss is 0.01218443363904953\n",
      "epoch: 3161 step: 1, loss is 0.012172365561127663\n",
      "epoch: 3162 step: 1, loss is 0.012160311453044415\n",
      "epoch: 3163 step: 1, loss is 0.012148275971412659\n",
      "epoch: 3164 step: 1, loss is 0.012136256322264671\n",
      "epoch: 3165 step: 1, loss is 0.012124250642955303\n",
      "epoch: 3166 step: 1, loss is 0.012112262658774853\n",
      "epoch: 3167 step: 1, loss is 0.012100288644433022\n",
      "epoch: 3168 step: 1, loss is 0.012088331393897533\n",
      "epoch: 3169 step: 1, loss is 0.01207638904452324\n",
      "epoch: 3170 step: 1, loss is 0.012064460664987564\n",
      "epoch: 3171 step: 1, loss is 0.012052549980580807\n",
      "epoch: 3172 step: 1, loss is 0.012040650472044945\n",
      "epoch: 3173 step: 1, loss is 0.012028771452605724\n",
      "epoch: 3174 step: 1, loss is 0.012016910128295422\n",
      "epoch: 3175 step: 1, loss is 0.012005056254565716\n",
      "epoch: 3176 step: 1, loss is 0.01199322659522295\n",
      "epoch: 3177 step: 1, loss is 0.011981400661170483\n",
      "epoch: 3178 step: 1, loss is 0.011969603598117828\n",
      "epoch: 3179 step: 1, loss is 0.011957813054323196\n",
      "epoch: 3180 step: 1, loss is 0.011946037411689758\n",
      "epoch: 3181 step: 1, loss is 0.011934284120798111\n",
      "epoch: 3182 step: 1, loss is 0.01192254014313221\n",
      "epoch: 3183 step: 1, loss is 0.011910812929272652\n",
      "epoch: 3184 step: 1, loss is 0.011899097822606564\n",
      "epoch: 3185 step: 1, loss is 0.011887402273714542\n",
      "epoch: 3186 step: 1, loss is 0.01187572255730629\n",
      "epoch: 3187 step: 1, loss is 0.011864053085446358\n",
      "epoch: 3188 step: 1, loss is 0.011852405034005642\n",
      "epoch: 3189 step: 1, loss is 0.011840765364468098\n",
      "epoch: 3190 step: 1, loss is 0.011829141527414322\n",
      "epoch: 3191 step: 1, loss is 0.011817538179457188\n",
      "epoch: 3192 step: 1, loss is 0.011805943213403225\n",
      "epoch: 3193 step: 1, loss is 0.011794370599091053\n",
      "epoch: 3194 step: 1, loss is 0.011782807298004627\n",
      "epoch: 3195 step: 1, loss is 0.01177126169204712\n",
      "epoch: 3196 step: 1, loss is 0.01175973005592823\n",
      "epoch: 3197 step: 1, loss is 0.011748213320970535\n",
      "epoch: 3198 step: 1, loss is 0.011736707761883736\n",
      "epoch: 3199 step: 1, loss is 0.011725218966603279\n",
      "epoch: 3200 step: 1, loss is 0.01171374786645174\n",
      "epoch: 3201 step: 1, loss is 0.011702287942171097\n",
      "epoch: 3202 step: 1, loss is 0.011690844781696796\n",
      "epoch: 3203 step: 1, loss is 0.011679418385028839\n",
      "epoch: 3204 step: 1, loss is 0.011668003164231777\n",
      "epoch: 3205 step: 1, loss is 0.011656605638563633\n",
      "epoch: 3206 step: 1, loss is 0.011645225808024406\n",
      "epoch: 3207 step: 1, loss is 0.011633849702775478\n",
      "epoch: 3208 step: 1, loss is 0.011622494086623192\n",
      "epoch: 3209 step: 1, loss is 0.0116111533716321\n",
      "epoch: 3210 step: 1, loss is 0.01159982942044735\n",
      "epoch: 3211 step: 1, loss is 0.011588516645133495\n",
      "epoch: 3212 step: 1, loss is 0.01157721970230341\n",
      "epoch: 3213 step: 1, loss is 0.011565935797989368\n",
      "epoch: 3214 step: 1, loss is 0.011554664000868797\n",
      "epoch: 3215 step: 1, loss is 0.011543413624167442\n",
      "epoch: 3216 step: 1, loss is 0.011532174423336983\n",
      "epoch: 3217 step: 1, loss is 0.011520948261022568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3218 step: 1, loss is 0.011509733274579048\n",
      "epoch: 3219 step: 1, loss is 0.011498544365167618\n",
      "epoch: 3220 step: 1, loss is 0.011487357318401337\n",
      "epoch: 3221 step: 1, loss is 0.011476189829409122\n",
      "epoch: 3222 step: 1, loss is 0.011465037241578102\n",
      "epoch: 3223 step: 1, loss is 0.011453895829617977\n",
      "epoch: 3224 step: 1, loss is 0.011442769318819046\n",
      "epoch: 3225 step: 1, loss is 0.011431658640503883\n",
      "epoch: 3226 step: 1, loss is 0.01142056006938219\n",
      "epoch: 3227 step: 1, loss is 0.011409482918679714\n",
      "epoch: 3228 step: 1, loss is 0.011398415081202984\n",
      "epoch: 3229 step: 1, loss is 0.011387360282242298\n",
      "epoch: 3230 step: 1, loss is 0.011376315727829933\n",
      "epoch: 3231 step: 1, loss is 0.011365290731191635\n",
      "epoch: 3232 step: 1, loss is 0.011354279704391956\n",
      "epoch: 3233 step: 1, loss is 0.011343277990818024\n",
      "epoch: 3234 step: 1, loss is 0.011332295835018158\n",
      "epoch: 3235 step: 1, loss is 0.011321326717734337\n",
      "epoch: 3236 step: 1, loss is 0.01131037063896656\n",
      "epoch: 3237 step: 1, loss is 0.01129942573606968\n",
      "epoch: 3238 step: 1, loss is 0.011288500390946865\n",
      "epoch: 3239 step: 1, loss is 0.011277583427727222\n",
      "epoch: 3240 step: 1, loss is 0.011266682296991348\n",
      "epoch: 3241 step: 1, loss is 0.011255794204771519\n",
      "epoch: 3242 step: 1, loss is 0.011244920082390308\n",
      "epoch: 3243 step: 1, loss is 0.011234062723815441\n",
      "epoch: 3244 step: 1, loss is 0.011223218403756618\n",
      "epoch: 3245 step: 1, loss is 0.011212383396923542\n",
      "epoch: 3246 step: 1, loss is 0.011201569810509682\n",
      "epoch: 3247 step: 1, loss is 0.011190763674676418\n",
      "epoch: 3248 step: 1, loss is 0.011179975233972073\n",
      "epoch: 3249 step: 1, loss is 0.011169196106493473\n",
      "epoch: 3250 step: 1, loss is 0.011158434674143791\n",
      "epoch: 3251 step: 1, loss is 0.011147680692374706\n",
      "epoch: 3252 step: 1, loss is 0.011136946268379688\n",
      "epoch: 3253 step: 1, loss is 0.011126222088932991\n",
      "epoch: 3254 step: 1, loss is 0.011115515604615211\n",
      "epoch: 3255 step: 1, loss is 0.011104816570878029\n",
      "epoch: 3256 step: 1, loss is 0.011094133369624615\n",
      "epoch: 3257 step: 1, loss is 0.011083466000854969\n",
      "epoch: 3258 step: 1, loss is 0.011072810739278793\n",
      "epoch: 3259 step: 1, loss is 0.01106217410415411\n",
      "epoch: 3260 step: 1, loss is 0.011051541194319725\n",
      "epoch: 3261 step: 1, loss is 0.01104092225432396\n",
      "epoch: 3262 step: 1, loss is 0.011030321940779686\n",
      "epoch: 3263 step: 1, loss is 0.011019732803106308\n",
      "epoch: 3264 step: 1, loss is 0.011009158566594124\n",
      "epoch: 3265 step: 1, loss is 0.010998600162565708\n",
      "epoch: 3266 step: 1, loss is 0.010988050140440464\n",
      "epoch: 3267 step: 1, loss is 0.01097751222550869\n",
      "epoch: 3268 step: 1, loss is 0.010966988280415535\n",
      "epoch: 3269 step: 1, loss is 0.010956482961773872\n",
      "epoch: 3270 step: 1, loss is 0.010945986956357956\n",
      "epoch: 3271 step: 1, loss is 0.010935507714748383\n",
      "epoch: 3272 step: 1, loss is 0.010925034061074257\n",
      "epoch: 3273 step: 1, loss is 0.010914578102529049\n",
      "epoch: 3274 step: 1, loss is 0.010904135182499886\n",
      "epoch: 3275 step: 1, loss is 0.010893707163631916\n",
      "epoch: 3276 step: 1, loss is 0.010883290320634842\n",
      "epoch: 3277 step: 1, loss is 0.010872885584831238\n",
      "epoch: 3278 step: 1, loss is 0.010862497612833977\n",
      "epoch: 3279 step: 1, loss is 0.010852115228772163\n",
      "epoch: 3280 step: 1, loss is 0.01084175519645214\n",
      "epoch: 3281 step: 1, loss is 0.010831402614712715\n",
      "epoch: 3282 step: 1, loss is 0.010821063071489334\n",
      "epoch: 3283 step: 1, loss is 0.010810734704136848\n",
      "epoch: 3284 step: 1, loss is 0.01080042403191328\n",
      "epoch: 3285 step: 1, loss is 0.010790125466883183\n",
      "epoch: 3286 step: 1, loss is 0.010779837146401405\n",
      "epoch: 3287 step: 1, loss is 0.010769560001790524\n",
      "epoch: 3288 step: 1, loss is 0.010759303346276283\n",
      "epoch: 3289 step: 1, loss is 0.01074905600398779\n",
      "epoch: 3290 step: 1, loss is 0.010738816112279892\n",
      "epoch: 3291 step: 1, loss is 0.010728592984378338\n",
      "epoch: 3292 step: 1, loss is 0.010718385688960552\n",
      "epoch: 3293 step: 1, loss is 0.010708190500736237\n",
      "epoch: 3294 step: 1, loss is 0.010698003694415092\n",
      "epoch: 3295 step: 1, loss is 0.010687834583222866\n",
      "epoch: 3296 step: 1, loss is 0.010677667334675789\n",
      "epoch: 3297 step: 1, loss is 0.010667525231838226\n",
      "epoch: 3298 step: 1, loss is 0.010657389648258686\n",
      "epoch: 3299 step: 1, loss is 0.010647268034517765\n",
      "epoch: 3300 step: 1, loss is 0.010637161321938038\n",
      "epoch: 3301 step: 1, loss is 0.010627059265971184\n",
      "epoch: 3302 step: 1, loss is 0.010616977699100971\n",
      "epoch: 3303 step: 1, loss is 0.010606910102069378\n",
      "epoch: 3304 step: 1, loss is 0.010596846230328083\n",
      "epoch: 3305 step: 1, loss is 0.010586800053715706\n",
      "epoch: 3306 step: 1, loss is 0.010576773434877396\n",
      "epoch: 3307 step: 1, loss is 0.010566743090748787\n",
      "epoch: 3308 step: 1, loss is 0.010556740686297417\n",
      "epoch: 3309 step: 1, loss is 0.01054674293845892\n",
      "epoch: 3310 step: 1, loss is 0.010536758229136467\n",
      "epoch: 3311 step: 1, loss is 0.010526785627007484\n",
      "epoch: 3312 step: 1, loss is 0.010516824200749397\n",
      "epoch: 3313 step: 1, loss is 0.010506880469620228\n",
      "epoch: 3314 step: 1, loss is 0.010496941395103931\n",
      "epoch: 3315 step: 1, loss is 0.010487022809684277\n",
      "epoch: 3316 step: 1, loss is 0.010477111674845219\n",
      "epoch: 3317 step: 1, loss is 0.010467218235135078\n",
      "epoch: 3318 step: 1, loss is 0.010457328520715237\n",
      "epoch: 3319 step: 1, loss is 0.010447453707456589\n",
      "epoch: 3320 step: 1, loss is 0.01043759472668171\n",
      "epoch: 3321 step: 1, loss is 0.010427741333842278\n",
      "epoch: 3322 step: 1, loss is 0.010417904704809189\n",
      "epoch: 3323 step: 1, loss is 0.010408079251646996\n",
      "epoch: 3324 step: 1, loss is 0.010398268699645996\n",
      "epoch: 3325 step: 1, loss is 0.010388467460870743\n",
      "epoch: 3326 step: 1, loss is 0.010378679260611534\n",
      "epoch: 3327 step: 1, loss is 0.01036890223622322\n",
      "epoch: 3328 step: 1, loss is 0.01035914197564125\n",
      "epoch: 3329 step: 1, loss is 0.010349386371672153\n",
      "epoch: 3330 step: 1, loss is 0.010339644737541676\n",
      "epoch: 3331 step: 1, loss is 0.010329913347959518\n",
      "epoch: 3332 step: 1, loss is 0.010320200584828854\n",
      "epoch: 3333 step: 1, loss is 0.01031049620360136\n",
      "epoch: 3334 step: 1, loss is 0.010300802066922188\n",
      "epoch: 3335 step: 1, loss is 0.01029112283140421\n",
      "epoch: 3336 step: 1, loss is 0.010281451977789402\n",
      "epoch: 3337 step: 1, loss is 0.010271796025335789\n",
      "epoch: 3338 step: 1, loss is 0.01026215124875307\n",
      "epoch: 3339 step: 1, loss is 0.010252519510686398\n",
      "epoch: 3340 step: 1, loss is 0.010242896154522896\n",
      "epoch: 3341 step: 1, loss is 0.010233288630843163\n",
      "epoch: 3342 step: 1, loss is 0.010223687626421452\n",
      "epoch: 3343 step: 1, loss is 0.010214103385806084\n",
      "epoch: 3344 step: 1, loss is 0.010204529389739037\n",
      "epoch: 3345 step: 1, loss is 0.01019496750086546\n",
      "epoch: 3346 step: 1, loss is 0.010185416787862778\n",
      "epoch: 3347 step: 1, loss is 0.010175872594118118\n",
      "epoch: 3348 step: 1, loss is 0.010166345164179802\n",
      "epoch: 3349 step: 1, loss is 0.010156833566725254\n",
      "epoch: 3350 step: 1, loss is 0.01014732662588358\n",
      "epoch: 3351 step: 1, loss is 0.010137834586203098\n",
      "epoch: 3352 step: 1, loss is 0.010128355585038662\n",
      "epoch: 3353 step: 1, loss is 0.010118884965777397\n",
      "epoch: 3354 step: 1, loss is 0.010109426453709602\n",
      "epoch: 3355 step: 1, loss is 0.010099980048835278\n",
      "epoch: 3356 step: 1, loss is 0.010090547613799572\n",
      "epoch: 3357 step: 1, loss is 0.010081122629344463\n",
      "epoch: 3358 step: 1, loss is 0.01007170882076025\n",
      "epoch: 3359 step: 1, loss is 0.01006231363862753\n",
      "epoch: 3360 step: 1, loss is 0.010052923113107681\n",
      "epoch: 3361 step: 1, loss is 0.010043543763458729\n",
      "epoch: 3362 step: 1, loss is 0.010034177452325821\n",
      "epoch: 3363 step: 1, loss is 0.010024823248386383\n",
      "epoch: 3364 step: 1, loss is 0.010015481151640415\n",
      "epoch: 3365 step: 1, loss is 0.010006150230765343\n",
      "epoch: 3366 step: 1, loss is 0.009996829554438591\n",
      "epoch: 3367 step: 1, loss is 0.009987518191337585\n",
      "epoch: 3368 step: 1, loss is 0.009978223592042923\n",
      "epoch: 3369 step: 1, loss is 0.009968936443328857\n",
      "epoch: 3370 step: 1, loss is 0.009959658607840538\n",
      "epoch: 3371 step: 1, loss is 0.009950392879545689\n",
      "epoch: 3372 step: 1, loss is 0.009941142052412033\n",
      "epoch: 3373 step: 1, loss is 0.009931901469826698\n",
      "epoch: 3374 step: 1, loss is 0.00992266833782196\n",
      "epoch: 3375 step: 1, loss is 0.009913449175655842\n",
      "epoch: 3376 step: 1, loss is 0.009904246777296066\n",
      "epoch: 3377 step: 1, loss is 0.009895049966871738\n",
      "epoch: 3378 step: 1, loss is 0.009885863400995731\n",
      "epoch: 3379 step: 1, loss is 0.00987668801099062\n",
      "epoch: 3380 step: 1, loss is 0.009867529384791851\n",
      "epoch: 3381 step: 1, loss is 0.009858373552560806\n",
      "epoch: 3382 step: 1, loss is 0.009849228896200657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3383 step: 1, loss is 0.009840101934969425\n",
      "epoch: 3384 step: 1, loss is 0.009830985218286514\n",
      "epoch: 3385 step: 1, loss is 0.009821875020861626\n",
      "epoch: 3386 step: 1, loss is 0.009812776930630207\n",
      "epoch: 3387 step: 1, loss is 0.009803693741559982\n",
      "epoch: 3388 step: 1, loss is 0.009794619865715504\n",
      "epoch: 3389 step: 1, loss is 0.009785554371774197\n",
      "epoch: 3390 step: 1, loss is 0.009776498191058636\n",
      "epoch: 3391 step: 1, loss is 0.009767458774149418\n",
      "epoch: 3392 step: 1, loss is 0.009758426807820797\n",
      "epoch: 3393 step: 1, loss is 0.009749406948685646\n",
      "epoch: 3394 step: 1, loss is 0.00974040012806654\n",
      "epoch: 3395 step: 1, loss is 0.009731398895382881\n",
      "epoch: 3396 step: 1, loss is 0.009722409769892693\n",
      "epoch: 3397 step: 1, loss is 0.009713434614241123\n",
      "epoch: 3398 step: 1, loss is 0.0097044687718153\n",
      "epoch: 3399 step: 1, loss is 0.009695509448647499\n",
      "epoch: 3400 step: 1, loss is 0.00968656875193119\n",
      "epoch: 3401 step: 1, loss is 0.00967763364315033\n",
      "epoch: 3402 step: 1, loss is 0.00966870877891779\n",
      "epoch: 3403 step: 1, loss is 0.009659799747169018\n",
      "epoch: 3404 step: 1, loss is 0.009650894440710545\n",
      "epoch: 3405 step: 1, loss is 0.009642004035413265\n",
      "epoch: 3406 step: 1, loss is 0.009633122012019157\n",
      "epoch: 3407 step: 1, loss is 0.009624255821108818\n",
      "epoch: 3408 step: 1, loss is 0.009615393355488777\n",
      "epoch: 3409 step: 1, loss is 0.009606540203094482\n",
      "epoch: 3410 step: 1, loss is 0.00959770567715168\n",
      "epoch: 3411 step: 1, loss is 0.009588880464434624\n",
      "epoch: 3412 step: 1, loss is 0.009580063633620739\n",
      "epoch: 3413 step: 1, loss is 0.00957125797867775\n",
      "epoch: 3414 step: 1, loss is 0.009562458842992783\n",
      "epoch: 3415 step: 1, loss is 0.009553677402436733\n",
      "epoch: 3416 step: 1, loss is 0.009544900618493557\n",
      "epoch: 3417 step: 1, loss is 0.009536141529679298\n",
      "epoch: 3418 step: 1, loss is 0.009527385234832764\n",
      "epoch: 3419 step: 1, loss is 0.009518643841147423\n",
      "epoch: 3420 step: 1, loss is 0.009509911760687828\n",
      "epoch: 3421 step: 1, loss is 0.009501186199486256\n",
      "epoch: 3422 step: 1, loss is 0.009492476470768452\n",
      "epoch: 3423 step: 1, loss is 0.00948377512395382\n",
      "epoch: 3424 step: 1, loss is 0.009475084953010082\n",
      "epoch: 3425 step: 1, loss is 0.009466398507356644\n",
      "epoch: 3426 step: 1, loss is 0.00945773534476757\n",
      "epoch: 3427 step: 1, loss is 0.009449074044823647\n",
      "epoch: 3428 step: 1, loss is 0.009440422058105469\n",
      "epoch: 3429 step: 1, loss is 0.009431783109903336\n",
      "epoch: 3430 step: 1, loss is 0.009423157200217247\n",
      "epoch: 3431 step: 1, loss is 0.00941453780978918\n",
      "epoch: 3432 step: 1, loss is 0.009405928663909435\n",
      "epoch: 3433 step: 1, loss is 0.00939733162522316\n",
      "epoch: 3434 step: 1, loss is 0.00938874389976263\n",
      "epoch: 3435 step: 1, loss is 0.009380166418850422\n",
      "epoch: 3436 step: 1, loss is 0.009371599182486534\n",
      "epoch: 3437 step: 1, loss is 0.009363040328025818\n",
      "epoch: 3438 step: 1, loss is 0.009354494512081146\n",
      "epoch: 3439 step: 1, loss is 0.00934595987200737\n",
      "epoch: 3440 step: 1, loss is 0.009337430819869041\n",
      "epoch: 3441 step: 1, loss is 0.009328917600214481\n",
      "epoch: 3442 step: 1, loss is 0.00932040810585022\n",
      "epoch: 3443 step: 1, loss is 0.009311913512647152\n",
      "epoch: 3444 step: 1, loss is 0.009303427301347256\n",
      "epoch: 3445 step: 1, loss is 0.00929495133459568\n",
      "epoch: 3446 step: 1, loss is 0.009286482818424702\n",
      "epoch: 3447 step: 1, loss is 0.009278026409447193\n",
      "epoch: 3448 step: 1, loss is 0.00926957931369543\n",
      "epoch: 3449 step: 1, loss is 0.009261143393814564\n",
      "epoch: 3450 step: 1, loss is 0.009252717718482018\n",
      "epoch: 3451 step: 1, loss is 0.009244297631084919\n",
      "epoch: 3452 step: 1, loss is 0.00923589151352644\n",
      "epoch: 3453 step: 1, loss is 0.00922749750316143\n",
      "epoch: 3454 step: 1, loss is 0.009219111874699593\n",
      "epoch: 3455 step: 1, loss is 0.009210732765495777\n",
      "epoch: 3456 step: 1, loss is 0.009202368557453156\n",
      "epoch: 3457 step: 1, loss is 0.009194009937345982\n",
      "epoch: 3458 step: 1, loss is 0.009185663424432278\n",
      "epoch: 3459 step: 1, loss is 0.00917732622474432\n",
      "epoch: 3460 step: 1, loss is 0.009168998338282108\n",
      "epoch: 3461 step: 1, loss is 0.009160677902400494\n",
      "epoch: 3462 step: 1, loss is 0.009152376092970371\n",
      "epoch: 3463 step: 1, loss is 0.009144079871475697\n",
      "epoch: 3464 step: 1, loss is 0.009135791100561619\n",
      "epoch: 3465 step: 1, loss is 0.009127514436841011\n",
      "epoch: 3466 step: 1, loss is 0.009119244292378426\n",
      "epoch: 3467 step: 1, loss is 0.009110984392464161\n",
      "epoch: 3468 step: 1, loss is 0.009102730080485344\n",
      "epoch: 3469 step: 1, loss is 0.009094496257603168\n",
      "epoch: 3470 step: 1, loss is 0.009086266160011292\n",
      "epoch: 3471 step: 1, loss is 0.009078042581677437\n",
      "epoch: 3472 step: 1, loss is 0.009069839492440224\n",
      "epoch: 3473 step: 1, loss is 0.009061633609235287\n",
      "epoch: 3474 step: 1, loss is 0.009053441695868969\n",
      "epoch: 3475 step: 1, loss is 0.009045260027050972\n",
      "epoch: 3476 step: 1, loss is 0.009037085808813572\n",
      "epoch: 3477 step: 1, loss is 0.009028926491737366\n",
      "epoch: 3478 step: 1, loss is 0.009020775556564331\n",
      "epoch: 3479 step: 1, loss is 0.009012631140649319\n",
      "epoch: 3480 step: 1, loss is 0.009004497900605202\n",
      "epoch: 3481 step: 1, loss is 0.008996372111141682\n",
      "epoch: 3482 step: 1, loss is 0.00898826029151678\n",
      "epoch: 3483 step: 1, loss is 0.008980155922472477\n",
      "epoch: 3484 step: 1, loss is 0.008972059935331345\n",
      "epoch: 3485 step: 1, loss is 0.008963976986706257\n",
      "epoch: 3486 step: 1, loss is 0.008955899626016617\n",
      "epoch: 3487 step: 1, loss is 0.008947829715907574\n",
      "epoch: 3488 step: 1, loss is 0.008939770050346851\n",
      "epoch: 3489 step: 1, loss is 0.008931724354624748\n",
      "epoch: 3490 step: 1, loss is 0.008923684246838093\n",
      "epoch: 3491 step: 1, loss is 0.008915656246244907\n",
      "epoch: 3492 step: 1, loss is 0.008907636627554893\n",
      "epoch: 3493 step: 1, loss is 0.008899625390768051\n",
      "epoch: 3494 step: 1, loss is 0.00889162439852953\n",
      "epoch: 3495 step: 1, loss is 0.008883635513484478\n",
      "epoch: 3496 step: 1, loss is 0.0088756512850523\n",
      "epoch: 3497 step: 1, loss is 0.008867676369845867\n",
      "epoch: 3498 step: 1, loss is 0.008859711699187756\n",
      "epoch: 3499 step: 1, loss is 0.008851757273077965\n",
      "epoch: 3500 step: 1, loss is 0.008843806572258472\n",
      "epoch: 3501 step: 1, loss is 0.008835875429213047\n",
      "epoch: 3502 step: 1, loss is 0.008827948942780495\n",
      "epoch: 3503 step: 1, loss is 0.008820028975605965\n",
      "epoch: 3504 step: 1, loss is 0.00881212204694748\n",
      "epoch: 3505 step: 1, loss is 0.008804220706224442\n",
      "epoch: 3506 step: 1, loss is 0.008796331472694874\n",
      "epoch: 3507 step: 1, loss is 0.008788451552391052\n",
      "epoch: 3508 step: 1, loss is 0.008780578151345253\n",
      "epoch: 3509 step: 1, loss is 0.008772714994847775\n",
      "epoch: 3510 step: 1, loss is 0.008764862082898617\n",
      "epoch: 3511 step: 1, loss is 0.00875701941549778\n",
      "epoch: 3512 step: 1, loss is 0.00874918233603239\n",
      "epoch: 3513 step: 1, loss is 0.008741353638470173\n",
      "epoch: 3514 step: 1, loss is 0.008733539842069149\n",
      "epoch: 3515 step: 1, loss is 0.008725730702280998\n",
      "epoch: 3516 step: 1, loss is 0.008717931807041168\n",
      "epoch: 3517 step: 1, loss is 0.008710140362381935\n",
      "epoch: 3518 step: 1, loss is 0.008702359162271023\n",
      "epoch: 3519 step: 1, loss is 0.008694588206708431\n",
      "epoch: 3520 step: 1, loss is 0.008686822839081287\n",
      "epoch: 3521 step: 1, loss is 0.008679069578647614\n",
      "epoch: 3522 step: 1, loss is 0.008671323768794537\n",
      "epoch: 3523 step: 1, loss is 0.008663589134812355\n",
      "epoch: 3524 step: 1, loss is 0.008655857294797897\n",
      "epoch: 3525 step: 1, loss is 0.008648140355944633\n",
      "epoch: 3526 step: 1, loss is 0.008640432730317116\n",
      "epoch: 3527 step: 1, loss is 0.008632734417915344\n",
      "epoch: 3528 step: 1, loss is 0.008625040762126446\n",
      "epoch: 3529 step: 1, loss is 0.008617360144853592\n",
      "epoch: 3530 step: 1, loss is 0.008609682321548462\n",
      "epoch: 3531 step: 1, loss is 0.008602017536759377\n",
      "epoch: 3532 step: 1, loss is 0.008594361133873463\n",
      "epoch: 3533 step: 1, loss is 0.00858671497553587\n",
      "epoch: 3534 step: 1, loss is 0.008579077199101448\n",
      "epoch: 3535 step: 1, loss is 0.008571447804570198\n",
      "epoch: 3536 step: 1, loss is 0.008563829585909843\n",
      "epoch: 3537 step: 1, loss is 0.008556213229894638\n",
      "epoch: 3538 step: 1, loss is 0.008548612706363201\n",
      "epoch: 3539 step: 1, loss is 0.008541017770767212\n",
      "epoch: 3540 step: 1, loss is 0.00853343028575182\n",
      "epoch: 3541 step: 1, loss is 0.008525853976607323\n",
      "epoch: 3542 step: 1, loss is 0.008518286980688572\n",
      "epoch: 3543 step: 1, loss is 0.008510725572705269\n",
      "epoch: 3544 step: 1, loss is 0.008503178134560585\n",
      "epoch: 3545 step: 1, loss is 0.008495635353028774\n",
      "epoch: 3546 step: 1, loss is 0.008488096296787262\n",
      "epoch: 3547 step: 1, loss is 0.008480575866997242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3548 step: 1, loss is 0.008473056368529797\n",
      "epoch: 3549 step: 1, loss is 0.008465549908578396\n",
      "epoch: 3550 step: 1, loss is 0.008458049967885017\n",
      "epoch: 3551 step: 1, loss is 0.008450557477772236\n",
      "epoch: 3552 step: 1, loss is 0.008443080820143223\n",
      "epoch: 3553 step: 1, loss is 0.008435606956481934\n",
      "epoch: 3554 step: 1, loss is 0.008428140543401241\n",
      "epoch: 3555 step: 1, loss is 0.008420681580901146\n",
      "epoch: 3556 step: 1, loss is 0.008413231000304222\n",
      "epoch: 3557 step: 1, loss is 0.008405796252191067\n",
      "epoch: 3558 step: 1, loss is 0.008398366160690784\n",
      "epoch: 3559 step: 1, loss is 0.008390940725803375\n",
      "epoch: 3560 step: 1, loss is 0.008383531123399734\n",
      "epoch: 3561 step: 1, loss is 0.008376124314963818\n",
      "epoch: 3562 step: 1, loss is 0.008368725888431072\n",
      "epoch: 3563 step: 1, loss is 0.008361338637769222\n",
      "epoch: 3564 step: 1, loss is 0.008353957906365395\n",
      "epoch: 3565 step: 1, loss is 0.008346587419509888\n",
      "epoch: 3566 step: 1, loss is 0.008339226245880127\n",
      "epoch: 3567 step: 1, loss is 0.00833186972886324\n",
      "epoch: 3568 step: 1, loss is 0.008324524387717247\n",
      "epoch: 3569 step: 1, loss is 0.008317185565829277\n",
      "epoch: 3570 step: 1, loss is 0.008309855125844479\n",
      "epoch: 3571 step: 1, loss is 0.008302533999085426\n",
      "epoch: 3572 step: 1, loss is 0.008295221254229546\n",
      "epoch: 3573 step: 1, loss is 0.008287917822599411\n",
      "epoch: 3574 step: 1, loss is 0.008280623704195023\n",
      "epoch: 3575 step: 1, loss is 0.008273331448435783\n",
      "epoch: 3576 step: 1, loss is 0.008266055956482887\n",
      "epoch: 3577 step: 1, loss is 0.00825878232717514\n",
      "epoch: 3578 step: 1, loss is 0.008251518942415714\n",
      "epoch: 3579 step: 1, loss is 0.008244266733527184\n",
      "epoch: 3580 step: 1, loss is 0.008237021043896675\n",
      "epoch: 3581 step: 1, loss is 0.008229780942201614\n",
      "epoch: 3582 step: 1, loss is 0.008222552947700024\n",
      "epoch: 3583 step: 1, loss is 0.00821533054113388\n",
      "epoch: 3584 step: 1, loss is 0.008208116516470909\n",
      "epoch: 3585 step: 1, loss is 0.008200908079743385\n",
      "epoch: 3586 step: 1, loss is 0.008193710818886757\n",
      "epoch: 3587 step: 1, loss is 0.008186525665223598\n",
      "epoch: 3588 step: 1, loss is 0.008179344236850739\n",
      "epoch: 3589 step: 1, loss is 0.008172169327735901\n",
      "epoch: 3590 step: 1, loss is 0.008165006525814533\n",
      "epoch: 3591 step: 1, loss is 0.008157848380506039\n",
      "epoch: 3592 step: 1, loss is 0.008150703273713589\n",
      "epoch: 3593 step: 1, loss is 0.008143560960888863\n",
      "epoch: 3594 step: 1, loss is 0.008136429823935032\n",
      "epoch: 3595 step: 1, loss is 0.008129307068884373\n",
      "epoch: 3596 step: 1, loss is 0.008122187107801437\n",
      "epoch: 3597 step: 1, loss is 0.008115079253911972\n",
      "epoch: 3598 step: 1, loss is 0.008107975125312805\n",
      "epoch: 3599 step: 1, loss is 0.008100886829197407\n",
      "epoch: 3600 step: 1, loss is 0.008093804121017456\n",
      "epoch: 3601 step: 1, loss is 0.008086727932095528\n",
      "epoch: 3602 step: 1, loss is 0.008079659193754196\n",
      "epoch: 3603 step: 1, loss is 0.008072596043348312\n",
      "epoch: 3604 step: 1, loss is 0.008065546862781048\n",
      "epoch: 3605 step: 1, loss is 0.008058503270149231\n",
      "epoch: 3606 step: 1, loss is 0.008051467128098011\n",
      "epoch: 3607 step: 1, loss is 0.008044438436627388\n",
      "epoch: 3608 step: 1, loss is 0.008037418127059937\n",
      "epoch: 3609 step: 1, loss is 0.008030406199395657\n",
      "epoch: 3610 step: 1, loss is 0.008023397997021675\n",
      "epoch: 3611 step: 1, loss is 0.008016403764486313\n",
      "epoch: 3612 step: 1, loss is 0.008009413257241249\n",
      "epoch: 3613 step: 1, loss is 0.008002434857189655\n",
      "epoch: 3614 step: 1, loss is 0.007995459251105785\n",
      "epoch: 3615 step: 1, loss is 0.007988495752215385\n",
      "epoch: 3616 step: 1, loss is 0.007981540635228157\n",
      "epoch: 3617 step: 1, loss is 0.007974587380886078\n",
      "epoch: 3618 step: 1, loss is 0.007967645302414894\n",
      "epoch: 3619 step: 1, loss is 0.007960710674524307\n",
      "epoch: 3620 step: 1, loss is 0.007953785359859467\n",
      "epoch: 3621 step: 1, loss is 0.007946865633130074\n",
      "epoch: 3622 step: 1, loss is 0.007939957082271576\n",
      "epoch: 3623 step: 1, loss is 0.007933054119348526\n",
      "epoch: 3624 step: 1, loss is 0.007926157675683498\n",
      "epoch: 3625 step: 1, loss is 0.007919268682599068\n",
      "epoch: 3626 step: 1, loss is 0.007912389934062958\n",
      "epoch: 3627 step: 1, loss is 0.007905516773462296\n",
      "epoch: 3628 step: 1, loss is 0.007898655720055103\n",
      "epoch: 3629 step: 1, loss is 0.00789179652929306\n",
      "epoch: 3630 step: 1, loss is 0.007884945720434189\n",
      "epoch: 3631 step: 1, loss is 0.007878107950091362\n",
      "epoch: 3632 step: 1, loss is 0.007871270179748535\n",
      "epoch: 3633 step: 1, loss is 0.007864449173212051\n",
      "epoch: 3634 step: 1, loss is 0.007857628166675568\n",
      "epoch: 3635 step: 1, loss is 0.007850813679397106\n",
      "epoch: 3636 step: 1, loss is 0.007844013161957264\n",
      "epoch: 3637 step: 1, loss is 0.00783721823245287\n",
      "epoch: 3638 step: 1, loss is 0.007830428890883923\n",
      "epoch: 3639 step: 1, loss is 0.007823646999895573\n",
      "epoch: 3640 step: 1, loss is 0.007816876284778118\n",
      "epoch: 3641 step: 1, loss is 0.007810108829289675\n",
      "epoch: 3642 step: 1, loss is 0.007803351152688265\n",
      "epoch: 3643 step: 1, loss is 0.0077966004610061646\n",
      "epoch: 3644 step: 1, loss is 0.0077898623421788216\n",
      "epoch: 3645 step: 1, loss is 0.007783123757690191\n",
      "epoch: 3646 step: 1, loss is 0.007776395417749882\n",
      "epoch: 3647 step: 1, loss is 0.007769676391035318\n",
      "epoch: 3648 step: 1, loss is 0.007762962952256203\n",
      "epoch: 3649 step: 1, loss is 0.007756258826702833\n",
      "epoch: 3650 step: 1, loss is 0.007749559357762337\n",
      "epoch: 3651 step: 1, loss is 0.007742868736386299\n",
      "epoch: 3652 step: 1, loss is 0.0077361855655908585\n",
      "epoch: 3653 step: 1, loss is 0.007729509845376015\n",
      "epoch: 3654 step: 1, loss is 0.007722841575741768\n",
      "epoch: 3655 step: 1, loss is 0.007716183550655842\n",
      "epoch: 3656 step: 1, loss is 0.007709526922553778\n",
      "epoch: 3657 step: 1, loss is 0.007702881470322609\n",
      "epoch: 3658 step: 1, loss is 0.007696242071688175\n",
      "epoch: 3659 step: 1, loss is 0.0076896115206182\n",
      "epoch: 3660 step: 1, loss is 0.007682989351451397\n",
      "epoch: 3661 step: 1, loss is 0.007676370441913605\n",
      "epoch: 3662 step: 1, loss is 0.007669759448617697\n",
      "epoch: 3663 step: 1, loss is 0.007663159631192684\n",
      "epoch: 3664 step: 1, loss is 0.007656565401703119\n",
      "epoch: 3665 step: 1, loss is 0.007649977691471577\n",
      "epoch: 3666 step: 1, loss is 0.007643397897481918\n",
      "epoch: 3667 step: 1, loss is 0.007636825554072857\n",
      "epoch: 3668 step: 1, loss is 0.0076302592642605305\n",
      "epoch: 3669 step: 1, loss is 0.007623700890690088\n",
      "epoch: 3670 step: 1, loss is 0.007617149502038956\n",
      "epoch: 3671 step: 1, loss is 0.007610605098307133\n",
      "epoch: 3672 step: 1, loss is 0.0076040709391236305\n",
      "epoch: 3673 step: 1, loss is 0.007597541436553001\n",
      "epoch: 3674 step: 1, loss is 0.00759101752191782\n",
      "epoch: 3675 step: 1, loss is 0.007584502920508385\n",
      "epoch: 3676 step: 1, loss is 0.007577994838356972\n",
      "epoch: 3677 step: 1, loss is 0.007571496535092592\n",
      "epoch: 3678 step: 1, loss is 0.007565000094473362\n",
      "epoch: 3679 step: 1, loss is 0.007558514829725027\n",
      "epoch: 3680 step: 1, loss is 0.007552038412541151\n",
      "epoch: 3681 step: 1, loss is 0.007545564789324999\n",
      "epoch: 3682 step: 1, loss is 0.007539099082350731\n",
      "epoch: 3683 step: 1, loss is 0.00753264082595706\n",
      "epoch: 3684 step: 1, loss is 0.007526190020143986\n",
      "epoch: 3685 step: 1, loss is 0.0075197480618953705\n",
      "epoch: 3686 step: 1, loss is 0.007513311225920916\n",
      "epoch: 3687 step: 1, loss is 0.00750688323751092\n",
      "epoch: 3688 step: 1, loss is 0.007500460371375084\n",
      "epoch: 3689 step: 1, loss is 0.007494043558835983\n",
      "epoch: 3690 step: 1, loss is 0.007487638387829065\n",
      "epoch: 3691 step: 1, loss is 0.007481234148144722\n",
      "epoch: 3692 step: 1, loss is 0.007474843878298998\n",
      "epoch: 3693 step: 1, loss is 0.007468455005437136\n",
      "epoch: 3694 step: 1, loss is 0.007462072651833296\n",
      "epoch: 3695 step: 1, loss is 0.0074556972831487656\n",
      "epoch: 3696 step: 1, loss is 0.007449336349964142\n",
      "epoch: 3697 step: 1, loss is 0.00744297681376338\n",
      "epoch: 3698 step: 1, loss is 0.007436626125127077\n",
      "epoch: 3699 step: 1, loss is 0.00743027962744236\n",
      "epoch: 3700 step: 1, loss is 0.007423942908644676\n",
      "epoch: 3701 step: 1, loss is 0.007417610380798578\n",
      "epoch: 3702 step: 1, loss is 0.007411286234855652\n",
      "epoch: 3703 step: 1, loss is 0.007404969539493322\n",
      "epoch: 3704 step: 1, loss is 0.007398655172437429\n",
      "epoch: 3705 step: 1, loss is 0.007392355240881443\n",
      "epoch: 3706 step: 1, loss is 0.007386059500277042\n",
      "epoch: 3707 step: 1, loss is 0.007379768416285515\n",
      "epoch: 3708 step: 1, loss is 0.00737348198890686\n",
      "epoch: 3709 step: 1, loss is 0.0073672072030603886\n",
      "epoch: 3710 step: 1, loss is 0.007360939402133226\n",
      "epoch: 3711 step: 1, loss is 0.0073546781204640865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3712 step: 1, loss is 0.00734842149540782\n",
      "epoch: 3713 step: 1, loss is 0.007342172786593437\n",
      "epoch: 3714 step: 1, loss is 0.0073359329253435135\n",
      "epoch: 3715 step: 1, loss is 0.007329696789383888\n",
      "epoch: 3716 step: 1, loss is 0.007323471829295158\n",
      "epoch: 3717 step: 1, loss is 0.007317247334867716\n",
      "epoch: 3718 step: 1, loss is 0.007311033084988594\n",
      "epoch: 3719 step: 1, loss is 0.0073048267513513565\n",
      "epoch: 3720 step: 1, loss is 0.007298625539988279\n",
      "epoch: 3721 step: 1, loss is 0.007292431779205799\n",
      "epoch: 3722 step: 1, loss is 0.007286242209374905\n",
      "epoch: 3723 step: 1, loss is 0.007280062884092331\n",
      "epoch: 3724 step: 1, loss is 0.0072738900780677795\n",
      "epoch: 3725 step: 1, loss is 0.007267719134688377\n",
      "epoch: 3726 step: 1, loss is 0.007261557504534721\n",
      "epoch: 3727 step: 1, loss is 0.007255406118929386\n",
      "epoch: 3728 step: 1, loss is 0.0072492570616304874\n",
      "epoch: 3729 step: 1, loss is 0.007243116851896048\n",
      "epoch: 3730 step: 1, loss is 0.00723698316141963\n",
      "epoch: 3731 step: 1, loss is 0.007230854593217373\n",
      "epoch: 3732 step: 1, loss is 0.0072247362695634365\n",
      "epoch: 3733 step: 1, loss is 0.007218620274215937\n",
      "epoch: 3734 step: 1, loss is 0.007212513126432896\n",
      "epoch: 3735 step: 1, loss is 0.007206411100924015\n",
      "epoch: 3736 step: 1, loss is 0.007200318388640881\n",
      "epoch: 3737 step: 1, loss is 0.007194230332970619\n",
      "epoch: 3738 step: 1, loss is 0.0071881492622196674\n",
      "epoch: 3739 step: 1, loss is 0.007182073779404163\n",
      "epoch: 3740 step: 1, loss is 0.007176007144153118\n",
      "epoch: 3741 step: 1, loss is 0.007169945165514946\n",
      "epoch: 3742 step: 1, loss is 0.007163891568779945\n",
      "epoch: 3743 step: 1, loss is 0.007157845422625542\n",
      "epoch: 3744 step: 1, loss is 0.0071518030017614365\n",
      "epoch: 3745 step: 1, loss is 0.007145762909203768\n",
      "epoch: 3746 step: 1, loss is 0.0071397386491298676\n",
      "epoch: 3747 step: 1, loss is 0.007133718580007553\n",
      "epoch: 3748 step: 1, loss is 0.007127696648240089\n",
      "epoch: 3749 step: 1, loss is 0.007121692411601543\n",
      "epoch: 3750 step: 1, loss is 0.007115688640624285\n",
      "epoch: 3751 step: 1, loss is 0.0071096899919211864\n",
      "epoch: 3752 step: 1, loss is 0.007103703450411558\n",
      "epoch: 3753 step: 1, loss is 0.007097721099853516\n",
      "epoch: 3754 step: 1, loss is 0.007091742008924484\n",
      "epoch: 3755 step: 1, loss is 0.007085773162543774\n",
      "epoch: 3756 step: 1, loss is 0.007079811301082373\n",
      "epoch: 3757 step: 1, loss is 0.007073852699249983\n",
      "epoch: 3758 step: 1, loss is 0.007067900616675615\n",
      "epoch: 3759 step: 1, loss is 0.007061954587697983\n",
      "epoch: 3760 step: 1, loss is 0.007056017406284809\n",
      "epoch: 3761 step: 1, loss is 0.007050088606774807\n",
      "epoch: 3762 step: 1, loss is 0.007044158410280943\n",
      "epoch: 3763 step: 1, loss is 0.007038243114948273\n",
      "epoch: 3764 step: 1, loss is 0.007032329682260752\n",
      "epoch: 3765 step: 1, loss is 0.0070264218375086784\n",
      "epoch: 3766 step: 1, loss is 0.007020522374659777\n",
      "epoch: 3767 step: 1, loss is 0.0070146312937140465\n",
      "epoch: 3768 step: 1, loss is 0.007008743938058615\n",
      "epoch: 3769 step: 1, loss is 0.007002860773354769\n",
      "epoch: 3770 step: 1, loss is 0.006996986456215382\n",
      "epoch: 3771 step: 1, loss is 0.006991119123995304\n",
      "epoch: 3772 step: 1, loss is 0.0069852545857429504\n",
      "epoch: 3773 step: 1, loss is 0.00697939982637763\n",
      "epoch: 3774 step: 1, loss is 0.00697355205193162\n",
      "epoch: 3775 step: 1, loss is 0.006967706140130758\n",
      "epoch: 3776 step: 1, loss is 0.006961870472878218\n",
      "epoch: 3777 step: 1, loss is 0.006956042256206274\n",
      "epoch: 3778 step: 1, loss is 0.0069502172991633415\n",
      "epoch: 3779 step: 1, loss is 0.006944400258362293\n",
      "epoch: 3780 step: 1, loss is 0.006938589736819267\n",
      "epoch: 3781 step: 1, loss is 0.006932780146598816\n",
      "epoch: 3782 step: 1, loss is 0.006926981266587973\n",
      "epoch: 3783 step: 1, loss is 0.006921184249222279\n",
      "epoch: 3784 step: 1, loss is 0.0069154007360339165\n",
      "epoch: 3785 step: 1, loss is 0.006909618154168129\n",
      "epoch: 3786 step: 1, loss is 0.006903845351189375\n",
      "epoch: 3787 step: 1, loss is 0.006898072082549334\n",
      "epoch: 3788 step: 1, loss is 0.006892312318086624\n",
      "epoch: 3789 step: 1, loss is 0.006886555813252926\n",
      "epoch: 3790 step: 1, loss is 0.0068808067589998245\n",
      "epoch: 3791 step: 1, loss is 0.006875060964375734\n",
      "epoch: 3792 step: 1, loss is 0.0068693263456225395\n",
      "epoch: 3793 step: 1, loss is 0.006863593589514494\n",
      "epoch: 3794 step: 1, loss is 0.0068578654900193214\n",
      "epoch: 3795 step: 1, loss is 0.006852146703749895\n",
      "epoch: 3796 step: 1, loss is 0.006846433971077204\n",
      "epoch: 3797 step: 1, loss is 0.006840728223323822\n",
      "epoch: 3798 step: 1, loss is 0.006835022941231728\n",
      "epoch: 3799 step: 1, loss is 0.006829327903687954\n",
      "epoch: 3800 step: 1, loss is 0.006823640316724777\n",
      "epoch: 3801 step: 1, loss is 0.006817960646003485\n",
      "epoch: 3802 step: 1, loss is 0.006812283769249916\n",
      "epoch: 3803 step: 1, loss is 0.006806607358157635\n",
      "epoch: 3804 step: 1, loss is 0.006800944451242685\n",
      "epoch: 3805 step: 1, loss is 0.006795285735279322\n",
      "epoch: 3806 step: 1, loss is 0.006789634469896555\n",
      "epoch: 3807 step: 1, loss is 0.006783986929804087\n",
      "epoch: 3808 step: 1, loss is 0.00677834264934063\n",
      "epoch: 3809 step: 1, loss is 0.006772711873054504\n",
      "epoch: 3810 step: 1, loss is 0.006767082493752241\n",
      "epoch: 3811 step: 1, loss is 0.006761459633708\n",
      "epoch: 3812 step: 1, loss is 0.00675584189593792\n",
      "epoch: 3813 step: 1, loss is 0.0067502278834581375\n",
      "epoch: 3814 step: 1, loss is 0.0067446231842041016\n",
      "epoch: 3815 step: 1, loss is 0.0067390259355306625\n",
      "epoch: 3816 step: 1, loss is 0.0067334300838410854\n",
      "epoch: 3817 step: 1, loss is 0.006727844476699829\n",
      "epoch: 3818 step: 1, loss is 0.006722262594848871\n",
      "epoch: 3819 step: 1, loss is 0.006716682575643063\n",
      "epoch: 3820 step: 1, loss is 0.006711117457598448\n",
      "epoch: 3821 step: 1, loss is 0.0067055546678602695\n",
      "epoch: 3822 step: 1, loss is 0.006699996069073677\n",
      "epoch: 3823 step: 1, loss is 0.006694444455206394\n",
      "epoch: 3824 step: 1, loss is 0.006688898429274559\n",
      "epoch: 3825 step: 1, loss is 0.006683357525616884\n",
      "epoch: 3826 step: 1, loss is 0.006677823141217232\n",
      "epoch: 3827 step: 1, loss is 0.006672294810414314\n",
      "epoch: 3828 step: 1, loss is 0.00666677113622427\n",
      "epoch: 3829 step: 1, loss is 0.006661253049969673\n",
      "epoch: 3830 step: 1, loss is 0.006655743811279535\n",
      "epoch: 3831 step: 1, loss is 0.0066502392292022705\n",
      "epoch: 3832 step: 1, loss is 0.006644740235060453\n",
      "epoch: 3833 step: 1, loss is 0.006639246828854084\n",
      "epoch: 3834 step: 1, loss is 0.006633759010583162\n",
      "epoch: 3835 step: 1, loss is 0.006628273520618677\n",
      "epoch: 3836 step: 1, loss is 0.0066227987408638\n",
      "epoch: 3837 step: 1, loss is 0.006617327220737934\n",
      "epoch: 3838 step: 1, loss is 0.00661186408251524\n",
      "epoch: 3839 step: 1, loss is 0.006606405600905418\n",
      "epoch: 3840 step: 1, loss is 0.006600952707231045\n",
      "epoch: 3841 step: 1, loss is 0.006595505401492119\n",
      "epoch: 3842 step: 1, loss is 0.006590063218027353\n",
      "epoch: 3843 step: 1, loss is 0.006584621965885162\n",
      "epoch: 3844 step: 1, loss is 0.00657919654622674\n",
      "epoch: 3845 step: 1, loss is 0.00657377066090703\n",
      "epoch: 3846 step: 1, loss is 0.00656835176050663\n",
      "epoch: 3847 step: 1, loss is 0.0065629370510578156\n",
      "epoch: 3848 step: 1, loss is 0.006557531654834747\n",
      "epoch: 3849 step: 1, loss is 0.006552128121256828\n",
      "epoch: 3850 step: 1, loss is 0.006546732038259506\n",
      "epoch: 3851 step: 1, loss is 0.006541341543197632\n",
      "epoch: 3852 step: 1, loss is 0.006535955239087343\n",
      "epoch: 3853 step: 1, loss is 0.006530578248202801\n",
      "epoch: 3854 step: 1, loss is 0.006525203585624695\n",
      "epoch: 3855 step: 1, loss is 0.006519835442304611\n",
      "epoch: 3856 step: 1, loss is 0.0065144747495651245\n",
      "epoch: 3857 step: 1, loss is 0.006509115919470787\n",
      "epoch: 3858 step: 1, loss is 0.006503766402602196\n",
      "epoch: 3859 step: 1, loss is 0.006498420611023903\n",
      "epoch: 3860 step: 1, loss is 0.006493080873042345\n",
      "epoch: 3861 step: 1, loss is 0.006487745326012373\n",
      "epoch: 3862 step: 1, loss is 0.006482416298240423\n",
      "epoch: 3863 step: 1, loss is 0.006477093789726496\n",
      "epoch: 3864 step: 1, loss is 0.0064717745408415794\n",
      "epoch: 3865 step: 1, loss is 0.006466466002166271\n",
      "epoch: 3866 step: 1, loss is 0.006461158860474825\n",
      "epoch: 3867 step: 1, loss is 0.006455857306718826\n",
      "epoch: 3868 step: 1, loss is 0.0064505599439144135\n",
      "epoch: 3869 step: 1, loss is 0.00644526956602931\n",
      "epoch: 3870 step: 1, loss is 0.006439987104386091\n",
      "epoch: 3871 step: 1, loss is 0.0064347051084041595\n",
      "epoch: 3872 step: 1, loss is 0.006429432891309261\n",
      "epoch: 3873 step: 1, loss is 0.0064241657964885235\n",
      "epoch: 3874 step: 1, loss is 0.006418902892619371\n",
      "epoch: 3875 step: 1, loss is 0.006413645576685667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3876 step: 1, loss is 0.006408394780009985\n",
      "epoch: 3877 step: 1, loss is 0.006403146777302027\n",
      "epoch: 3878 step: 1, loss is 0.006397907622158527\n",
      "epoch: 3879 step: 1, loss is 0.006392672657966614\n",
      "epoch: 3880 step: 1, loss is 0.006387439556419849\n",
      "epoch: 3881 step: 1, loss is 0.00638221763074398\n",
      "epoch: 3882 step: 1, loss is 0.006376998499035835\n",
      "epoch: 3883 step: 1, loss is 0.006371786352247\n",
      "epoch: 3884 step: 1, loss is 0.0063665760681033134\n",
      "epoch: 3885 step: 1, loss is 0.006361376494169235\n",
      "epoch: 3886 step: 1, loss is 0.006356177851557732\n",
      "epoch: 3887 step: 1, loss is 0.006350985262542963\n",
      "epoch: 3888 step: 1, loss is 0.006345798727124929\n",
      "epoch: 3889 step: 1, loss is 0.00634062010794878\n",
      "epoch: 3890 step: 1, loss is 0.006335440557450056\n",
      "epoch: 3891 step: 1, loss is 0.0063302721828222275\n",
      "epoch: 3892 step: 1, loss is 0.00632510706782341\n",
      "epoch: 3893 step: 1, loss is 0.0063199508003890514\n",
      "epoch: 3894 step: 1, loss is 0.006314792670309544\n",
      "epoch: 3895 step: 1, loss is 0.006309647113084793\n",
      "epoch: 3896 step: 1, loss is 0.006304501090198755\n",
      "epoch: 3897 step: 1, loss is 0.006299363914877176\n",
      "epoch: 3898 step: 1, loss is 0.006294230464845896\n",
      "epoch: 3899 step: 1, loss is 0.006289105396717787\n",
      "epoch: 3900 step: 1, loss is 0.006283978931605816\n",
      "epoch: 3901 step: 1, loss is 0.006278864573687315\n",
      "epoch: 3902 step: 1, loss is 0.006273753009736538\n",
      "epoch: 3903 step: 1, loss is 0.006268646568059921\n",
      "epoch: 3904 step: 1, loss is 0.00626354617998004\n",
      "epoch: 3905 step: 1, loss is 0.006258447654545307\n",
      "epoch: 3906 step: 1, loss is 0.006253358907997608\n",
      "epoch: 3907 step: 1, loss is 0.006248275749385357\n",
      "epoch: 3908 step: 1, loss is 0.0062431939877569675\n",
      "epoch: 3909 step: 1, loss is 0.0062381187453866005\n",
      "epoch: 3910 step: 1, loss is 0.0062330495566129684\n",
      "epoch: 3911 step: 1, loss is 0.0062279850244522095\n",
      "epoch: 3912 step: 1, loss is 0.006222923751920462\n",
      "epoch: 3913 step: 1, loss is 0.006217872258275747\n",
      "epoch: 3914 step: 1, loss is 0.006212819367647171\n",
      "epoch: 3915 step: 1, loss is 0.006207780912518501\n",
      "epoch: 3916 step: 1, loss is 0.006202740129083395\n",
      "epoch: 3917 step: 1, loss is 0.006197704933583736\n",
      "epoch: 3918 step: 1, loss is 0.006192677654325962\n",
      "epoch: 3919 step: 1, loss is 0.006187656428664923\n",
      "epoch: 3920 step: 1, loss is 0.006182638928294182\n",
      "epoch: 3921 step: 1, loss is 0.006177624687552452\n",
      "epoch: 3922 step: 1, loss is 0.006172617897391319\n",
      "epoch: 3923 step: 1, loss is 0.006167616695165634\n",
      "epoch: 3924 step: 1, loss is 0.006162620149552822\n",
      "epoch: 3925 step: 1, loss is 0.006157627794891596\n",
      "epoch: 3926 step: 1, loss is 0.006152639165520668\n",
      "epoch: 3927 step: 1, loss is 0.0061476584523916245\n",
      "epoch: 3928 step: 1, loss is 0.006142682395875454\n",
      "epoch: 3929 step: 1, loss is 0.006137710064649582\n",
      "epoch: 3930 step: 1, loss is 0.00613274285569787\n",
      "epoch: 3931 step: 1, loss is 0.006127781700342894\n",
      "epoch: 3932 step: 1, loss is 0.0061228275299072266\n",
      "epoch: 3933 step: 1, loss is 0.006117875687777996\n",
      "epoch: 3934 step: 1, loss is 0.006112929433584213\n",
      "epoch: 3935 step: 1, loss is 0.00610799016430974\n",
      "epoch: 3936 step: 1, loss is 0.0061030518263578415\n",
      "epoch: 3937 step: 1, loss is 0.006098119542002678\n",
      "epoch: 3938 step: 1, loss is 0.006093197036534548\n",
      "epoch: 3939 step: 1, loss is 0.006088275462388992\n",
      "epoch: 3940 step: 1, loss is 0.00608335854485631\n",
      "epoch: 3941 step: 1, loss is 0.006078450009226799\n",
      "epoch: 3942 step: 1, loss is 0.006073545198887587\n",
      "epoch: 3943 step: 1, loss is 0.006068641319870949\n",
      "epoch: 3944 step: 1, loss is 0.006063747685402632\n",
      "epoch: 3945 step: 1, loss is 0.0060588582418859005\n",
      "epoch: 3946 step: 1, loss is 0.0060539706610143185\n",
      "epoch: 3947 step: 1, loss is 0.006049090530723333\n",
      "epoch: 3948 step: 1, loss is 0.006044215522706509\n",
      "epoch: 3949 step: 1, loss is 0.006039341911673546\n",
      "epoch: 3950 step: 1, loss is 0.006034476682543755\n",
      "epoch: 3951 step: 1, loss is 0.006029615178704262\n",
      "epoch: 3952 step: 1, loss is 0.00602476391941309\n",
      "epoch: 3953 step: 1, loss is 0.006019912660121918\n",
      "epoch: 3954 step: 1, loss is 0.0060150641947984695\n",
      "epoch: 3955 step: 1, loss is 0.006010225508362055\n",
      "epoch: 3956 step: 1, loss is 0.006005386356264353\n",
      "epoch: 3957 step: 1, loss is 0.006000555120408535\n",
      "epoch: 3958 step: 1, loss is 0.005995729472488165\n",
      "epoch: 3959 step: 1, loss is 0.005990907549858093\n",
      "epoch: 3960 step: 1, loss is 0.005986094009131193\n",
      "epoch: 3961 step: 1, loss is 0.005981280002743006\n",
      "epoch: 3962 step: 1, loss is 0.005976477172225714\n",
      "epoch: 3963 step: 1, loss is 0.0059716710820794106\n",
      "epoch: 3964 step: 1, loss is 0.005966874770820141\n",
      "epoch: 3965 step: 1, loss is 0.0059620835818350315\n",
      "epoch: 3966 step: 1, loss is 0.005957295652478933\n",
      "epoch: 3967 step: 1, loss is 0.0059525142423808575\n",
      "epoch: 3968 step: 1, loss is 0.005947738420218229\n",
      "epoch: 3969 step: 1, loss is 0.005942966789007187\n",
      "epoch: 3970 step: 1, loss is 0.0059381998144090176\n",
      "epoch: 3971 step: 1, loss is 0.005933432374149561\n",
      "epoch: 3972 step: 1, loss is 0.005928676575422287\n",
      "epoch: 3973 step: 1, loss is 0.005923923570662737\n",
      "epoch: 3974 step: 1, loss is 0.005919173359870911\n",
      "epoch: 3975 step: 1, loss is 0.005914431996643543\n",
      "epoch: 3976 step: 1, loss is 0.005909695290029049\n",
      "epoch: 3977 step: 1, loss is 0.005904958583414555\n",
      "epoch: 3978 step: 1, loss is 0.005900230724364519\n",
      "epoch: 3979 step: 1, loss is 0.005895506124943495\n",
      "epoch: 3980 step: 1, loss is 0.005890786647796631\n",
      "epoch: 3981 step: 1, loss is 0.005886072758585215\n",
      "epoch: 3982 step: 1, loss is 0.005881360732018948\n",
      "epoch: 3983 step: 1, loss is 0.0058766561560332775\n",
      "epoch: 3984 step: 1, loss is 0.005871954374015331\n",
      "epoch: 3985 step: 1, loss is 0.005867261905223131\n",
      "epoch: 3986 step: 1, loss is 0.005862570833414793\n",
      "epoch: 3987 step: 1, loss is 0.005857882089912891\n",
      "epoch: 3988 step: 1, loss is 0.005853200796991587\n",
      "epoch: 3989 step: 1, loss is 0.005848526023328304\n",
      "epoch: 3990 step: 1, loss is 0.0058438535779714584\n",
      "epoch: 3991 step: 1, loss is 0.00583918672055006\n",
      "epoch: 3992 step: 1, loss is 0.005834524054080248\n",
      "epoch: 3993 step: 1, loss is 0.005829865112900734\n",
      "epoch: 3994 step: 1, loss is 0.0058252145536243916\n",
      "epoch: 3995 step: 1, loss is 0.005820564925670624\n",
      "epoch: 3996 step: 1, loss is 0.005815918557345867\n",
      "epoch: 3997 step: 1, loss is 0.005811280570924282\n",
      "epoch: 3998 step: 1, loss is 0.005806646775454283\n",
      "epoch: 3999 step: 1, loss is 0.005802017170935869\n",
      "epoch: 4000 step: 1, loss is 0.005797392688691616\n",
      "epoch: 4001 step: 1, loss is 0.005792771466076374\n",
      "epoch: 4002 step: 1, loss is 0.0057881553657352924\n",
      "epoch: 4003 step: 1, loss is 0.005783543456345797\n",
      "epoch: 4004 step: 1, loss is 0.00577893853187561\n",
      "epoch: 4005 step: 1, loss is 0.00577433779835701\n",
      "epoch: 4006 step: 1, loss is 0.005769738461822271\n",
      "epoch: 4007 step: 1, loss is 0.005765145178884268\n",
      "epoch: 4008 step: 1, loss is 0.005760557018220425\n",
      "epoch: 4009 step: 1, loss is 0.005755973048508167\n",
      "epoch: 4010 step: 1, loss is 0.00575139420107007\n",
      "epoch: 4011 step: 1, loss is 0.005746821872889996\n",
      "epoch: 4012 step: 1, loss is 0.005742249544709921\n",
      "epoch: 4013 step: 1, loss is 0.005737686529755592\n",
      "epoch: 4014 step: 1, loss is 0.0057331258431077\n",
      "epoch: 4015 step: 1, loss is 0.005728567019104958\n",
      "epoch: 4016 step: 1, loss is 0.005724017508327961\n",
      "epoch: 4017 step: 1, loss is 0.005719471722841263\n",
      "epoch: 4018 step: 1, loss is 0.005714928265661001\n",
      "epoch: 4019 step: 1, loss is 0.0057103936560451984\n",
      "epoch: 4020 step: 1, loss is 0.005705856718122959\n",
      "epoch: 4021 step: 1, loss is 0.005701329093426466\n",
      "epoch: 4022 step: 1, loss is 0.005696805194020271\n",
      "epoch: 4023 step: 1, loss is 0.005692285019904375\n",
      "epoch: 4024 step: 1, loss is 0.005687774624675512\n",
      "epoch: 4025 step: 1, loss is 0.005683261901140213\n",
      "epoch: 4026 step: 1, loss is 0.005678755231201649\n",
      "epoch: 4027 step: 1, loss is 0.005674252286553383\n",
      "epoch: 4028 step: 1, loss is 0.005669757258147001\n",
      "epoch: 4029 step: 1, loss is 0.005665264092385769\n",
      "epoch: 4030 step: 1, loss is 0.005660776514559984\n",
      "epoch: 4031 step: 1, loss is 0.005656293127685785\n",
      "epoch: 4032 step: 1, loss is 0.005651812069118023\n",
      "epoch: 4033 step: 1, loss is 0.005647337529808283\n",
      "epoch: 4034 step: 1, loss is 0.005642867647111416\n",
      "epoch: 4035 step: 1, loss is 0.005638402421027422\n",
      "epoch: 4036 step: 1, loss is 0.005633939523249865\n",
      "epoch: 4037 step: 1, loss is 0.0056294845417141914\n",
      "epoch: 4038 step: 1, loss is 0.005625030491501093\n",
      "epoch: 4039 step: 1, loss is 0.005620583891868591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4040 step: 1, loss is 0.005616141017526388\n",
      "epoch: 4041 step: 1, loss is 0.0056117018684744835\n",
      "epoch: 4042 step: 1, loss is 0.005607266444712877\n",
      "epoch: 4043 step: 1, loss is 0.005602835677564144\n",
      "epoch: 4044 step: 1, loss is 0.005598410032689571\n",
      "epoch: 4045 step: 1, loss is 0.00559398764744401\n",
      "epoch: 4046 step: 1, loss is 0.005589572247117758\n",
      "epoch: 4047 step: 1, loss is 0.005585160106420517\n",
      "epoch: 4048 step: 1, loss is 0.005580752622336149\n",
      "epoch: 4049 step: 1, loss is 0.005576348397880793\n",
      "epoch: 4050 step: 1, loss is 0.0055719478987157345\n",
      "epoch: 4051 step: 1, loss is 0.005567552521824837\n",
      "epoch: 4052 step: 1, loss is 0.005563161801546812\n",
      "epoch: 4053 step: 1, loss is 0.005558778066188097\n",
      "epoch: 4054 step: 1, loss is 0.005554392002522945\n",
      "epoch: 4055 step: 1, loss is 0.005550015717744827\n",
      "epoch: 4056 step: 1, loss is 0.005545642226934433\n",
      "epoch: 4057 step: 1, loss is 0.005541273392736912\n",
      "epoch: 4058 step: 1, loss is 0.005536908749490976\n",
      "epoch: 4059 step: 1, loss is 0.005532548762857914\n",
      "epoch: 4060 step: 1, loss is 0.005528191104531288\n",
      "epoch: 4061 step: 1, loss is 0.005523842293769121\n",
      "epoch: 4062 step: 1, loss is 0.005519490223377943\n",
      "epoch: 4063 step: 1, loss is 0.005515150260180235\n",
      "epoch: 4064 step: 1, loss is 0.005510811693966389\n",
      "epoch: 4065 step: 1, loss is 0.005506475921720266\n",
      "epoch: 4066 step: 1, loss is 0.005502146668732166\n",
      "epoch: 4067 step: 1, loss is 0.005497819744050503\n",
      "epoch: 4068 step: 1, loss is 0.005493499338626862\n",
      "epoch: 4069 step: 1, loss is 0.005489180330187082\n",
      "epoch: 4070 step: 1, loss is 0.005484866444021463\n",
      "epoch: 4071 step: 1, loss is 0.005480559077113867\n",
      "epoch: 4072 step: 1, loss is 0.0054762535728514194\n",
      "epoch: 4073 step: 1, loss is 0.005471954587846994\n",
      "epoch: 4074 step: 1, loss is 0.005467656068503857\n",
      "epoch: 4075 step: 1, loss is 0.005463364999741316\n",
      "epoch: 4076 step: 1, loss is 0.0054590776562690735\n",
      "epoch: 4077 step: 1, loss is 0.0054547968320548534\n",
      "epoch: 4078 step: 1, loss is 0.005450516939163208\n",
      "epoch: 4079 step: 1, loss is 0.005446240771561861\n",
      "epoch: 4080 step: 1, loss is 0.005441972054541111\n",
      "epoch: 4081 step: 1, loss is 0.0054377042688429356\n",
      "epoch: 4082 step: 1, loss is 0.005433440674096346\n",
      "epoch: 4083 step: 1, loss is 0.005429184529930353\n",
      "epoch: 4084 step: 1, loss is 0.005424928851425648\n",
      "epoch: 4085 step: 1, loss is 0.005420680157840252\n",
      "epoch: 4086 step: 1, loss is 0.005416432395577431\n",
      "epoch: 4087 step: 1, loss is 0.005412192549556494\n",
      "epoch: 4088 step: 1, loss is 0.00540795736014843\n",
      "epoch: 4089 step: 1, loss is 0.005403724033385515\n",
      "epoch: 4090 step: 1, loss is 0.005399493966251612\n",
      "epoch: 4091 step: 1, loss is 0.005395267158746719\n",
      "epoch: 4092 step: 1, loss is 0.005391049198806286\n",
      "epoch: 4093 step: 1, loss is 0.00538683170452714\n",
      "epoch: 4094 step: 1, loss is 0.0053826202638447285\n",
      "epoch: 4095 step: 1, loss is 0.00537841347977519\n",
      "epoch: 4096 step: 1, loss is 0.005374209955334663\n",
      "epoch: 4097 step: 1, loss is 0.005370010156184435\n",
      "epoch: 4098 step: 1, loss is 0.005365812219679356\n",
      "epoch: 4099 step: 1, loss is 0.005361618939787149\n",
      "epoch: 4100 step: 1, loss is 0.005357434041798115\n",
      "epoch: 4101 step: 1, loss is 0.005353250075131655\n",
      "epoch: 4102 step: 1, loss is 0.00534907216206193\n",
      "epoch: 4103 step: 1, loss is 0.005344895180314779\n",
      "epoch: 4104 step: 1, loss is 0.005340726114809513\n",
      "epoch: 4105 step: 1, loss is 0.005336557514965534\n",
      "epoch: 4106 step: 1, loss is 0.005332393571734428\n",
      "epoch: 4107 step: 1, loss is 0.005328236147761345\n",
      "epoch: 4108 step: 1, loss is 0.005324081052094698\n",
      "epoch: 4109 step: 1, loss is 0.005319930147379637\n",
      "epoch: 4110 step: 1, loss is 0.005315781570971012\n",
      "epoch: 4111 step: 1, loss is 0.005311639979481697\n",
      "epoch: 4112 step: 1, loss is 0.005307500716298819\n",
      "epoch: 4113 step: 1, loss is 0.005303367041051388\n",
      "epoch: 4114 step: 1, loss is 0.005299236159771681\n",
      "epoch: 4115 step: 1, loss is 0.005295109003782272\n",
      "epoch: 4116 step: 1, loss is 0.005290988367050886\n",
      "epoch: 4117 step: 1, loss is 0.005286866798996925\n",
      "epoch: 4118 step: 1, loss is 0.005282754078507423\n",
      "epoch: 4119 step: 1, loss is 0.005278644617646933\n",
      "epoch: 4120 step: 1, loss is 0.005274537950754166\n",
      "epoch: 4121 step: 1, loss is 0.0052704354748129845\n",
      "epoch: 4122 step: 1, loss is 0.0052663362585008144\n",
      "epoch: 4123 step: 1, loss is 0.005262244492769241\n",
      "epoch: 4124 step: 1, loss is 0.00525815412402153\n",
      "epoch: 4125 step: 1, loss is 0.005254064686596394\n",
      "epoch: 4126 step: 1, loss is 0.005249983165413141\n",
      "epoch: 4127 step: 1, loss is 0.005245906300842762\n",
      "epoch: 4128 step: 1, loss is 0.005241830833256245\n",
      "epoch: 4129 step: 1, loss is 0.005237761419266462\n",
      "epoch: 4130 step: 1, loss is 0.0052336957305669785\n",
      "epoch: 4131 step: 1, loss is 0.005229630973190069\n",
      "epoch: 4132 step: 1, loss is 0.0052255732007324696\n",
      "epoch: 4133 step: 1, loss is 0.005221515893936157\n",
      "epoch: 4134 step: 1, loss is 0.005217468831688166\n",
      "epoch: 4135 step: 1, loss is 0.005213421303778887\n",
      "epoch: 4136 step: 1, loss is 0.005209376569837332\n",
      "epoch: 4137 step: 1, loss is 0.005205336958169937\n",
      "epoch: 4138 step: 1, loss is 0.005201303865760565\n",
      "epoch: 4139 step: 1, loss is 0.005197272635996342\n",
      "epoch: 4140 step: 1, loss is 0.005193245131522417\n",
      "epoch: 4141 step: 1, loss is 0.0051892222836613655\n",
      "epoch: 4142 step: 1, loss is 0.005185201298445463\n",
      "epoch: 4143 step: 1, loss is 0.005181186832487583\n",
      "epoch: 4144 step: 1, loss is 0.005177174229174852\n",
      "epoch: 4145 step: 1, loss is 0.005173165816813707\n",
      "epoch: 4146 step: 1, loss is 0.005169161129742861\n",
      "epoch: 4147 step: 1, loss is 0.005165163893252611\n",
      "epoch: 4148 step: 1, loss is 0.005161167588084936\n",
      "epoch: 4149 step: 1, loss is 0.005157175939530134\n",
      "epoch: 4150 step: 1, loss is 0.0051531861536204815\n",
      "epoch: 4151 step: 1, loss is 0.005149201489984989\n",
      "epoch: 4152 step: 1, loss is 0.0051452224142849445\n",
      "epoch: 4153 step: 1, loss is 0.005141246132552624\n",
      "epoch: 4154 step: 1, loss is 0.005137273576110601\n",
      "epoch: 4155 step: 1, loss is 0.00513330427929759\n",
      "epoch: 4156 step: 1, loss is 0.0051293387077748775\n",
      "epoch: 4157 step: 1, loss is 0.005125375930219889\n",
      "epoch: 4158 step: 1, loss is 0.00512141827493906\n",
      "epoch: 4159 step: 1, loss is 0.005117465276271105\n",
      "epoch: 4160 step: 1, loss is 0.005113516468554735\n",
      "epoch: 4161 step: 1, loss is 0.0051095690578222275\n",
      "epoch: 4162 step: 1, loss is 0.0051056272350251675\n",
      "epoch: 4163 step: 1, loss is 0.005101689603179693\n",
      "epoch: 4164 step: 1, loss is 0.005097754765301943\n",
      "epoch: 4165 step: 1, loss is 0.0050938245840370655\n",
      "epoch: 4166 step: 1, loss is 0.005089896265417337\n",
      "epoch: 4167 step: 1, loss is 0.005085972603410482\n",
      "epoch: 4168 step: 1, loss is 0.005082054529339075\n",
      "epoch: 4169 step: 1, loss is 0.005078136455267668\n",
      "epoch: 4170 step: 1, loss is 0.00507422536611557\n",
      "epoch: 4171 step: 1, loss is 0.0050703175365924835\n",
      "epoch: 4172 step: 1, loss is 0.005066412501037121\n",
      "epoch: 4173 step: 1, loss is 0.005062511656433344\n",
      "epoch: 4174 step: 1, loss is 0.005058615002781153\n",
      "epoch: 4175 step: 1, loss is 0.00505472207441926\n",
      "epoch: 4176 step: 1, loss is 0.005050833337008953\n",
      "epoch: 4177 step: 1, loss is 0.005046948324888945\n",
      "epoch: 4178 step: 1, loss is 0.00504306610673666\n",
      "epoch: 4179 step: 1, loss is 0.005039188079535961\n",
      "epoch: 4180 step: 1, loss is 0.005035312380641699\n",
      "epoch: 4181 step: 1, loss is 0.00503144133836031\n",
      "epoch: 4182 step: 1, loss is 0.005027575418353081\n",
      "epoch: 4183 step: 1, loss is 0.005023711360991001\n",
      "epoch: 4184 step: 1, loss is 0.005019851960241795\n",
      "epoch: 4185 step: 1, loss is 0.005015995353460312\n",
      "epoch: 4186 step: 1, loss is 0.00501214200630784\n",
      "epoch: 4187 step: 1, loss is 0.005008295178413391\n",
      "epoch: 4188 step: 1, loss is 0.005004447884857655\n",
      "epoch: 4189 step: 1, loss is 0.005000607576221228\n",
      "epoch: 4190 step: 1, loss is 0.004996770992875099\n",
      "epoch: 4191 step: 1, loss is 0.004992939066141844\n",
      "epoch: 4192 step: 1, loss is 0.004989107605069876\n",
      "epoch: 4193 step: 1, loss is 0.004985279403626919\n",
      "epoch: 4194 step: 1, loss is 0.0049814581871032715\n",
      "epoch: 4195 step: 1, loss is 0.004977638833224773\n",
      "epoch: 4196 step: 1, loss is 0.00497382041066885\n",
      "epoch: 4197 step: 1, loss is 0.004970010370016098\n",
      "epoch: 4198 step: 1, loss is 0.004966201726347208\n",
      "epoch: 4199 step: 1, loss is 0.004962398204952478\n",
      "epoch: 4200 step: 1, loss is 0.004958596546202898\n",
      "epoch: 4201 step: 1, loss is 0.004954799544066191\n",
      "epoch: 4202 step: 1, loss is 0.004951006732881069\n",
      "epoch: 4203 step: 1, loss is 0.004947216250002384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4204 step: 1, loss is 0.00494342902675271\n",
      "epoch: 4205 step: 1, loss is 0.004939646925777197\n",
      "epoch: 4206 step: 1, loss is 0.004935867618769407\n",
      "epoch: 4207 step: 1, loss is 0.004932093899697065\n",
      "epoch: 4208 step: 1, loss is 0.004928321577608585\n",
      "epoch: 4209 step: 1, loss is 0.004924550652503967\n",
      "epoch: 4210 step: 1, loss is 0.0049207876436412334\n",
      "epoch: 4211 step: 1, loss is 0.004917025100439787\n",
      "epoch: 4212 step: 1, loss is 0.0049132672138512135\n",
      "epoch: 4213 step: 1, loss is 0.004909515380859375\n",
      "epoch: 4214 step: 1, loss is 0.004905764013528824\n",
      "epoch: 4215 step: 1, loss is 0.004902016371488571\n",
      "epoch: 4216 step: 1, loss is 0.0048982747830450535\n",
      "epoch: 4217 step: 1, loss is 0.004894535522907972\n",
      "epoch: 4218 step: 1, loss is 0.00489079998806119\n",
      "epoch: 4219 step: 1, loss is 0.00488706398755312\n",
      "epoch: 4220 step: 1, loss is 0.0048833368346095085\n",
      "epoch: 4221 step: 1, loss is 0.004879612475633621\n",
      "epoch: 4222 step: 1, loss is 0.004875888116657734\n",
      "epoch: 4223 step: 1, loss is 0.004872173070907593\n",
      "epoch: 4224 step: 1, loss is 0.004868457093834877\n",
      "epoch: 4225 step: 1, loss is 0.004864745307713747\n",
      "epoch: 4226 step: 1, loss is 0.004861038643866777\n",
      "epoch: 4227 step: 1, loss is 0.004857333842664957\n",
      "epoch: 4228 step: 1, loss is 0.004853631369769573\n",
      "epoch: 4229 step: 1, loss is 0.004849937744438648\n",
      "epoch: 4230 step: 1, loss is 0.004846242722123861\n",
      "epoch: 4231 step: 1, loss is 0.0048425509594380856\n",
      "epoch: 4232 step: 1, loss is 0.004838865250349045\n",
      "epoch: 4233 step: 1, loss is 0.004835181869566441\n",
      "epoch: 4234 step: 1, loss is 0.004831504076719284\n",
      "epoch: 4235 step: 1, loss is 0.004827826749533415\n",
      "epoch: 4236 step: 1, loss is 0.004824153613299131\n",
      "epoch: 4237 step: 1, loss is 0.0048204814083874226\n",
      "epoch: 4238 step: 1, loss is 0.00481681851670146\n",
      "epoch: 4239 step: 1, loss is 0.004813154693692923\n",
      "epoch: 4240 step: 1, loss is 0.004809495527297258\n",
      "epoch: 4241 step: 1, loss is 0.00480584055185318\n",
      "epoch: 4242 step: 1, loss is 0.004802187904715538\n",
      "epoch: 4243 step: 1, loss is 0.004798538517206907\n",
      "epoch: 4244 step: 1, loss is 0.004794896114617586\n",
      "epoch: 4245 step: 1, loss is 0.00479125278070569\n",
      "epoch: 4246 step: 1, loss is 0.00478761363774538\n",
      "epoch: 4247 step: 1, loss is 0.004783977754414082\n",
      "epoch: 4248 step: 1, loss is 0.004780349787324667\n",
      "epoch: 4249 step: 1, loss is 0.004776718560606241\n",
      "epoch: 4250 step: 1, loss is 0.0047730952501297\n",
      "epoch: 4251 step: 1, loss is 0.004769476130604744\n",
      "epoch: 4252 step: 1, loss is 0.00476585840806365\n",
      "epoch: 4253 step: 1, loss is 0.00476224347949028\n",
      "epoch: 4254 step: 1, loss is 0.004758631810545921\n",
      "epoch: 4255 step: 1, loss is 0.00475502572953701\n",
      "epoch: 4256 step: 1, loss is 0.004751421976834536\n",
      "epoch: 4257 step: 1, loss is 0.0047478205524384975\n",
      "epoch: 4258 step: 1, loss is 0.004744223318994045\n",
      "epoch: 4259 step: 1, loss is 0.004740630742162466\n",
      "epoch: 4260 step: 1, loss is 0.0047370390966534615\n",
      "epoch: 4261 step: 1, loss is 0.00473345210775733\n",
      "epoch: 4262 step: 1, loss is 0.004729869309812784\n",
      "epoch: 4263 step: 1, loss is 0.004726288840174675\n",
      "epoch: 4264 step: 1, loss is 0.0047227139584720135\n",
      "epoch: 4265 step: 1, loss is 0.004719139076769352\n",
      "epoch: 4266 step: 1, loss is 0.0047155702486634254\n",
      "epoch: 4267 step: 1, loss is 0.004712001420557499\n",
      "epoch: 4268 step: 1, loss is 0.004708439111709595\n",
      "epoch: 4269 step: 1, loss is 0.004704880062490702\n",
      "epoch: 4270 step: 1, loss is 0.004701321478933096\n",
      "epoch: 4271 step: 1, loss is 0.004697766155004501\n",
      "epoch: 4272 step: 1, loss is 0.004694216884672642\n",
      "epoch: 4273 step: 1, loss is 0.004690670408308506\n",
      "epoch: 4274 step: 1, loss is 0.004687129054218531\n",
      "epoch: 4275 step: 1, loss is 0.004683589097112417\n",
      "epoch: 4276 step: 1, loss is 0.00468005146831274\n",
      "epoch: 4277 step: 1, loss is 0.004676515702158213\n",
      "epoch: 4278 step: 1, loss is 0.0046729883179068565\n",
      "epoch: 4279 step: 1, loss is 0.004669458605349064\n",
      "epoch: 4280 step: 1, loss is 0.004665937274694443\n",
      "epoch: 4281 step: 1, loss is 0.004662415944039822\n",
      "epoch: 4282 step: 1, loss is 0.004658899269998074\n",
      "epoch: 4283 step: 1, loss is 0.004655386786907911\n",
      "epoch: 4284 step: 1, loss is 0.004651876166462898\n",
      "epoch: 4285 step: 1, loss is 0.0046483674086630344\n",
      "epoch: 4286 step: 1, loss is 0.004644863307476044\n",
      "epoch: 4287 step: 1, loss is 0.004641362465918064\n",
      "epoch: 4288 step: 1, loss is 0.004637868143618107\n",
      "epoch: 4289 step: 1, loss is 0.00463437382131815\n",
      "epoch: 4290 step: 1, loss is 0.004630881827324629\n",
      "epoch: 4291 step: 1, loss is 0.004627397749572992\n",
      "epoch: 4292 step: 1, loss is 0.0046239132061600685\n",
      "epoch: 4293 step: 1, loss is 0.004620428662747145\n",
      "epoch: 4294 step: 1, loss is 0.00461695296689868\n",
      "epoch: 4295 step: 1, loss is 0.004613478668034077\n",
      "epoch: 4296 step: 1, loss is 0.0046100071631371975\n",
      "epoch: 4297 step: 1, loss is 0.004606539849191904\n",
      "epoch: 4298 step: 1, loss is 0.0046030739322304726\n",
      "epoch: 4299 step: 1, loss is 0.004599612671881914\n",
      "epoch: 4300 step: 1, loss is 0.0045961556024849415\n",
      "epoch: 4301 step: 1, loss is 0.0045926994644105434\n",
      "epoch: 4302 step: 1, loss is 0.004589248914271593\n",
      "epoch: 4303 step: 1, loss is 0.004585798364132643\n",
      "epoch: 4304 step: 1, loss is 0.0045823524706065655\n",
      "epoch: 4305 step: 1, loss is 0.004578910768032074\n",
      "epoch: 4306 step: 1, loss is 0.004575472790747881\n",
      "epoch: 4307 step: 1, loss is 0.004572036676108837\n",
      "epoch: 4308 step: 1, loss is 0.004568604752421379\n",
      "epoch: 4309 step: 1, loss is 0.004565177485346794\n",
      "epoch: 4310 step: 1, loss is 0.004561749752610922\n",
      "epoch: 4311 step: 1, loss is 0.004558328073471785\n",
      "epoch: 4312 step: 1, loss is 0.004554908722639084\n",
      "epoch: 4313 step: 1, loss is 0.0045514898374676704\n",
      "epoch: 4314 step: 1, loss is 0.00454807560890913\n",
      "epoch: 4315 step: 1, loss is 0.00454466650262475\n",
      "epoch: 4316 step: 1, loss is 0.004541259724646807\n",
      "epoch: 4317 step: 1, loss is 0.004537856671959162\n",
      "epoch: 4318 step: 1, loss is 0.0045344531536102295\n",
      "epoch: 4319 step: 1, loss is 0.004531058017164469\n",
      "epoch: 4320 step: 1, loss is 0.004527662880718708\n",
      "epoch: 4321 step: 1, loss is 0.00452427240088582\n",
      "epoch: 4322 step: 1, loss is 0.004520883783698082\n",
      "epoch: 4323 step: 1, loss is 0.004517499357461929\n",
      "epoch: 4324 step: 1, loss is 0.004514118190854788\n",
      "epoch: 4325 step: 1, loss is 0.0045107402838766575\n",
      "epoch: 4326 step: 1, loss is 0.004507362376898527\n",
      "epoch: 4327 step: 1, loss is 0.004503992386162281\n",
      "epoch: 4328 step: 1, loss is 0.00450062146410346\n",
      "epoch: 4329 step: 1, loss is 0.004497256129980087\n",
      "epoch: 4330 step: 1, loss is 0.0044938926585018635\n",
      "epoch: 4331 step: 1, loss is 0.004490534774959087\n",
      "epoch: 4332 step: 1, loss is 0.004487176891416311\n",
      "epoch: 4333 step: 1, loss is 0.004483822733163834\n",
      "epoch: 4334 step: 1, loss is 0.004480472765862942\n",
      "epoch: 4335 step: 1, loss is 0.0044771237298846245\n",
      "epoch: 4336 step: 1, loss is 0.00447377935051918\n",
      "epoch: 4337 step: 1, loss is 0.004470438230782747\n",
      "epoch: 4338 step: 1, loss is 0.004467100370675325\n",
      "epoch: 4339 step: 1, loss is 0.004463765304535627\n",
      "epoch: 4340 step: 1, loss is 0.00446043536067009\n",
      "epoch: 4341 step: 1, loss is 0.004457102622836828\n",
      "epoch: 4342 step: 1, loss is 0.004453777801245451\n",
      "epoch: 4343 step: 1, loss is 0.0044504557736217976\n",
      "epoch: 4344 step: 1, loss is 0.004447135608643293\n",
      "epoch: 4345 step: 1, loss is 0.004443817771971226\n",
      "epoch: 4346 step: 1, loss is 0.004440505523234606\n",
      "epoch: 4347 step: 1, loss is 0.00443719606846571\n",
      "epoch: 4348 step: 1, loss is 0.0044338880106806755\n",
      "epoch: 4349 step: 1, loss is 0.004430581349879503\n",
      "epoch: 4350 step: 1, loss is 0.004427281208336353\n",
      "epoch: 4351 step: 1, loss is 0.004423982463777065\n",
      "epoch: 4352 step: 1, loss is 0.004420686513185501\n",
      "epoch: 4353 step: 1, loss is 0.00441739521920681\n",
      "epoch: 4354 step: 1, loss is 0.004414105787873268\n",
      "epoch: 4355 step: 1, loss is 0.0044108182191848755\n",
      "epoch: 4356 step: 1, loss is 0.0044075362384319305\n",
      "epoch: 4357 step: 1, loss is 0.004404257517307997\n",
      "epoch: 4358 step: 1, loss is 0.004400979727506638\n",
      "epoch: 4359 step: 1, loss is 0.0043977051973342896\n",
      "epoch: 4360 step: 1, loss is 0.00439443439245224\n",
      "epoch: 4361 step: 1, loss is 0.004391166847199202\n",
      "epoch: 4362 step: 1, loss is 0.004387901164591312\n",
      "epoch: 4363 step: 1, loss is 0.004384640604257584\n",
      "epoch: 4364 step: 1, loss is 0.004381380043923855\n",
      "epoch: 4365 step: 1, loss is 0.004378125071525574\n",
      "epoch: 4366 step: 1, loss is 0.0043748714961111546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4367 step: 1, loss is 0.004371621645987034\n",
      "epoch: 4368 step: 1, loss is 0.004368373192846775\n",
      "epoch: 4369 step: 1, loss is 0.004365130793303251\n",
      "epoch: 4370 step: 1, loss is 0.0043618883937597275\n",
      "epoch: 4371 step: 1, loss is 0.004358650650829077\n",
      "epoch: 4372 step: 1, loss is 0.004355415236204863\n",
      "epoch: 4373 step: 1, loss is 0.004352182149887085\n",
      "epoch: 4374 step: 1, loss is 0.004348952788859606\n",
      "epoch: 4375 step: 1, loss is 0.004345729015767574\n",
      "epoch: 4376 step: 1, loss is 0.004342503845691681\n",
      "epoch: 4377 step: 1, loss is 0.004339283332228661\n",
      "epoch: 4378 step: 1, loss is 0.004336066544055939\n",
      "epoch: 4379 step: 1, loss is 0.004332851618528366\n",
      "epoch: 4380 step: 1, loss is 0.0043296413496136665\n",
      "epoch: 4381 step: 1, loss is 0.004326432477682829\n",
      "epoch: 4382 step: 1, loss is 0.004323225934058428\n",
      "epoch: 4383 step: 1, loss is 0.004320023115724325\n",
      "epoch: 4384 step: 1, loss is 0.004316824488341808\n",
      "epoch: 4385 step: 1, loss is 0.004313627723604441\n",
      "epoch: 4386 step: 1, loss is 0.004310433752834797\n",
      "epoch: 4387 step: 1, loss is 0.004307241644710302\n",
      "epoch: 4388 step: 1, loss is 0.004304053261876106\n",
      "epoch: 4389 step: 1, loss is 0.004300868604332209\n",
      "epoch: 4390 step: 1, loss is 0.004297687206417322\n",
      "epoch: 4391 step: 1, loss is 0.004294509068131447\n",
      "epoch: 4392 step: 1, loss is 0.004291331395506859\n",
      "epoch: 4393 step: 1, loss is 0.004288156051188707\n",
      "epoch: 4394 step: 1, loss is 0.004284985829144716\n",
      "epoch: 4395 step: 1, loss is 0.004281818401068449\n",
      "epoch: 4396 step: 1, loss is 0.004278653766959906\n",
      "epoch: 4397 step: 1, loss is 0.0042754909954965115\n",
      "epoch: 4398 step: 1, loss is 0.004272331949323416\n",
      "epoch: 4399 step: 1, loss is 0.004269177094101906\n",
      "epoch: 4400 step: 1, loss is 0.004266022238880396\n",
      "epoch: 4401 step: 1, loss is 0.004262871574610472\n",
      "epoch: 4402 step: 1, loss is 0.0042597223073244095\n",
      "epoch: 4403 step: 1, loss is 0.004256580024957657\n",
      "epoch: 4404 step: 1, loss is 0.00425343681126833\n",
      "epoch: 4405 step: 1, loss is 0.0042502982541918755\n",
      "epoch: 4406 step: 1, loss is 0.004247162491083145\n",
      "epoch: 4407 step: 1, loss is 0.004244029056280851\n",
      "epoch: 4408 step: 1, loss is 0.004240898881107569\n",
      "epoch: 4409 step: 1, loss is 0.004237771034240723\n",
      "epoch: 4410 step: 1, loss is 0.0042346459813416\n",
      "epoch: 4411 step: 1, loss is 0.004231523256748915\n",
      "epoch: 4412 step: 1, loss is 0.0042284042574465275\n",
      "epoch: 4413 step: 1, loss is 0.004225288052111864\n",
      "epoch: 4414 step: 1, loss is 0.0042221746407449245\n",
      "epoch: 4415 step: 1, loss is 0.004219064489006996\n",
      "epoch: 4416 step: 1, loss is 0.004215956665575504\n",
      "epoch: 4417 step: 1, loss is 0.004212852567434311\n",
      "epoch: 4418 step: 1, loss is 0.004209750797599554\n",
      "epoch: 4419 step: 1, loss is 0.00420664856210351\n",
      "epoch: 4420 step: 1, loss is 0.004203552845865488\n",
      "epoch: 4421 step: 1, loss is 0.004200459457933903\n",
      "epoch: 4422 step: 1, loss is 0.004197368398308754\n",
      "epoch: 4423 step: 1, loss is 0.004194282926619053\n",
      "epoch: 4424 step: 1, loss is 0.004191197454929352\n",
      "epoch: 4425 step: 1, loss is 0.0041881138458848\n",
      "epoch: 4426 step: 1, loss is 0.0041850353591144085\n",
      "epoch: 4427 step: 1, loss is 0.004181957338005304\n",
      "epoch: 4428 step: 1, loss is 0.004178880713880062\n",
      "epoch: 4429 step: 1, loss is 0.004175810143351555\n",
      "epoch: 4430 step: 1, loss is 0.004172740038484335\n",
      "epoch: 4431 step: 1, loss is 0.004169676452875137\n",
      "epoch: 4432 step: 1, loss is 0.004166613332927227\n",
      "epoch: 4433 step: 1, loss is 0.004163552541285753\n",
      "epoch: 4434 step: 1, loss is 0.0041604964062571526\n",
      "epoch: 4435 step: 1, loss is 0.004157441668212414\n",
      "epoch: 4436 step: 1, loss is 0.004154388792812824\n",
      "epoch: 4437 step: 1, loss is 0.004151341039687395\n",
      "epoch: 4438 step: 1, loss is 0.0041482956148684025\n",
      "epoch: 4439 step: 1, loss is 0.004145250655710697\n",
      "epoch: 4440 step: 1, loss is 0.004142209887504578\n",
      "epoch: 4441 step: 1, loss is 0.00413917051628232\n",
      "epoch: 4442 step: 1, loss is 0.004136133939027786\n",
      "epoch: 4443 step: 1, loss is 0.004133101087063551\n",
      "epoch: 4444 step: 1, loss is 0.004130071494728327\n",
      "epoch: 4445 step: 1, loss is 0.004127045162022114\n",
      "epoch: 4446 step: 1, loss is 0.004124021157622337\n",
      "epoch: 4447 step: 1, loss is 0.004120997153222561\n",
      "epoch: 4448 step: 1, loss is 0.004117980599403381\n",
      "epoch: 4449 step: 1, loss is 0.004114962182939053\n",
      "epoch: 4450 step: 1, loss is 0.004111949820071459\n",
      "epoch: 4451 step: 1, loss is 0.004108940716832876\n",
      "epoch: 4452 step: 1, loss is 0.004105931613594294\n",
      "epoch: 4453 step: 1, loss is 0.004102926701307297\n",
      "epoch: 4454 step: 1, loss is 0.004099923651665449\n",
      "epoch: 4455 step: 1, loss is 0.004096921533346176\n",
      "epoch: 4456 step: 1, loss is 0.004093924071639776\n",
      "epoch: 4457 step: 1, loss is 0.0040909294039011\n",
      "epoch: 4458 step: 1, loss is 0.004087937995791435\n",
      "epoch: 4459 step: 1, loss is 0.0040849498473107815\n",
      "epoch: 4460 step: 1, loss is 0.004081963561475277\n",
      "epoch: 4461 step: 1, loss is 0.004078980535268784\n",
      "epoch: 4462 step: 1, loss is 0.004075996577739716\n",
      "epoch: 4463 step: 1, loss is 0.004073019605129957\n",
      "epoch: 4464 step: 1, loss is 0.004070042632520199\n",
      "epoch: 4465 step: 1, loss is 0.0040670703165233135\n",
      "epoch: 4466 step: 1, loss is 0.004064097069203854\n",
      "epoch: 4467 step: 1, loss is 0.004061129875481129\n",
      "epoch: 4468 step: 1, loss is 0.0040581668727099895\n",
      "epoch: 4469 step: 1, loss is 0.0040552024729549885\n",
      "epoch: 4470 step: 1, loss is 0.00405224459245801\n",
      "epoch: 4471 step: 1, loss is 0.004049286711961031\n",
      "epoch: 4472 step: 1, loss is 0.004046332091093063\n",
      "epoch: 4473 step: 1, loss is 0.004043381195515394\n",
      "epoch: 4474 step: 1, loss is 0.0040404312312603\n",
      "epoch: 4475 step: 1, loss is 0.004037484060972929\n",
      "epoch: 4476 step: 1, loss is 0.00403453828766942\n",
      "epoch: 4477 step: 1, loss is 0.0040315985679626465\n",
      "epoch: 4478 step: 1, loss is 0.00402865931391716\n",
      "epoch: 4479 step: 1, loss is 0.004025724250823259\n",
      "epoch: 4480 step: 1, loss is 0.004022791050374508\n",
      "epoch: 4481 step: 1, loss is 0.004019861109554768\n",
      "epoch: 4482 step: 1, loss is 0.004016929306089878\n",
      "epoch: 4483 step: 1, loss is 0.004014006350189447\n",
      "epoch: 4484 step: 1, loss is 0.004011081997305155\n",
      "epoch: 4485 step: 1, loss is 0.004008161369711161\n",
      "epoch: 4486 step: 1, loss is 0.004005244467407465\n",
      "epoch: 4487 step: 1, loss is 0.004002328030765057\n",
      "epoch: 4488 step: 1, loss is 0.003999419044703245\n",
      "epoch: 4489 step: 1, loss is 0.003996505402028561\n",
      "epoch: 4490 step: 1, loss is 0.003993598744273186\n",
      "epoch: 4491 step: 1, loss is 0.003990693483501673\n",
      "epoch: 4492 step: 1, loss is 0.003987794276326895\n",
      "epoch: 4493 step: 1, loss is 0.003984893672168255\n",
      "epoch: 4494 step: 1, loss is 0.003981994464993477\n",
      "epoch: 4495 step: 1, loss is 0.003979099448770285\n",
      "epoch: 4496 step: 1, loss is 0.00397621002048254\n",
      "epoch: 4497 step: 1, loss is 0.003973320592194796\n",
      "epoch: 4498 step: 1, loss is 0.003970432095229626\n",
      "epoch: 4499 step: 1, loss is 0.0039675491861999035\n",
      "epoch: 4500 step: 1, loss is 0.003964669071137905\n",
      "epoch: 4501 step: 1, loss is 0.003961791284382343\n",
      "epoch: 4502 step: 1, loss is 0.0039589121006429195\n",
      "epoch: 4503 step: 1, loss is 0.0039560385048389435\n",
      "epoch: 4504 step: 1, loss is 0.003953166771680117\n",
      "epoch: 4505 step: 1, loss is 0.003950297366827726\n",
      "epoch: 4506 step: 1, loss is 0.003947432152926922\n",
      "epoch: 4507 step: 1, loss is 0.003944567870348692\n",
      "epoch: 4508 step: 1, loss is 0.003941708244383335\n",
      "epoch: 4509 step: 1, loss is 0.003938847687095404\n",
      "epoch: 4510 step: 1, loss is 0.0039359936490654945\n",
      "epoch: 4511 step: 1, loss is 0.003933140076696873\n",
      "epoch: 4512 step: 1, loss is 0.003930286504328251\n",
      "epoch: 4513 step: 1, loss is 0.003927437588572502\n",
      "epoch: 4514 step: 1, loss is 0.003924592398107052\n",
      "epoch: 4515 step: 1, loss is 0.0039217509329319\n",
      "epoch: 4516 step: 1, loss is 0.0039189099334180355\n",
      "epoch: 4517 step: 1, loss is 0.0039160712622106075\n",
      "epoch: 4518 step: 1, loss is 0.003913236316293478\n",
      "epoch: 4519 step: 1, loss is 0.003910403698682785\n",
      "epoch: 4520 step: 1, loss is 0.003907573409378529\n",
      "epoch: 4521 step: 1, loss is 0.0039047435857355595\n",
      "epoch: 4522 step: 1, loss is 0.003901917953044176\n",
      "epoch: 4523 step: 1, loss is 0.003899096045643091\n",
      "epoch: 4524 step: 1, loss is 0.0038962727412581444\n",
      "epoch: 4525 step: 1, loss is 0.003893455723300576\n",
      "epoch: 4526 step: 1, loss is 0.0038906396366655827\n",
      "epoch: 4527 step: 1, loss is 0.0038878258783370256\n",
      "epoch: 4528 step: 1, loss is 0.0038850153796374798\n",
      "epoch: 4529 step: 1, loss is 0.003882207442075014\n",
      "epoch: 4530 step: 1, loss is 0.0038794027641415596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4531 step: 1, loss is 0.0038765985518693924\n",
      "epoch: 4532 step: 1, loss is 0.00387379783205688\n",
      "epoch: 4533 step: 1, loss is 0.0038709999062120914\n",
      "epoch: 4534 step: 1, loss is 0.0038682040758430958\n",
      "epoch: 4535 step: 1, loss is 0.003865411039441824\n",
      "epoch: 4536 step: 1, loss is 0.003862619400024414\n",
      "epoch: 4537 step: 1, loss is 0.0038598303217440844\n",
      "epoch: 4538 step: 1, loss is 0.0038570468313992023\n",
      "epoch: 4539 step: 1, loss is 0.0038542598485946655\n",
      "epoch: 4540 step: 1, loss is 0.003851480782032013\n",
      "epoch: 4541 step: 1, loss is 0.0038487003184854984\n",
      "epoch: 4542 step: 1, loss is 0.003845924511551857\n",
      "epoch: 4543 step: 1, loss is 0.0038431514985859394\n",
      "epoch: 4544 step: 1, loss is 0.0038403798826038837\n",
      "epoch: 4545 step: 1, loss is 0.003837612457573414\n",
      "epoch: 4546 step: 1, loss is 0.003834844334051013\n",
      "epoch: 4547 step: 1, loss is 0.0038320838939398527\n",
      "epoch: 4548 step: 1, loss is 0.003829321591183543\n",
      "epoch: 4549 step: 1, loss is 0.0038265613839030266\n",
      "epoch: 4550 step: 1, loss is 0.003823805833235383\n",
      "epoch: 4551 step: 1, loss is 0.0038210521452128887\n",
      "epoch: 4552 step: 1, loss is 0.0038182991556823254\n",
      "epoch: 4553 step: 1, loss is 0.003815548960119486\n",
      "epoch: 4554 step: 1, loss is 0.0038128052838146687\n",
      "epoch: 4555 step: 1, loss is 0.003810061374679208\n",
      "epoch: 4556 step: 1, loss is 0.0038073183968663216\n",
      "epoch: 4557 step: 1, loss is 0.0038045786786824465\n",
      "epoch: 4558 step: 1, loss is 0.003801839891821146\n",
      "epoch: 4559 step: 1, loss is 0.003799106227234006\n",
      "epoch: 4560 step: 1, loss is 0.003796372562646866\n",
      "epoch: 4561 step: 1, loss is 0.0037936412263661623\n",
      "epoch: 4562 step: 1, loss is 0.003790916409343481\n",
      "epoch: 4563 step: 1, loss is 0.0037881890311837196\n",
      "epoch: 4564 step: 1, loss is 0.0037854653783142567\n",
      "epoch: 4565 step: 1, loss is 0.0037827480118721724\n",
      "epoch: 4566 step: 1, loss is 0.003780028782784939\n",
      "epoch: 4567 step: 1, loss is 0.0037773142103105783\n",
      "epoch: 4568 step: 1, loss is 0.003774596843868494\n",
      "epoch: 4569 step: 1, loss is 0.0037718883249908686\n",
      "epoch: 4570 step: 1, loss is 0.0037691807374358177\n",
      "epoch: 4571 step: 1, loss is 0.0037664715200662613\n",
      "epoch: 4572 step: 1, loss is 0.003763770218938589\n",
      "epoch: 4573 step: 1, loss is 0.0037610691506415606\n",
      "epoch: 4574 step: 1, loss is 0.0037583704106509686\n",
      "epoch: 4575 step: 1, loss is 0.0037556756287813187\n",
      "epoch: 4576 step: 1, loss is 0.003752980148419738\n",
      "epoch: 4577 step: 1, loss is 0.003750287462025881\n",
      "epoch: 4578 step: 1, loss is 0.0037475971039384604\n",
      "epoch: 4579 step: 1, loss is 0.0037449111696332693\n",
      "epoch: 4580 step: 1, loss is 0.00374222663231194\n",
      "epoch: 4581 step: 1, loss is 0.003739542793482542\n",
      "epoch: 4582 step: 1, loss is 0.0037368633784353733\n",
      "epoch: 4583 step: 1, loss is 0.003734186291694641\n",
      "epoch: 4584 step: 1, loss is 0.00373150990344584\n",
      "epoch: 4585 step: 1, loss is 0.0037288374733179808\n",
      "epoch: 4586 step: 1, loss is 0.0037261685356497765\n",
      "epoch: 4587 step: 1, loss is 0.003723498433828354\n",
      "epoch: 4588 step: 1, loss is 0.0037208315916359425\n",
      "epoch: 4589 step: 1, loss is 0.003718166146427393\n",
      "epoch: 4590 step: 1, loss is 0.003715503728017211\n",
      "epoch: 4591 step: 1, loss is 0.003712845966219902\n",
      "epoch: 4592 step: 1, loss is 0.003710188204422593\n",
      "epoch: 4593 step: 1, loss is 0.0037075350992381573\n",
      "epoch: 4594 step: 1, loss is 0.003704883623868227\n",
      "epoch: 4595 step: 1, loss is 0.003702234709635377\n",
      "epoch: 4596 step: 1, loss is 0.003699587658047676\n",
      "epoch: 4597 step: 1, loss is 0.0036969417706131935\n",
      "epoch: 4598 step: 1, loss is 0.0036942968145012856\n",
      "epoch: 4599 step: 1, loss is 0.0036916558165103197\n",
      "epoch: 4600 step: 1, loss is 0.0036890176124870777\n",
      "epoch: 4601 step: 1, loss is 0.0036863810382783413\n",
      "epoch: 4602 step: 1, loss is 0.003683747025206685\n",
      "epoch: 4603 step: 1, loss is 0.003681117668747902\n",
      "epoch: 4604 step: 1, loss is 0.0036784890107810497\n",
      "epoch: 4605 step: 1, loss is 0.003675861284136772\n",
      "epoch: 4606 step: 1, loss is 0.0036732363514602184\n",
      "epoch: 4607 step: 1, loss is 0.0036706167738884687\n",
      "epoch: 4608 step: 1, loss is 0.00366799533367157\n",
      "epoch: 4609 step: 1, loss is 0.0036653773859143257\n",
      "epoch: 4610 step: 1, loss is 0.0036627603694796562\n",
      "epoch: 4611 step: 1, loss is 0.003660147776827216\n",
      "epoch: 4612 step: 1, loss is 0.003657535882666707\n",
      "epoch: 4613 step: 1, loss is 0.003654927248135209\n",
      "epoch: 4614 step: 1, loss is 0.0036523216404020786\n",
      "epoch: 4615 step: 1, loss is 0.0036497197579592466\n",
      "epoch: 4616 step: 1, loss is 0.0036471167113631964\n",
      "epoch: 4617 step: 1, loss is 0.00364451901987195\n",
      "epoch: 4618 step: 1, loss is 0.0036419187672436237\n",
      "epoch: 4619 step: 1, loss is 0.0036393236368894577\n",
      "epoch: 4620 step: 1, loss is 0.003636731766164303\n",
      "epoch: 4621 step: 1, loss is 0.0036341401282697916\n",
      "epoch: 4622 step: 1, loss is 0.0036315531469881535\n",
      "epoch: 4623 step: 1, loss is 0.0036289659328758717\n",
      "epoch: 4624 step: 1, loss is 0.0036263829097151756\n",
      "epoch: 4625 step: 1, loss is 0.003623802214860916\n",
      "epoch: 4626 step: 1, loss is 0.003621223848313093\n",
      "epoch: 4627 step: 1, loss is 0.003618646180257201\n",
      "epoch: 4628 step: 1, loss is 0.003616071306169033\n",
      "epoch: 4629 step: 1, loss is 0.0036134994588792324\n",
      "epoch: 4630 step: 1, loss is 0.003610928077250719\n",
      "epoch: 4631 step: 1, loss is 0.0036083576269447803\n",
      "epoch: 4632 step: 1, loss is 0.0036057927645742893\n",
      "epoch: 4633 step: 1, loss is 0.0036032292991876602\n",
      "epoch: 4634 step: 1, loss is 0.003600670024752617\n",
      "epoch: 4635 step: 1, loss is 0.003598108422011137\n",
      "epoch: 4636 step: 1, loss is 0.00359555147588253\n",
      "epoch: 4637 step: 1, loss is 0.0035929977893829346\n",
      "epoch: 4638 step: 1, loss is 0.0035904436372220516\n",
      "epoch: 4639 step: 1, loss is 0.003587893443182111\n",
      "epoch: 4640 step: 1, loss is 0.0035853444132953882\n",
      "epoch: 4641 step: 1, loss is 0.0035827967803925276\n",
      "epoch: 4642 step: 1, loss is 0.003580254502594471\n",
      "epoch: 4643 step: 1, loss is 0.0035777143202722073\n",
      "epoch: 4644 step: 1, loss is 0.003575175302103162\n",
      "epoch: 4645 step: 1, loss is 0.003572638612240553\n",
      "epoch: 4646 step: 1, loss is 0.0035701014567166567\n",
      "epoch: 4647 step: 1, loss is 0.0035675708204507828\n",
      "epoch: 4648 step: 1, loss is 0.0035650383215397596\n",
      "epoch: 4649 step: 1, loss is 0.003562511410564184\n",
      "epoch: 4650 step: 1, loss is 0.003559982404112816\n",
      "epoch: 4651 step: 1, loss is 0.00355745991691947\n",
      "epoch: 4652 step: 1, loss is 0.003554937429726124\n",
      "epoch: 4653 step: 1, loss is 0.0035524163395166397\n",
      "epoch: 4654 step: 1, loss is 0.003549901768565178\n",
      "epoch: 4655 step: 1, loss is 0.0035473862662911415\n",
      "epoch: 4656 step: 1, loss is 0.0035448733251541853\n",
      "epoch: 4657 step: 1, loss is 0.0035423613153398037\n",
      "epoch: 4658 step: 1, loss is 0.003539851401001215\n",
      "epoch: 4659 step: 1, loss is 0.00353734428063035\n",
      "epoch: 4660 step: 1, loss is 0.0035348411183804274\n",
      "epoch: 4661 step: 1, loss is 0.00353233702480793\n",
      "epoch: 4662 step: 1, loss is 0.0035298368893563747\n",
      "epoch: 4663 step: 1, loss is 0.0035273386165499687\n",
      "epoch: 4664 step: 1, loss is 0.0035248438362032175\n",
      "epoch: 4665 step: 1, loss is 0.0035223497543483973\n",
      "epoch: 4666 step: 1, loss is 0.00351985776796937\n",
      "epoch: 4667 step: 1, loss is 0.0035173676442354918\n",
      "epoch: 4668 step: 1, loss is 0.0035148817114531994\n",
      "epoch: 4669 step: 1, loss is 0.003512395080178976\n",
      "epoch: 4670 step: 1, loss is 0.0035099126398563385\n",
      "epoch: 4671 step: 1, loss is 0.0035074304323643446\n",
      "epoch: 4672 step: 1, loss is 0.0035049528814852238\n",
      "epoch: 4673 step: 1, loss is 0.0035024737007915974\n",
      "epoch: 4674 step: 1, loss is 0.0035000008065253496\n",
      "epoch: 4675 step: 1, loss is 0.003497525816783309\n",
      "epoch: 4676 step: 1, loss is 0.003495056414976716\n",
      "epoch: 4677 step: 1, loss is 0.0034925888758152723\n",
      "epoch: 4678 step: 1, loss is 0.0034901227336376905\n",
      "epoch: 4679 step: 1, loss is 0.0034876596182584763\n",
      "epoch: 4680 step: 1, loss is 0.0034851981326937675\n",
      "epoch: 4681 step: 1, loss is 0.003482738509774208\n",
      "epoch: 4682 step: 1, loss is 0.0034802791196852922\n",
      "epoch: 4683 step: 1, loss is 0.0034778236877173185\n",
      "epoch: 4684 step: 1, loss is 0.0034753696527332067\n",
      "epoch: 4685 step: 1, loss is 0.0034729193430393934\n",
      "epoch: 4686 step: 1, loss is 0.0034704692661762238\n",
      "epoch: 4687 step: 1, loss is 0.0034680222161114216\n",
      "epoch: 4688 step: 1, loss is 0.0034655777271836996\n",
      "epoch: 4689 step: 1, loss is 0.003463135799393058\n",
      "epoch: 4690 step: 1, loss is 0.003460693871602416\n",
      "epoch: 4691 step: 1, loss is 0.0034582563675940037\n",
      "epoch: 4692 step: 1, loss is 0.0034558186307549477\n",
      "epoch: 4693 step: 1, loss is 0.003453383920714259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4694 step: 1, loss is 0.003450950840488076\n",
      "epoch: 4695 step: 1, loss is 0.003448520554229617\n",
      "epoch: 4696 step: 1, loss is 0.0034460918977856636\n",
      "epoch: 4697 step: 1, loss is 0.0034436648711562157\n",
      "epoch: 4698 step: 1, loss is 0.0034412420354783535\n",
      "epoch: 4699 step: 1, loss is 0.0034388210624456406\n",
      "epoch: 4700 step: 1, loss is 0.0034363986924290657\n",
      "epoch: 4701 step: 1, loss is 0.003433981677517295\n",
      "epoch: 4702 step: 1, loss is 0.0034315644297748804\n",
      "epoch: 4703 step: 1, loss is 0.0034291499760001898\n",
      "epoch: 4704 step: 1, loss is 0.0034267394803464413\n",
      "epoch: 4705 step: 1, loss is 0.0034243296831846237\n",
      "epoch: 4706 step: 1, loss is 0.003421920584514737\n",
      "epoch: 4707 step: 1, loss is 0.003419515211135149\n",
      "epoch: 4708 step: 1, loss is 0.003417109837755561\n",
      "epoch: 4709 step: 1, loss is 0.003414710285142064\n",
      "epoch: 4710 step: 1, loss is 0.003412309568375349\n",
      "epoch: 4711 step: 1, loss is 0.003409912111237645\n",
      "epoch: 4712 step: 1, loss is 0.0034075186122208834\n",
      "epoch: 4713 step: 1, loss is 0.0034051237162202597\n",
      "epoch: 4714 step: 1, loss is 0.003402731381356716\n",
      "epoch: 4715 step: 1, loss is 0.0034003430046141148\n",
      "epoch: 4716 step: 1, loss is 0.003397954860702157\n",
      "epoch: 4717 step: 1, loss is 0.0033955704420804977\n",
      "epoch: 4718 step: 1, loss is 0.0033931871876120567\n",
      "epoch: 4719 step: 1, loss is 0.0033908062614500523\n",
      "epoch: 4720 step: 1, loss is 0.0033884260337799788\n",
      "epoch: 4721 step: 1, loss is 0.0033860511612147093\n",
      "epoch: 4722 step: 1, loss is 0.0033836746588349342\n",
      "epoch: 4723 step: 1, loss is 0.0033813014160841703\n",
      "epoch: 4724 step: 1, loss is 0.003378931200131774\n",
      "epoch: 4725 step: 1, loss is 0.003376560751348734\n",
      "epoch: 4726 step: 1, loss is 0.003374192863702774\n",
      "epoch: 4727 step: 1, loss is 0.003371827770024538\n",
      "epoch: 4728 step: 1, loss is 0.0033694643061608076\n",
      "epoch: 4729 step: 1, loss is 0.0033671052660793066\n",
      "epoch: 4730 step: 1, loss is 0.003364745294675231\n",
      "epoch: 4731 step: 1, loss is 0.003362388117238879\n",
      "epoch: 4732 step: 1, loss is 0.0033600328024476767\n",
      "epoch: 4733 step: 1, loss is 0.003357679583132267\n",
      "epoch: 4734 step: 1, loss is 0.0033553289249539375\n",
      "epoch: 4735 step: 1, loss is 0.00335297966375947\n",
      "epoch: 4736 step: 1, loss is 0.0033506304025650024\n",
      "epoch: 4737 step: 1, loss is 0.0033482848666608334\n",
      "epoch: 4738 step: 1, loss is 0.003345944220200181\n",
      "epoch: 4739 step: 1, loss is 0.003343601943925023\n",
      "epoch: 4740 step: 1, loss is 0.0033412640914320946\n",
      "epoch: 4741 step: 1, loss is 0.0033389264717698097\n",
      "epoch: 4742 step: 1, loss is 0.003336590249091387\n",
      "epoch: 4743 step: 1, loss is 0.0033342582173645496\n",
      "epoch: 4744 step: 1, loss is 0.003331925254315138\n",
      "epoch: 4745 step: 1, loss is 0.0033295960165560246\n",
      "epoch: 4746 step: 1, loss is 0.0033272707369178534\n",
      "epoch: 4747 step: 1, loss is 0.003324944758787751\n",
      "epoch: 4748 step: 1, loss is 0.0033226204104721546\n",
      "epoch: 4749 step: 1, loss is 0.0033202997874468565\n",
      "epoch: 4750 step: 1, loss is 0.003317978698760271\n",
      "epoch: 4751 step: 1, loss is 0.003315662033855915\n",
      "epoch: 4752 step: 1, loss is 0.0033133465331047773\n",
      "epoch: 4753 step: 1, loss is 0.00331103359349072\n",
      "epoch: 4754 step: 1, loss is 0.0033087225165218115\n",
      "epoch: 4755 step: 1, loss is 0.003306412138044834\n",
      "epoch: 4756 step: 1, loss is 0.0033041031565517187\n",
      "epoch: 4757 step: 1, loss is 0.003301798366010189\n",
      "epoch: 4758 step: 1, loss is 0.0032994947396218777\n",
      "epoch: 4759 step: 1, loss is 0.003297193441540003\n",
      "epoch: 4760 step: 1, loss is 0.0032948912121355534\n",
      "epoch: 4761 step: 1, loss is 0.003292592940852046\n",
      "epoch: 4762 step: 1, loss is 0.0032902986276894808\n",
      "epoch: 4763 step: 1, loss is 0.003288004780188203\n",
      "epoch: 4764 step: 1, loss is 0.003285713028162718\n",
      "epoch: 4765 step: 1, loss is 0.0032834219746291637\n",
      "epoch: 4766 step: 1, loss is 0.0032811323180794716\n",
      "epoch: 4767 step: 1, loss is 0.0032788473181426525\n",
      "epoch: 4768 step: 1, loss is 0.0032765623182058334\n",
      "epoch: 4769 step: 1, loss is 0.003274280112236738\n",
      "epoch: 4770 step: 1, loss is 0.003271997207775712\n",
      "epoch: 4771 step: 1, loss is 0.0032697180286049843\n",
      "epoch: 4772 step: 1, loss is 0.003267442574724555\n",
      "epoch: 4773 step: 1, loss is 0.0032651666551828384\n",
      "epoch: 4774 step: 1, loss is 0.0032628949265927076\n",
      "epoch: 4775 step: 1, loss is 0.003260625060647726\n",
      "epoch: 4776 step: 1, loss is 0.0032583565916866064\n",
      "epoch: 4777 step: 1, loss is 0.0032560883555561304\n",
      "epoch: 4778 step: 1, loss is 0.003253822447732091\n",
      "epoch: 4779 step: 1, loss is 0.003251558169722557\n",
      "epoch: 4780 step: 1, loss is 0.003249295987188816\n",
      "epoch: 4781 step: 1, loss is 0.003247037995606661\n",
      "epoch: 4782 step: 1, loss is 0.0032447795383632183\n",
      "epoch: 4783 step: 1, loss is 0.0032425234094262123\n",
      "epoch: 4784 step: 1, loss is 0.003240270307287574\n",
      "epoch: 4785 step: 1, loss is 0.0032380176708102226\n",
      "epoch: 4786 step: 1, loss is 0.0032357685267925262\n",
      "epoch: 4787 step: 1, loss is 0.003233519848436117\n",
      "epoch: 4788 step: 1, loss is 0.0032312734983861446\n",
      "epoch: 4789 step: 1, loss is 0.003229028545320034\n",
      "epoch: 4790 step: 1, loss is 0.003226787317544222\n",
      "epoch: 4791 step: 1, loss is 0.0032245456241071224\n",
      "epoch: 4792 step: 1, loss is 0.0032223062589764595\n",
      "epoch: 4793 step: 1, loss is 0.003220068523660302\n",
      "epoch: 4794 step: 1, loss is 0.003217834047973156\n",
      "epoch: 4795 step: 1, loss is 0.0032156016677618027\n",
      "epoch: 4796 step: 1, loss is 0.0032133685890585184\n",
      "epoch: 4797 step: 1, loss is 0.0032111413311213255\n",
      "epoch: 4798 step: 1, loss is 0.003208912443369627\n",
      "epoch: 4799 step: 1, loss is 0.0032066877465695143\n",
      "epoch: 4800 step: 1, loss is 0.0032044637482613325\n",
      "epoch: 4801 step: 1, loss is 0.0032022427767515182\n",
      "epoch: 4802 step: 1, loss is 0.003200021805241704\n",
      "epoch: 4803 step: 1, loss is 0.0031978024635463953\n",
      "epoch: 4804 step: 1, loss is 0.0031955870799720287\n",
      "epoch: 4805 step: 1, loss is 0.003193371929228306\n",
      "epoch: 4806 step: 1, loss is 0.0031911591067910194\n",
      "epoch: 4807 step: 1, loss is 0.0031889479141682386\n",
      "epoch: 4808 step: 1, loss is 0.0031867362558841705\n",
      "epoch: 4809 step: 1, loss is 0.0031845311168581247\n",
      "epoch: 4810 step: 1, loss is 0.0031823264434933662\n",
      "epoch: 4811 step: 1, loss is 0.0031801217701286077\n",
      "epoch: 4812 step: 1, loss is 0.003177921287715435\n",
      "epoch: 4813 step: 1, loss is 0.003175721038132906\n",
      "epoch: 4814 step: 1, loss is 0.003173522651195526\n",
      "epoch: 4815 step: 1, loss is 0.003171325894072652\n",
      "epoch: 4816 step: 1, loss is 0.0031691326294094324\n",
      "epoch: 4817 step: 1, loss is 0.0031669395975768566\n",
      "epoch: 4818 step: 1, loss is 0.0031647479627281427\n",
      "epoch: 4819 step: 1, loss is 0.0031625593546777964\n",
      "epoch: 4820 step: 1, loss is 0.0031603716779500246\n",
      "epoch: 4821 step: 1, loss is 0.0031581867951899767\n",
      "epoch: 4822 step: 1, loss is 0.0031560040079057217\n",
      "epoch: 4823 step: 1, loss is 0.003153823781758547\n",
      "epoch: 4824 step: 1, loss is 0.003151641460135579\n",
      "epoch: 4825 step: 1, loss is 0.0031494642607867718\n",
      "epoch: 4826 step: 1, loss is 0.0031472884584218264\n",
      "epoch: 4827 step: 1, loss is 0.003145113354548812\n",
      "epoch: 4828 step: 1, loss is 0.003142940578982234\n",
      "epoch: 4829 step: 1, loss is 0.003140767803415656\n",
      "epoch: 4830 step: 1, loss is 0.0031385989859700203\n",
      "epoch: 4831 step: 1, loss is 0.0031364327296614647\n",
      "epoch: 4832 step: 1, loss is 0.0031342683359980583\n",
      "epoch: 4833 step: 1, loss is 0.003132104640826583\n",
      "epoch: 4834 step: 1, loss is 0.0031299435067921877\n",
      "epoch: 4835 step: 1, loss is 0.0031277851667255163\n",
      "epoch: 4836 step: 1, loss is 0.0031256249640136957\n",
      "epoch: 4837 step: 1, loss is 0.003123470116406679\n",
      "epoch: 4838 step: 1, loss is 0.0031213141046464443\n",
      "epoch: 4839 step: 1, loss is 0.0031191613525152206\n",
      "epoch: 4840 step: 1, loss is 0.003117009997367859\n",
      "epoch: 4841 step: 1, loss is 0.003114862134680152\n",
      "epoch: 4842 step: 1, loss is 0.0031127131078392267\n",
      "epoch: 4843 step: 1, loss is 0.003110566409304738\n",
      "epoch: 4844 step: 1, loss is 0.003108423203229904\n",
      "epoch: 4845 step: 1, loss is 0.0031062813941389322\n",
      "epoch: 4846 step: 1, loss is 0.0031041414476931095\n",
      "epoch: 4847 step: 1, loss is 0.003102003363892436\n",
      "epoch: 4848 step: 1, loss is 0.003099866909906268\n",
      "epoch: 4849 step: 1, loss is 0.003097730688750744\n",
      "epoch: 4850 step: 1, loss is 0.003095597494393587\n",
      "epoch: 4851 step: 1, loss is 0.003093466628342867\n",
      "epoch: 4852 step: 1, loss is 0.0030913359951227903\n",
      "epoch: 4853 step: 1, loss is 0.0030892088543623686\n",
      "epoch: 4854 step: 1, loss is 0.0030870826449245214\n",
      "epoch: 4855 step: 1, loss is 0.003084956668317318\n",
      "epoch: 4856 step: 1, loss is 0.003082836512476206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4857 step: 1, loss is 0.003080713329836726\n",
      "epoch: 4858 step: 1, loss is 0.0030785941053181887\n",
      "epoch: 4859 step: 1, loss is 0.0030764774419367313\n",
      "epoch: 4860 step: 1, loss is 0.003074362874031067\n",
      "epoch: 4861 step: 1, loss is 0.0030722483061254025\n",
      "epoch: 4862 step: 1, loss is 0.0030701367650181055\n",
      "epoch: 4863 step: 1, loss is 0.0030680252239108086\n",
      "epoch: 4864 step: 1, loss is 0.0030659169424325228\n",
      "epoch: 4865 step: 1, loss is 0.003063810523599386\n",
      "epoch: 4866 step: 1, loss is 0.0030617034062743187\n",
      "epoch: 4867 step: 1, loss is 0.0030596002470701933\n",
      "epoch: 4868 step: 1, loss is 0.0030574991833418608\n",
      "epoch: 4869 step: 1, loss is 0.0030553999822586775\n",
      "epoch: 4870 step: 1, loss is 0.0030533026438206434\n",
      "epoch: 4871 step: 1, loss is 0.0030512057710438967\n",
      "epoch: 4872 step: 1, loss is 0.003049110993742943\n",
      "epoch: 4873 step: 1, loss is 0.003047018777579069\n",
      "epoch: 4874 step: 1, loss is 0.003044926095753908\n",
      "epoch: 4875 step: 1, loss is 0.0030428380705416203\n",
      "epoch: 4876 step: 1, loss is 0.0030407484155148268\n",
      "epoch: 4877 step: 1, loss is 0.0030386624857783318\n",
      "epoch: 4878 step: 1, loss is 0.0030365774873644114\n",
      "epoch: 4879 step: 1, loss is 0.0030344955157488585\n",
      "epoch: 4880 step: 1, loss is 0.0030324135441333055\n",
      "epoch: 4881 step: 1, loss is 0.0030303362291306257\n",
      "epoch: 4882 step: 1, loss is 0.0030282579828053713\n",
      "epoch: 4883 step: 1, loss is 0.0030261827632784843\n",
      "epoch: 4884 step: 1, loss is 0.003024107776582241\n",
      "epoch: 4885 step: 1, loss is 0.0030220348853617907\n",
      "epoch: 4886 step: 1, loss is 0.003019964788109064\n",
      "epoch: 4887 step: 1, loss is 0.0030178965535014868\n",
      "epoch: 4888 step: 1, loss is 0.003015829250216484\n",
      "epoch: 4889 step: 1, loss is 0.0030137619469314814\n",
      "epoch: 4890 step: 1, loss is 0.0030116986017674208\n",
      "epoch: 4891 step: 1, loss is 0.0030096375849097967\n",
      "epoch: 4892 step: 1, loss is 0.003007576335221529\n",
      "epoch: 4893 step: 1, loss is 0.0030055195093154907\n",
      "epoch: 4894 step: 1, loss is 0.003003461519256234\n",
      "epoch: 4895 step: 1, loss is 0.003001403994858265\n",
      "epoch: 4896 step: 1, loss is 0.0029993511270731688\n",
      "epoch: 4897 step: 1, loss is 0.002997299190610647\n",
      "epoch: 4898 step: 1, loss is 0.002995249815285206\n",
      "epoch: 4899 step: 1, loss is 0.0029932018369436264\n",
      "epoch: 4900 step: 1, loss is 0.0029911533929407597\n",
      "epoch: 4901 step: 1, loss is 0.0029891077429056168\n",
      "epoch: 4902 step: 1, loss is 0.0029870641883462667\n",
      "epoch: 4903 step: 1, loss is 0.002985021099448204\n",
      "epoch: 4904 step: 1, loss is 0.00298298173584044\n",
      "epoch: 4905 step: 1, loss is 0.0029809423722326756\n",
      "epoch: 4906 step: 1, loss is 0.0029789062682539225\n",
      "epoch: 4907 step: 1, loss is 0.0029768706299364567\n",
      "epoch: 4908 step: 1, loss is 0.002974837552756071\n",
      "epoch: 4909 step: 1, loss is 0.0029728058725595474\n",
      "epoch: 4910 step: 1, loss is 0.0029707758221775293\n",
      "epoch: 4911 step: 1, loss is 0.0029687455389648676\n",
      "epoch: 4912 step: 1, loss is 0.0029667196795344353\n",
      "epoch: 4913 step: 1, loss is 0.002964694518595934\n",
      "epoch: 4914 step: 1, loss is 0.0029626707546412945\n",
      "epoch: 4915 step: 1, loss is 0.002960648387670517\n",
      "epoch: 4916 step: 1, loss is 0.0029586281161755323\n",
      "epoch: 4917 step: 1, loss is 0.002956609008833766\n",
      "epoch: 4918 step: 1, loss is 0.0029545926954597235\n",
      "epoch: 4919 step: 1, loss is 0.0029525773134082556\n",
      "epoch: 4920 step: 1, loss is 0.0029505654238164425\n",
      "epoch: 4921 step: 1, loss is 0.00294855167157948\n",
      "epoch: 4922 step: 1, loss is 0.0029465400148183107\n",
      "epoch: 4923 step: 1, loss is 0.0029445320833474398\n",
      "epoch: 4924 step: 1, loss is 0.002942526014521718\n",
      "epoch: 4925 step: 1, loss is 0.0029405197128653526\n",
      "epoch: 4926 step: 1, loss is 0.00293851550668478\n",
      "epoch: 4927 step: 1, loss is 0.002936513163149357\n",
      "epoch: 4928 step: 1, loss is 0.002934511750936508\n",
      "epoch: 4929 step: 1, loss is 0.0029325124341994524\n",
      "epoch: 4930 step: 1, loss is 0.0029305173084139824\n",
      "epoch: 4931 step: 1, loss is 0.002928521716967225\n",
      "epoch: 4932 step: 1, loss is 0.002926527289673686\n",
      "epoch: 4933 step: 1, loss is 0.0029245337937027216\n",
      "epoch: 4934 step: 1, loss is 0.002922543790191412\n",
      "epoch: 4935 step: 1, loss is 0.0029205537866801023\n",
      "epoch: 4936 step: 1, loss is 0.0029185672756284475\n",
      "epoch: 4937 step: 1, loss is 0.0029165814630687237\n",
      "epoch: 4938 step: 1, loss is 0.0029145951848477125\n",
      "epoch: 4939 step: 1, loss is 0.002912611234933138\n",
      "epoch: 4940 step: 1, loss is 0.0029106317088007927\n",
      "epoch: 4941 step: 1, loss is 0.002908652648329735\n",
      "epoch: 4942 step: 1, loss is 0.0029066738206893206\n",
      "epoch: 4943 step: 1, loss is 0.002904697321355343\n",
      "epoch: 4944 step: 1, loss is 0.00290272431448102\n",
      "epoch: 4945 step: 1, loss is 0.0029007515404373407\n",
      "epoch: 4946 step: 1, loss is 0.002898778300732374\n",
      "epoch: 4947 step: 1, loss is 0.002896808786317706\n",
      "epoch: 4948 step: 1, loss is 0.0028948411345481873\n",
      "epoch: 4949 step: 1, loss is 0.002892874414101243\n",
      "epoch: 4950 step: 1, loss is 0.002890909556299448\n",
      "epoch: 4951 step: 1, loss is 0.0028889449313282967\n",
      "epoch: 4952 step: 1, loss is 0.002886983333155513\n",
      "epoch: 4953 step: 1, loss is 0.002885023830458522\n",
      "epoch: 4954 step: 1, loss is 0.002883064327761531\n",
      "epoch: 4955 step: 1, loss is 0.0028811064548790455\n",
      "epoch: 4956 step: 1, loss is 0.002879151375964284\n",
      "epoch: 4957 step: 1, loss is 0.0028771981596946716\n",
      "epoch: 4958 step: 1, loss is 0.0028752475045621395\n",
      "epoch: 4959 step: 1, loss is 0.0028732959181070328\n",
      "epoch: 4960 step: 1, loss is 0.002871345030143857\n",
      "epoch: 4961 step: 1, loss is 0.002869399031624198\n",
      "epoch: 4962 step: 1, loss is 0.0028674511704593897\n",
      "epoch: 4963 step: 1, loss is 0.002865508897230029\n",
      "epoch: 4964 step: 1, loss is 0.002863564295694232\n",
      "epoch: 4965 step: 1, loss is 0.002861623652279377\n",
      "epoch: 4966 step: 1, loss is 0.0028596832416951656\n",
      "epoch: 4967 step: 1, loss is 0.0028577446937561035\n",
      "epoch: 4968 step: 1, loss is 0.0028558094054460526\n",
      "epoch: 4969 step: 1, loss is 0.0028538743499666452\n",
      "epoch: 4970 step: 1, loss is 0.0028519406914711\n",
      "epoch: 4971 step: 1, loss is 0.0028500084299594164\n",
      "epoch: 4972 step: 1, loss is 0.002848078729584813\n",
      "epoch: 4973 step: 1, loss is 0.0028461527545005083\n",
      "epoch: 4974 step: 1, loss is 0.002844223054125905\n",
      "epoch: 4975 step: 1, loss is 0.0028422968462109566\n",
      "epoch: 4976 step: 1, loss is 0.0028403729666024446\n",
      "epoch: 4977 step: 1, loss is 0.002838450949639082\n",
      "epoch: 4978 step: 1, loss is 0.002836528467014432\n",
      "epoch: 4979 step: 1, loss is 0.0028346108738332987\n",
      "epoch: 4980 step: 1, loss is 0.00283269165083766\n",
      "epoch: 4981 step: 1, loss is 0.002830774988979101\n",
      "epoch: 4982 step: 1, loss is 0.0028288608882576227\n",
      "epoch: 4983 step: 1, loss is 0.00282694841735065\n",
      "epoch: 4984 step: 1, loss is 0.0028250375762581825\n",
      "epoch: 4985 step: 1, loss is 0.0028231267351657152\n",
      "epoch: 4986 step: 1, loss is 0.002821217989549041\n",
      "epoch: 4987 step: 1, loss is 0.002819312270730734\n",
      "epoch: 4988 step: 1, loss is 0.0028174060862511396\n",
      "epoch: 4989 step: 1, loss is 0.0028155006002634764\n",
      "epoch: 4990 step: 1, loss is 0.0028135988395661116\n",
      "epoch: 4991 step: 1, loss is 0.0028116984758526087\n",
      "epoch: 4992 step: 1, loss is 0.0028097983449697495\n",
      "epoch: 4993 step: 1, loss is 0.002807900309562683\n",
      "epoch: 4994 step: 1, loss is 0.0028060029726475477\n",
      "epoch: 4995 step: 1, loss is 0.002804109361022711\n",
      "epoch: 4996 step: 1, loss is 0.002802216447889805\n",
      "epoch: 4997 step: 1, loss is 0.002800324698910117\n",
      "epoch: 4998 step: 1, loss is 0.0027984350454062223\n",
      "epoch: 4999 step: 1, loss is 0.002796545857563615\n",
      "epoch: 5000 step: 1, loss is 0.0027946592308580875\n"
     ]
    }
   ],
   "source": [
    "from mindspore import Model\n",
    "from mindspore.nn import MSE\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "\n",
    "net_loss = nn.loss.MSELoss() # 定义损失函数\n",
    "#opt = nn.Momentum(net.trainable_params(), learning_rate=0.05, momentum=0.9) # 定义优化方法\n",
    "opt = nn.Adam(net.trainable_params()) # 定义优化方法,貌似只有这个可用。。。\n",
    "metrics = {\"Accuracy\":Accuracy()} # 评价指标\n",
    "ms_model = Model(net, net_loss, opt, metrics) # 将网络结构、损失函数、优化方法和评价指标进行关联\n",
    "\n",
    "from mindspore.train.callback import  LossMonitor\n",
    "ms_epoch = 5000\n",
    "ms_model.train(ms_epoch, ds_train, callbacks=[LossMonitor()], dataset_sink_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05203991 0.94782585]\n",
      " [0.94033617 0.05986408]\n",
      " [0.94813824 0.05176974]\n",
      " [0.04684253 0.95304847]]\n"
     ]
    }
   ],
   "source": [
    "eval_data = Tensor(eval_data, ms.float32)\n",
    "\n",
    "pred = ms_model.predict(eval_data).asnumpy()\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
